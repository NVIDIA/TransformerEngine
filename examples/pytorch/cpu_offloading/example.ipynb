{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU offloading example\n",
    "\n",
    "Transformer Engine offers a CPU offloading capability that can lower GPU memory usage by offloading a portion of activations into host memory. Because offload operations are overlapped with ongoing computations, you can reclaim significant GPU memory with only a minimal performance impact. To achieve full overlap, you can enable offloading for a subset of your modelâ€™s layers.\n",
    "\n",
    "This approach is particularly advantageous on systems with high-bandwidth NVLink connections between CPU and GPU. For instance, the GB200 Grace Blackwell Superchip features two GPUs of up to 372 GB of memory in total linked to 480 GB of CPU memory via NVLink at up to 900 GB/s.\n",
    "\n",
    "\n",
    "CPU Offloading in Transformer Engine can be easily integrated with any transformer training, because it supports offloading activation of all the layers, not only these provided by the Transformer Engine. For TE layers it additionally supports offloading of FP8 activations.\n",
    "\n",
    "Our tutorial covers two scenarios:\n",
    "\n",
    "1. A basic offloading setup, and\n",
    "2. A customized offload schedule illustrating more complex use case: pipeline-parallel execution.\n",
    "\n",
    "The tutorial was run on GB200 superchips.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic offloading setup\n",
    "\n",
    "Let's demonstrate the default CPU offloading functionality in Transformer Engine. To illustrate that CPU offloading works with any transformer layer (not just TE-specific ones), we'll create a custom layer implementation consisting of TE TransformerLayer and torch linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_engine.pytorch as te\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CustomTransformerLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.torch_linear = torch.nn.Linear(512, 512).to(torch.bfloat16)\n",
    "        self.te_transformer = te.TransformerLayer(512, 512, 4, params_dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.torch_linear(x)\n",
    "        x = self.te_transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how CPU Offloading API works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to offload activations of 4 out of 20 layers.\n",
    "cpu_offload, sync = te.cpu_offload.get_cpu_offload_context(\n",
    "    enabled=True, num_layers=4, model_layers=20, offload_activations=True)\n",
    "\n",
    "\n",
    "def fwd_without_offload(x):\n",
    "    with te.fp8_autocast():\n",
    "        for layer in model:\n",
    "            x = layer(x)\n",
    "    return x\n",
    "\n",
    "def fwd_with_offload(x):\n",
    "    with te.fp8_autocast():\n",
    "        # There are 2 things that need to be done in case of cpu offload:\n",
    "        # - put every layer forward computation inside the cpu offload context,\n",
    "        # - run synchronization function on every layer's output,\n",
    "        for layer in model:\n",
    "            with cpu_offload:\n",
    "                x = layer(x)\n",
    "            x = sync(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare memory usage and execution time between offloaded and non-offloaded versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 47.39 GiB of which 98.69 MiB is free. Process 3744630 has 47.29 GiB memory in use. Of the allocated memory 46.48 GiB is allocated by PyTorch, and 402.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m model = [CustomTransformerLayer().cuda() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m)]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# warm-up\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m x = \u001b[43mfwd_without_offload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m x.sum().backward()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# without offload\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mfwd_without_offload\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m te.fp8_autocast():\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mCustomTransformerLayer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     13\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.torch_linear(x)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mte_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/pgadzinski/TE/transformer_engine/pytorch/transformer.py:794\u001b[39m, in \u001b[36mTransformerLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, self_attn_mask_type, window_size, encoder_output, enc_dec_attn_mask, enc_dec_attn_mask_type, enc_dec_window_size, is_first_microbatch, checkpoint_core_attention, inference_params, rotary_pos_emb, core_attention_bias_type, core_attention_bias, alibi_slopes, cu_seqlens_q, cu_seqlens_kv, cu_seqlens_q_padded, cu_seqlens_kv_padded, max_seqlen_q, max_seqlen_kv, fast_zero_fill, pad_between_seqs)\u001b[39m\n\u001b[32m    791\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m._bias_dropout_add(attention_output, attention_bias, residual)\n\u001b[32m    793\u001b[39m \u001b[38;5;66;03m# MLP.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m mlp_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayernorm_mlp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_first_microbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_first_microbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_residual_connection_post_layernorm:\n\u001b[32m    799\u001b[39m     mlp_output, mlp_bias, residual = mlp_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:893\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    891\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m893\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    895\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/pgadzinski/TE/transformer_engine/pytorch/module/layernorm_mlp.py:1815\u001b[39m, in \u001b[36mLayerNormMLP.forward\u001b[39m\u001b[34m(self, inp, is_first_microbatch)\u001b[39m\n\u001b[32m   1760\u001b[39m         args = [\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m   1761\u001b[39m     args += (\n\u001b[32m   1762\u001b[39m         inp,\n\u001b[32m   1763\u001b[39m         \u001b[38;5;28mself\u001b[39m.layer_norm_weight,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1813\u001b[39m         debug,\n\u001b[32m   1814\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m     out = \u001b[43mfwd_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_layernorm_output:\n\u001b[32m   1818\u001b[39m     out, ln_out = out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:579\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    577\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    578\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    587\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/pgadzinski/TE/transformer_engine/pytorch/module/layernorm_mlp.py:462\u001b[39m, in \u001b[36m_LayerNormMLP.forward\u001b[39m\u001b[34m(ctx, inp, ln_weight, ln_bias, fc1_weight, fc1_bias, fc2_weight, fc2_bias, eps, is_first_microbatch, fp8, fp8_calibration, wgrad_store, fuse_wgrad_accumulation, fc1_input_quantizer, fc1_weight_quantizer, fc1_output_quantizer, fc1_grad_input_quantizer, fc1_grad_weight_quantizer, fc1_grad_output_quantizer, fc2_input_quantizer, fc2_weight_quantizer, fc2_output_quantizer, fc2_grad_input_quantizer, fc2_grad_weight_quantizer, fc2_grad_output_quantizer, cpu_offloading, tp_group, tp_size, sequence_parallel, tensor_parallel, activation_dtype, return_layernorm_output, return_layernorm_output_gathered, bias_gelu_fusion, set_parallel_mode, is_grad_enabled, fwd_ln_sm_margin, bwd_ln_sm_margin, zero_centered_gamma, activation, normalization, ub_overlap_ag, ub_overlap_rs, ub_overlap_rs_dgrad, ub_bulk_wgrad, ub_bulk_dgrad, gemm_gelu_fusion, fsdp_group, module, skip_fp8_weight_update, symmetric_ar_type, debug)\u001b[39m\n\u001b[32m    457\u001b[39m     reduce_scatter_out = torch.empty(dim_size, dtype=activation_dtype, device=device)\n\u001b[32m    459\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# FC2 GEMM\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m gemm_out, *_, reduce_scatter_out = \u001b[43mgeneral_gemm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfc2_weight_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mact_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_workspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactivation_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfc2_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfc2_output_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_split_accumulator\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_split_accumulator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mub\u001b[49m\u001b[43m=\u001b[49m\u001b[43mub_obj_fc2out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mub_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCommOverlapType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mub_overlap_rs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduce_scatter_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;66;03m# Finished FC2 GEMM...\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[32m    477\u001b[39m \n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# Deallocate tensors if no longer needed\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_grad_enabled:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/pgadzinski/TE/transformer_engine/pytorch/cpp_extensions/gemm.py:113\u001b[39m, in \u001b[36mgeneral_gemm\u001b[39m\u001b[34m(A, B, workspace, out_dtype, quantization_params, gelu, gelu_in, accumulate, layout, out, bias, use_split_accumulator, grad, ub, ub_type, extra_output, bulk_overlap)\u001b[39m\n\u001b[32m     88\u001b[39m args = (\n\u001b[32m     89\u001b[39m     A,\n\u001b[32m     90\u001b[39m     transa,  \u001b[38;5;66;03m# transa\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m     use_split_accumulator,\n\u001b[32m    105\u001b[39m )\n\u001b[32m    106\u001b[39m kwargs = {\n\u001b[32m    107\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomm_overlap\u001b[39m\u001b[33m\"\u001b[39m: ub,\n\u001b[32m    108\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomm_type\u001b[39m\u001b[33m\"\u001b[39m: ub_type,\n\u001b[32m    109\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mextra_output\u001b[39m\u001b[33m\"\u001b[39m: extra_output,\n\u001b[32m    110\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbulk_overlap\u001b[39m\u001b[33m\"\u001b[39m: bulk_overlap,\n\u001b[32m    111\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m out, bias_grad, gelu_input, extra_output = \u001b[43mtex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgeneric_gemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    116\u001b[39m     out = debug_quantizer.process_gemm_output(out)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 47.39 GiB of which 98.69 MiB is free. Process 3744630 has 47.29 GiB memory in use. Of the allocated memory 46.48 GiB is allocated by PyTorch, and 402.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "x = torch.randn(((4096, 128, 512)), dtype=torch.bfloat16).cuda()\n",
    "model = [CustomTransformerLayer().cuda() for _ in range(20)]\n",
    "\n",
    "\n",
    "# warm-up\n",
    "x = fwd_without_offload(x)\n",
    "x.sum().backward()\n",
    "\n",
    "# without offload\n",
    "x = torch.randn(((4096, 128, 512)), dtype=torch.bfloat16).cuda()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "x = fwd_without_offload(x)\n",
    "x.sum().backward()\n",
    "print(\"Memory usage without offload = \", round(torch.cuda.max_memory_allocated() / 1024 ** 3, 2), \"GB\")\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Time without offloading: {(t1 - t0)*1000:.2f} ms\")\n",
    "\n",
    "# offload warm-up\n",
    "x = torch.randn(((4096, 128, 512)), dtype=torch.bfloat16).cuda()\n",
    "x = fwd_with_offload(x)\n",
    "x.sum().backward()\n",
    "\n",
    "# with offload\n",
    "x = torch.randn(((4096, 128, 512)), dtype=torch.bfloat16).cuda()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "t0 = time.perf_counter()\n",
    "x = fwd_with_offload(x)\n",
    "x.sum().backward()\n",
    "print(\"Memory usage with offload = \", round(torch.cuda.max_memory_allocated() / 1024 ** 3, 2), \"GB\")\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Time with offloading: {(t1 - t0)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate nsys profile: \n",
    "\n",
    "![](./offloading_trace.png)\n",
    "\n",
    "We can see that compute and offloading/reloading is fully overlapped. \n",
    "\n",
    "It's worth describing the default offloading/reloading startedy we apply.\n",
    "If we offload `k` out of `N` layers, we offload layers `0`, `1`, ..., `k - 1`. Layer `0 <= i <= k - 1` need to finish offloading before layer `N - k + i` starts compute. This layer starts offloading after the layer `N - (k - i)` finishes backward and it needs to finish before the backward pass of layer `i`. This stratedy minimizes memory peak in most training workflows (with preallocated gradient buffers).\n",
    "\n",
    "There are some situations where this strategy may not be optimal and we may want to define our own. We demonstrate such case in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom synchronization of CPU offload: pipeline parallelism\n",
    "\n",
    "Some transformer training workflows are more complicated than the forward-backward scenario. For example consider pipeline parallelism. Suppose we have such a scenario on one node:\n",
    "\n",
    "| Step | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n",
    "|------|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----|\n",
    "| Operation | 0 fwd | 1 fwd | 2 fwd | 3 fwd | 0 bwd | 4 fwd | 1 bwd | 5 fwd | 2 bwd | 6 fwd | 3 bwd | 7 fwd | 4 bwd | 5 bwd | 6 bwd | 7 bwd |\n",
    "\n",
    "I have some idea - offload layers 1-6 with following synchronization:\n",
    "- layer 1 will end offload and release memory before layer 0 starts backward, it will start reload before forward of layer 4,\n",
    "- layer 2 will end offload and release memory before layer 0 starts backward, it will start reload before forward of layer 4,\n",
    "- layer 3 will end offload and release memory before layer 4 starts forward, it will start reload before forward of layer 6,\n",
    "- layer 4 will end offload and release memory before layer 5 starts forward, it will start reload before forward of layer 7,\n",
    "- layer 5 will end offload and release memory before layer 6 starts forward, it will start reload before backward of layer 4,\n",
    "- layer 6 will end offload and release memory before layer 7 starts forward, it will start reload before backward of layer 5,\n",
    "\n",
    "To implement such scenario we can use `synchronization_dict` argument of `get_cpu_offload_context()` method. \n",
    "One needs to provide offloaded layers as a keys and tuples `(offload_fwd: bool, offload_num: int, reload_fwd: bool, reload_num: int)`.\n",
    "Layer will finish offload when `offload_num` layers begins its forward/backward pass (depending on `offload_fwd` being True/False respectively).\n",
    "Layer will start reload when `reload_num` layers starts its forward/backward pass (depending on `reload_fwd` being True/False respectively).\n",
    "\n",
    "So let's create synchronization dict and see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "synchronization_dict = {\n",
    "    1: (False, 0, True, 4), \n",
    "    2: (False, 0, True, 5), \n",
    "    3: (True, 4, True, 6), \n",
    "    4: (True, 5, True, 7), \n",
    "    5: (True, 6, False, 4),  \n",
    "    6: (True, 7, False, 5), \n",
    "}\n",
    "\n",
    "cpu_offload, sync = te.cpu_offload.get_cpu_offload_context(\n",
    "    enabled=True, model_layers=8, offload_activations=True, synchronization_dict=synchronization_dict\n",
    ")\n",
    "\n",
    "inp = [torch.randn((512 * 128, 2, 512), dtype=torch.bfloat16).cuda() for _ in range(8)]\n",
    "out = [None] * 8\n",
    "\n",
    "model = CustomTransformerLayer().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates a simplified pipeline parallel scenario with 8 batches.\n",
    "While this is not a complete pipeline parallel implementation (it only shows forward and backward\n",
    "passes on a single node), it illustrates how computation and communication can overlap when\n",
    "forward and backward passes are interleaved in this way. We also no not claim that provided custom scenario is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[0] = model(inp[0])\n",
    "out[0] = sync(out[0])\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[1] = model(inp[1])\n",
    "out[1] = sync(out[1])\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[2] = model(inp[2])\n",
    "out[2] = sync(out[2])\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[3] = model(inp[3])\n",
    "out[3] = sync(out[3])\n",
    "out[0].sum().backward()\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[4] = model(inp[4])\n",
    "out[4] = sync(out[4])\n",
    "out[1].sum().backward()\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[5] = model(inp[5])\n",
    "out[5] = sync(out[5])\n",
    "out[2].sum().backward()\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[6] = model(inp[6])\n",
    "out[6] = sync(out[6])\n",
    "out[3].sum().backward()\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[7] = model(inp[7])\n",
    "out[7] = sync(out[7])\n",
    "out[4].sum().backward()\n",
    "out[5].sum().backward()\n",
    "out[6].sum().backward()\n",
    "out[7].sum().backward()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the nsys profile:\n",
    "\n",
    "![](./offloading_trace_pp.png)\n",
    "\n",
    "We can see that offload/reload is fully overlapped with compute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
