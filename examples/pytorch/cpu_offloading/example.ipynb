{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU offloading example\n",
    "\n",
    "Transformer Engine offers a CPU offloading capability that can lower GPU memory usage by offloading a portion of activations into host memory. Because offload operations are overlapped with ongoing computations, you can reclaim significant GPU memory with only a minimal performance impact. To achieve full overlap, you can enable offloading for a subset of your model’s layers.\n",
    "\n",
    "This approach is particularly advantageous on systems with high-bandwidth NVLink connections between CPU and GPU. For instance, the GB200 Grace Blackwell Superchip features two GPUs of up to 372 GB of memory in total linked to 480 GB of CPU memory via NVLink at up to 900 GB/s.\n",
    "\n",
    "CPU offloading cannot be combined with CUDA Graphs, since CUDA Graphs currently lack support for CPU–GPU synchronization.\n",
    "\n",
    "CPU Offloading in Transformer Engine can be easily integrated with any transformer training, because it supports offloading activation of all the layers, not only these provided by the Transformer Engine. For TE layers it additionally supports offloading of FP8 activations.\n",
    "\n",
    "Our tutorial covers two scenarios:\n",
    "\n",
    "1. A basic offloading setup, and\n",
    "2. A customized offload schedule illustrating more complex use case: pipeline-parallel execution.\n",
    "\n",
    "The tutorial was run on GB200 superchips.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic offloading setup\n",
    "\n",
    "Let's demonstrate the default CPU offloading functionality in Transformer Engine. To illustrate that CPU offloading works with any transformer layer (not just TE-specific ones), we'll create a custom layer implementation consisting of TE TransformerLayer and torch linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_engine.pytorch as te\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CustomTransformerLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.torch_linear = torch.nn.Linear(512, 512).to(torch.bfloat16)\n",
    "        self.te_transformer = te.TransformerLayer(512, 512, 4, params_dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.torch_linear(x)\n",
    "        x = self.te_transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how CPU Offloading API works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to offload activations of 10 out of 20 layers.\n",
    "cpu_offload, sync = te.cpu_offload.get_cpu_offload_context(\n",
    "    enabled=True, num_layers=10, model_layers=20, offload_activations=True)\n",
    "\n",
    "\n",
    "def fwd_without_offload(x):\n",
    "    with te.fp8_autocast():\n",
    "        for layer in model:\n",
    "            x = layer(x)\n",
    "    return x\n",
    "\n",
    "def fwd_with_offload(x):\n",
    "    with te.fp8_autocast():\n",
    "        # There are 2 things that need to be done in case of cpu offload:\n",
    "        # - put every layer forward computation inside the cpu offload context,\n",
    "        # - run synchronization function on every layer's output,\n",
    "        for layer in model:\n",
    "            with cpu_offload:\n",
    "                x = layer(x)\n",
    "            x = sync(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare memory usage and execution time between offloaded and non-offloaded versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage without offload =  32.09 GB\n",
      "Time without offloading: 4609.79 ms\n",
      "Memory usage with offload =  22.44 GB\n",
      "Time with offloading: 4850.55 ms\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "x = torch.randn((512 * 128, 2, 512), dtype=torch.bfloat16).cuda()\n",
    "model = [CustomTransformerLayer().cuda() for _ in range(20)]\n",
    "\n",
    "# without offload\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "x = fwd_without_offload(x)\n",
    "x.sum().backward()\n",
    "print(\"Memory usage without offload = \", round(torch.cuda.max_memory_allocated() / 1024 ** 3, 2), \"GB\")\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Time without offloading: {(t1 - t0)*1000:.2f} ms\")\n",
    "\n",
    "# offload warm-up\n",
    "x = torch.randn((512 * 128, 2, 512), dtype=torch.bfloat16).cuda()\n",
    "x = fwd_with_offload(x)\n",
    "x.sum().backward()\n",
    "\n",
    "# with offload\n",
    "x = torch.randn((512 * 128, 2, 512), dtype=torch.bfloat16).cuda()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "t0 = time.perf_counter()\n",
    "x = fwd_with_offload(x)\n",
    "x.sum().backward()\n",
    "print(\"Memory usage with offload = \", round(torch.cuda.max_memory_allocated() / 1024 ** 3, 2), \"GB\")\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Time with offloading: {(t1 - t0)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate nsys profile: \n",
    "\n",
    "![](./offloading_trace.png)\n",
    "\n",
    "We can see that compute and offloading/reloading is fully overlapped. \n",
    "\n",
    "It's worth describing the default offloading/reloading startedy we apply.\n",
    "If we offload `k` out of `N` layers, we offload layers `0`, `1`, ..., `k - 1`. Layer `0 <= i <= k - 1` need to finish offloading before layer `N - k + i` starts compute. This layer starts offloading after the layer `N - (k - i)` finishes backward and it needs to finish before the backward pass of layer `i`. This stratedy minimizes memory peak in most training workflows (with preallocated gradient buffers).\n",
    "\n",
    "There are some situations where this strategy may not be optimal and we may want to define our own. We demonstrate such case in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom synchronization of CPU offload: pipeline parallelism\n",
    "\n",
    "Some transformer training workflows are more complicated than the forward-backward scenario. For example consider pipeline parallelism. Suppose we have such a scenario on one node:\n",
    "\n",
    "| Step | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n",
    "|------|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----|\n",
    "| Operation | 0 fwd | 1 fwd | 2 fwd | 3 fwd | 0 bwd | 4 fwd | 1 bwd | 5 fwd | 2 bwd | 6 fwd | 3 bwd | 7 fwd | 4 bwd | 5 bwd | 6 bwd | 7 bwd |\n",
    "\n",
    "I have some idea - offload layers 1-6 with following synchronization:\n",
    "- layer 1 will end offload and release memory before layer 0 starts backward, it will start reload before forward of layer 4,\n",
    "- layer 2 will end offload and release memory before layer 0 starts backward, it will start reload before forward of layer 4,\n",
    "- layer 3 will end offload and release memory before layer 4 starts forward, it will start reload before forward of layer 6,\n",
    "- layer 4 will end offload and release memory before layer 5 starts forward, it will start reload before forward of layer 7,\n",
    "- layer 5 will end offload and release memory before layer 6 starts forward, it will start reload before backward of layer 4,\n",
    "- layer 6 will end offload and release memory before layer 7 starts forward, it will start reload before backward of layer 5,\n",
    "\n",
    "To implement such scenario we can use `synchronization_dict` argument of `get_cpu_offload_context()` method. \n",
    "One needs to provide offloaded layers as a keys and tuples `(offload_fwd: bool, offload_num: int, reload_fwd: bool, reload_num: int)`.\n",
    "Layer will finish offload when `offload_num` layers begins its forward/backward pass (depending on `offload_fwd` being True/False respectively).\n",
    "Layer will start reload when `reload_num` layers starts its forward/backward pass (depending on `reload_fwd` being True/False respectively).\n",
    "\n",
    "So let's create synchronization dict and see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "synchronization_dict = {\n",
    "    1: (False, 0, True, 4), \n",
    "    2: (False, 0, True, 5), \n",
    "    3: (True, 4, True, 6), \n",
    "    4: (True, 5, True, 7), \n",
    "    5: (True, 6, False, 4),  \n",
    "    6: (True, 7, False, 5), \n",
    "}\n",
    "\n",
    "cpu_offload, sync = te.cpu_offload.get_cpu_offload_context(\n",
    "    enabled=True, model_layers=8, offload_activations=True, synchronization_dict=synchronization_dict\n",
    ")\n",
    "\n",
    "inp = [torch.randn((128 * 512, 2, 512), dtype=torch.bfloat16).cuda() for _ in range(8)]\n",
    "out = [None] * 8\n",
    "\n",
    "model = CustomTransformerLayer().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates a simplified pipeline parallel scenario with 8 batches.\n",
    "While this is not a complete pipeline parallel implementation (it only shows forward and backward\n",
    "passes on a single node), it illustrates how computation and communication can overlap when\n",
    "forward and backward passes are interleaved in this way. We also no not claim that provided custom scenario is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[0] = model(inp[0])\n",
    "out[0] = sync(out[0])\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[1] = model(inp[1])\n",
    "out[1] = sync(out[1])\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[2] = model(inp[2])\n",
    "out[2] = sync(out[2])\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[3] = model(inp[3])\n",
    "out[3] = sync(out[3])\n",
    "out[0].sum().backward()\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[4] = model(inp[4])\n",
    "out[4] = sync(out[4])\n",
    "out[1].sum().backward()\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[5] = model(inp[5])\n",
    "out[5] = sync(out[5])\n",
    "out[2].sum().backward()\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[6] = model(inp[6])\n",
    "out[6] = sync(out[6])\n",
    "out[3].sum().backward()\n",
    "with te.fp8_autocast(), cpu_offload:\n",
    "    out[7] = model(inp[7])\n",
    "out[7] = sync(out[7])\n",
    "out[4].sum().backward()\n",
    "out[5].sum().backward()\n",
    "out[6].sum().backward()\n",
    "out[7].sum().backward()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the nsys profile:\n",
    "\n",
    "![](./offloading_trace_pp.png)\n",
    "\n",
    "We can see that offload/reload is fully overlapped with compute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
