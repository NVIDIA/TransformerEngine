{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f38cb68",
   "metadata": {},
   "source": [
    "# Attention Is All You Need!\n",
    "\n",
    "The core idea behind Transformer models is the attention mechanism [[1]](https://arxiv.org/abs/1706.03762). It identifies correlations between words, selects the most important parts of the sentence to focus on, and captures meaningful patterns and dependencies in the data. A typical attention mechanism looks like this, where the pre-softmax operations can be scaling, bias and/or masking, and the post-softmax operation is usually dropout.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"attn.png\" width=\"70%\">\n",
    "<figcaption> Figure 1: Dot product attention. </figcaption>\n",
    "</figure>\n",
    "\n",
    "[Transformer Engine](https://github.com/NVIDIA/TransformerEngine.git) supports the calculation of dot product attention in three frameworks, [PyTorch](https://github.com/pytorch/pytorch), [JAX](https://github.com/google/jax) and [PaddlePaddle](https://github.com/PaddlePaddle/Paddle), and their APIs are,\n",
    "- [transformer_engine.pytorch.DotProductAttention](../api/pytorch.rst#transformer_engine.pytorch.DotProductAttention)\n",
    "- [transformer_engine.jax.flax.DotProductAttention](../api/jax.rst#transformer_engine.jax.flax.DotProductAttention)\n",
    "- [transformer_engine.paddle.DotProductAttention](../api/paddle.rst#transformer_engine.paddle.DotProductAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996576c",
   "metadata": {},
   "source": [
    "## 1. Attention Backends\n",
    "\n",
    "For each supported framework, Transformer Engine provides more than one backends: while the framework-native backends provide a robust baseline, the more fused implementations offer higher performance, such as the [flash-attention](https://github.com/Dao-AILab/flash-attention) and [cuDNN attention](https://github.com/NVIDIA/cudnn-frontend).\n",
    "\n",
    "A list of the available attention backends is below - note that flash-attention is only available in PyTorch, while cuDNN attention is available in all three frameworks.\n",
    "\n",
    "| Framework | Backend (Module Name) | Module Location |\n",
    "| :-------- | :-------------------- | :-------------- |\n",
    "| PyTorch   | cuDNN attention (`FusedAttention`)<br> flash-attention (`FlashAttention`)<br> PyTorch-native attention (`UnfusedDotProductAttention`) | [transformer_engine.pytorch.attention](../../transformer_engine/pytorch/attention.py)      |\n",
    "| JAX       | cuDNN attention (`_FusedDotProductAttention`)<br> JAX-native attention (`_UnfusedDotProductAttention`)                                | [transformer_engine.jax.flax.transformer](../../transformer_engine/jax/flax/transformer.py)   |\n",
    "| PaddlePaddle    | cuDNN attention (`_te_forward`)<br> PaddlePaddle-native attention (`_pd_forward`)                                                           | [transformer_engine.paddle.layer.attention](../../transformer_engine/paddle/layer/attention.py) |\n",
    "\n",
    "### 1.1 Flash vs. Non-Flash\n",
    "\n",
    "The name \"flash attention\" has been a buzz word in the world of attention implementations. But it is important to clarify that out of the backends listed above, flash-attention and cuDNN attention (two of its three sub-backends) are both flash attention, even though ther names might indicate differently.\n",
    "\n",
    "The definition of flash attention is based on whether the implementation is using the flash algorithm or not.\n",
    "\n",
    "We all know that it is a challenge to deal with the quadratic time and space complexity of attention - it requires `O(N^2)` runtime and on-device memory to calculate the dot product attention of query `q`, key `k`, values `v` tensors with sequence length `N`. As we scale up to longer contexts, our training/inference time, as well as memory footprint, increase quadratically to the sequence length.\n",
    "\n",
    "The standard, non-flash attention algorithm has been to process the entire `q`, `k`, `v` tensors in one single step and consume `O(N^2)` memory on the GPU, untill the more efficient, less memory-demanding flash algorithm came along.\n",
    "\n",
    "The flash algorithm was proposed in [[2]](https://arxiv.org/abs/2205.14135), and compared to the standard, non-flash algorithm, it employs two techniques to improve the scaling pattern from quadratic to linear, allowing for a much wider range of sequence length in LLMs.\n",
    "\n",
    "- Tiling. The flash algorithm decomposes the input data into several tiles, with the tile size flexibly determined by the shared memory size on the hardware. It calculates the softmax for each of the tiles and then combines the results together. This reduces the memory footprint as well as the traffic between the global memory and shared memory.\n",
    "\n",
    "- Recomputation. The flash algorithm only stores the softmax normalization factors (linear to sequence length), instead of the full softmax matrix (qudratic to sequence length). This, again, saves the amount of writes/reads between global memory and shared memory, and reduces the amount of global memory required. Some recomputation is needed, however, to reproduce the attention scores in the backward. But this is a small price to pay compared to the time and memory savings from the two techniques above.\n",
    "\n",
    "### 1.2 flash-attention\n",
    "\n",
    "[flash-attention](https://github.com/Dao-AILab/flash-attention) was implemented by the same group of researchers who proposed the flash algorithm [[2]](https://arxiv.org/abs/2205.14135).\n",
    "\n",
    "It is open-source and has significant [performance](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#performance) advantage compared to the framework-native implementations in the community.\n",
    "\n",
    "It only supports the PyTorch framework and has been integrated into Transformer Engine in the PyTorch module: `transformer_engine.pytorch.attention.FusedAttention`. \n",
    "\n",
    "`FusedAttention` is a backend of `transformer_engine.pytorch.DotProductAttention`, and it wraps around flash-attention calls, providing a few miscellaneous functionalities such as converting the `attention_mask` to `cu_seqlens` in the case of `padding` mask.\n",
    "\n",
    "Transformer Engine updates its flash-attention dependency (see `flash-attn` in [setup.py](../../setup.py)) as development evolves. As of v1.7, Transformer Engine supports flash-attention 2.0.6+.\n",
    "\n",
    "### 1.3 cuDNN Attention\n",
    "\n",
    "[cuDNN attention](https://github.com/NVIDIA/cudnn-frontend) is the backbone of several fused attention backends mentioned above. It is developed at NVIDIA and has competing (and in many cases, superior) performances, compared to flash-attention. \n",
    "\n",
    "It requires [cuDNN](https://developer.nvidia.com/cudnn) and [cudnn-frontend](../../3rdparty/cudnn-frontend) to run, and Transformer Engine has support for it in all three frameworks.\n",
    "\n",
    "It has several sub-backends as cuDNN evolves, and the sub-backends 1 and 2 are both based on the flash algorithm as flash-attention is.\n",
    "\n",
    "| Sub-Backend |  Algorithm | Precision | Sequence Length | Architecture | Docs |\n",
    "| :---------- | :--------- | :-------- | :-------------- | :----------- | :--- |\n",
    "| 0 | Non-Flash | BF16/FP16       | <=512       | sm80, 90 | [cuDNN](https://docs.nvidia.com/deeplearning/cudnn/latest/developer/graph-api.html#fused-attention-fprop) |\n",
    "| 1 | Flash     | BF16/FP16       | Any         | sm80+    | [cuDNN](https://docs.nvidia.com/deeplearning/cudnn/latest/developer/graph-api.html#fused-flash-attention-fprop),<br>[cudnn-frontend](https://github.com/NVIDIA/cudnn-frontend/blob/main/docs/operations/Attention.md#scaled-dot-product-attention) |\n",
    "| 2 | Flash     | FP8             | cuDNN pre-9.0: <=512<br>cuDNN 9.0+: Any | cuDNN pre-9.0: sm90<br>cuDNN 9.0+:  sm90+ | cuDNN 9.0+: [cudnn-frontend](https://github.com/NVIDIA/cudnn-frontend/blob/main/docs/operations/Attention.md#scaled-dot-product-attention-fp8) |\n",
    "\n",
    "To compare cuDNN attention's performance against flash-attention's, you can use this following script by modifying the `ModelConfig`. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3106bb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: cd: TransformerEngine/benchmark/attention/: No such file or directory\n",
      "/bin/bash: line 1: ModelConfig: command not found\n",
      "bash: ./run.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd ../../../\n",
    "!cd TransformerEngine/benchmark/attention/\n",
    "!ModelConfig\n",
    "!bash ./run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03482e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36cfda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df.to_csv('timing.csv')\n",
    "df = pd.read_csv('timing.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ad69e",
   "metadata": {},
   "source": [
    "## 2. Backend Selection\n",
    "\n",
    "Given the various backends and sub-backends, Transformer Engine selects the most appropriate one based on both user input and backend performance heuristics. It is first determined whether a backend is eligible based on user input, such as sequence length, number of heads, head size, mask type, and bias type. Runtime environment plays a role here too, since different `flash-attention` or cuDNN versions may have support different input parameters. When multiple backends/sub-backends are eligible, the performance heuristics determine which one of them to choose.\n",
    "\n",
    "Generally, the fused versions of the implementation are more performant than the unfused versions. Also, based on our benchmarks for multiple commonly-used configs, on Hopper architectures, cuDNN attention (sub-backend 1) is faster than `flash-attention`, and on Ampere, slower. Our general selection order is therefore, \n",
    "\n",
    "| Framework | Selection Order                                                                                                                              |\n",
    "| :--------- | :---------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| PyTorch   | sm90: cuDNN attention > TriDao attention > PyTorch native implementation<br>sm80: TriDao attention > cuDNN attention > PyTorch native implementation |\n",
    "| JAX       | cuDNN attention > JAX native implementation                                                                                                          |\n",
    "| Paddle    | cuDNN attention > Paddle native implementation                                                                                                       |\n",
    "\n",
    "As we monitor the performance of different backends/sub-backends, this order may change.\n",
    "\n",
    "Usually users do not need to concern themselves with the selection logic. However, if convergence or performance issues arise, or simply out of curiousity, users can set `NVTE_DEBUG=1` to see which backend has been used exactly.\n",
    "```\n",
    "        [DotProductAttention]: using flash-attn 2.4.2\n",
    "        [DotProductAttention]: using cuDNN attention (backend 0)\n",
    "        [DotProductAttention]: using cuDNN attention (backend 1)\n",
    "        [DotProductAttention]: using cuDNN attention (backend 2)\n",
    "        [DotProductAttention]: fp8_dpa=False, fp8_mha=False, FP8_BWD_ONLY=True\n",
    "```\n",
    "\n",
    "If users need to file an issue with Transformer Engine, it's good to run with `NVTE_DEBUG=1 NVTE_DEBUG_LEVEL=2` and include the printed details in the issue as well.\n",
    "```\n",
    "        [DotProductAttention]: using cuDNN attention (backend 1)\n",
    "        [DotProductAttention]: dtype=torch.bfloat16, b=2, s_q=2048, s_kv=2048, h_q=16, h_kv=2, d=64, qkv_layout='bshd',\n",
    "                               mask_type='padding', bias_type='post_scale_bias', bias_shape='1hss', dropout=0.1,\n",
    "                               is_training=True, context_parallel=True, sm=80, cudnn_version=9.0\n",
    "```\n",
    "\n",
    "Some environment variables are provided if users need to experiment with different backends:\n",
    "```\n",
    "        NVTE_FLASH_ATTN = 0 # disables flash-attention; default = 1\n",
    "        NVTE_FUSED_ATTN = 0 # disables cuDNN attention; default = 1\n",
    "        NVTE_FUSED_ATTN_BACKEND = 0/1/2 # informs Transformer Engine of user perference for cuDNN sub-backends\n",
    "```\n",
    "While `NVTE_FLASH_ATTN=0` and `NVTE_FUSED_ATTN=0` are forceful in turning a backend on or off, the `NVTE_FUSED_ATTN_BACKEND` environment variable only shows user perference. It takes effect only when that backend is an eligible one; if not, Transformer Engine will route the attention calculation to what it had determined as appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9865bd",
   "metadata": {},
   "source": [
    "## 3. Backend Support\n",
    "\n",
    "Different backends have different support for the commonly-used features in Transformer models. As of v1.7, all backends support self- and cross-attention, dropout, and BF16/FP16 precisions. But they vary in other features.\n",
    "\n",
    "| Backend | Precision      | Architecture | Sliding Window Attention | MQA/GQA | Context Parallelism | Deterministic |\n",
    "| :------ | :------------- | :----------- | :----------------------- | :------ | :------------------ | :------------ |\n",
    "| cuDNN attention            | BF16/FP16/FP8  |  sm80+ | No  | Yes | No (`bshd`, `sbhd`, `thd`) | Sub-backend 0, 2: yes<br>Sub-backend 1: yes if workspace optimization path |\n",
    "| TriDao attention           | BF16/FP16      |  sm80+ | Yes | Yes | Yes (`bshd`)                      | Yes if `deterministic=True`                                                                                    |\n",
    "| Framework-native attention | BF16/FP16/FP32 |  Any   | No unless passed in as a mask  | Yes | No                                  | Yes |\n",
    "\n",
    "The \"workspace optimization\" path in the table is a feature in cuDNN sub-backend 1. It trades memory for performance and is turned on by default, when the required workspace size (`batch_size x seqlen_q x seqlen_kv`) is <= 256MB. It provides 20-30% more performance than the non-workspace optimization path, and is deterministic. Users can control it by setting the following environment variable. Please be aware of the Out-Of-Memory risk while doing so.\n",
    "```\n",
    "# CUDNN_FRONTEND_ATTN_DP_WORKSPACE_LIMIT\n",
    "# - unset: enables workspace optimization when required workspace is <= 256MB\n",
    "#          or when bias gradient needs to be computed\n",
    "# -     n: enables workspace optimization when required workspace is <= n bytes\n",
    "# -    -1: enables workspace optimization always\n",
    "# -     0: disables workspace optimization always\n",
    "``` \n",
    "The non-workspace optimization path for cuDNN sub-backend 1 is non-deterministic, and when `deterministic=True` is set for Transformer Engine's `DotProductAttention` module in PyTorch, it turns on the workspace optimization path as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5e9cc",
   "metadata": {},
   "source": [
    "### 3.1 QKV Layout\n",
    "\n",
    "The query (`q`), key (`k`), value (`v`) tensors passed into Transformer Engine may be in various memory layouts. Transformer Engine had defined 15 layouts, 3 formats and 5 layout groups to faciliate this calculation. Here, `b` is the batch size, `s` sequence length, `h` number of heads, `d` head dimension, and `t` the total number of tokens in a batch, i.e. `t = sum(s_i) for i in 0,...,b-1`. A few examples of these layouts are,\n",
    "- `sb3hd`: tensors are sequence first; `q`, `k` and `v` are in one memory space; they are interleaved at the `h * d` dimension; `q, k, v = [qkv[:,:,i,:,:] for i in range(3)]`.\n",
    "- `bshd_bsh2d`: tensors are batch first; `q`, `k` and `v` are in two memory spaces `q` and `kv`; `k` and `v` are interleaved at the `d` dimension inside the `kv` space; `q` is contiguous and `k, v = [kv[:,:,:,i,:] for i in range(2)]`. The second `s` can be different from the first `s` in the case of cross attention, and same for `h` when MQA/GQA is employed.\n",
    "- `thd_thd_thd`: tensors have variable sequence lengths in the batch; `q`, `k` and `v` are in three memory spaces; they are not interleaved in any way and are all contiguous.\n",
    "\n",
    "We group these 15 QKV layouts into 3 QKV formats and 5 QKV layout groups to help simplify the code when multiple layouts share the same properties.\n",
    "\n",
    "| qkv_layout        | qkv_layout_group=`3hd` | qkv_layout_group=`h3d` | qkv_layout_group=`hd_2hd` | qkv_layout_group=`hd_h2d` | qkv_layout_group=`hd_hd_hd` |\n",
    "| :--------------- | :-------------------- | :----- | :---------- | :---------- | :-------------- |\n",
    "| qkv_format=`sbhd` | `sb3hd`                | `sbh3d` | `sbhd_sb2hd` | `sbhd_sbh2d` | `sbhd_sbhd_sbhd` |\n",
    "| qkv_format=`bshd` | `bs3hd`                | `bsh3d` | `bshd_bs2hd` | `bshd_bsh2d` | `bshd_bshd_bshd` |\n",
    "| qkv_format=`thd`  | `t3hd`                 | `th3d`  | `thd_t2hd`   | `thd_th2d`   | `thd_thd_thd`    |\n",
    "\n",
    "Transformer Engine supports all 15 layouts in PyTorch, and 3 layouts, `bs3hd`, `bshd_bs2hd` and `bshd_bshd_bshd`, in JAX and Paddle. A utility function in PyTorch is [transformer_engine.pytorch.attention._get_qkv_layout](../../transformer_engine/pytorch/attention.py) to help users figure out what `qkv_layout` they have.\n",
    "\n",
    "Transformer Engine, as of v1.7, has the following support matrix for different QKV formats:\n",
    "\n",
    "| Backend | Supported QKV Formats |\n",
    "| :--------------- | :-------------------- |\n",
    "| TriDao attention | `bshd`, `sbhd`, `thd` (`sbhd` requires transpose operations) |\n",
    "| cuDNN attention  | `bshd`, `sbhd`, `thd`  |\n",
    "| Framework-native attention | `bshd`, `sbhd` (`sbhd` requires transpose operations) |\n",
    "\n",
    "In Pytorch, when RoPE is employed, the QKV layout may change. If the initial QKV layout is not in the `hd_hd_hd` QKV group, `transformer_engine.pytorch.attention._get_qkv_layout` will convert it to the `hd_hd_hd` group. For example, `sbh3d` will be converted to `sbhd_sbhd_sbhd`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19162830",
   "metadata": {},
   "source": [
    "### 3.2 Attention Mask\n",
    "\n",
    "Transformer Engine supports 5 mask types: `no_mask`, `padding`, `causal`, `padding_causal` (equivalent to `causal_padding`), and `arbitrary`. All masks are defined as `True - masking the corresponding element out` and `False - including the element in attention calculation`.\n",
    "\n",
    "The support matrix for attention mask is:\n",
    "\n",
    "| Backend          | Supported Mask Types  | Require Mask Tensor |\n",
    "| :--------------- | :-------------------- | :------------------ |\n",
    "| TriDao attention | `no_mask`, `causal`, `padding`, `padding_causal` | `no_mask`, `causal`: no<br>`padding`, `padding_causal`: yes if `cu_seqlens` not provided|\n",
    "| cuDNN attention  | `no_mask`, `causal`, `padding`, `padding_causal` | No |\n",
    "| Framework-native attention | `no_mask`, `causal`, `arbitrary` | `no_mask`, `causal`: no<br>`arbitrary`: yes |\n",
    "\n",
    "For `padding` and `padding_causal` mask types, an `attention_mask` tensor should be provided for TriDao attention. For self-attention, `attention_mask` should be one tensor in shape `[batch_size, 1, 1, seqlen_q]`, and for cross-attention, it should be a list of two tensors in shapes `[batch_size, 1, 1, seqlen_q]` and `[batch_size, 1, 1, seqlen_kv]`. For example,\n",
    "\n",
    "Alternatively, users can pass in `cu_seqlens` tensors. In the case where `cu_seqlens` and `attention_mask` are both passed in, Transformer Engine will pick `cu_seqlens` to save extra compute from `get_cu_seqlens()`. Example:\n",
    "\n",
    "For `qkv_format=thd`, if `max_seqlen_q` and `max_seqlen_kv` are not present, Transformer Engine will extract them from the `q`, `k`, `v` tensors. This may cost a GPU-CPU copy as well as a synchronization operation, so it's recommended that users set `max_seqlen_q` and `max_seqlen_kv` when running with `thd` layouts.\n",
    "\n",
    "As of v1.7, cuDNN attention does not support `Arbitrary` masks. However, users can try the `post_scale_bias` path to apply the mask. An example script to convert the mask to a bias and call cuDNN attention is [here](./arbitrary_mask_to_bias.py). This path is more performant than the unfused path.\n",
    "\n",
    "Since v2.1, `flash-attention` changed its implementation for `causal` mask in cross attention (see [here](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#21-change-behavior-of-causal-flag)). Please note that in this case, `flash-attention` uses the bottom right diagonal while cuDNN attention uses the top left.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c4b9d",
   "metadata": {},
   "source": [
    "### 3.3 Attention Bias\n",
    "\n",
    "Transformer Engine supports 4 bias types: `no_bias`, `pre_scale_bias`, `post_scale_bias`, and `ALiBi` (with/without custom slopes).\n",
    "\n",
    "| Backend | Bias Type | Bias Shape | Bias Dtype | Architecture |\n",
    "| :------ | :-------- | :--------- | :--------- | :----------- |\n",
    "| TriDao attention           | `no_bias`, `ALiBi` (with slopes) | NA | AliBi slopes: FP32 | sm80+ |\n",
    "| cuDNN attention            | `no_bias`, `post_scale_bias`, `ALiBi` (without slopes) | `post_scale_bias`: BHSS, 1HSS, B1SS, 11SS for forward and 1HSS for backward | `post_scale_bias`: same as data dtype<br>ALiBi slopes: FP32 | cuDNN 8.9.6+: sm90<br>cuDNN 9.0+: sm80, 90 |\n",
    "| Framework-native attention | `no_bias`, `pre_scale_bias`, `post_scale_bias` | `post_scale_bias`: BHSS, 1HSS, B1SS, 11SS | `post_scale_bias`: same as data dtype | sm80+ |\n",
    "\n",
    "TriDao attention enables `ALiBi` bias by user passing in a `alibi_slopes` tensor. This can be the default slopes that come with vanila ALiBi, or custom slopes from the user. On the other hand, cuDNN attention supports ALiBi by taking a boolean flag rather than a slopes tensor. As of v8.9.6, it only supports vanila ALiBi calculations.\n",
    "\n",
    "The framework-native attention backends do not explicitly support ALiBi, however, users can generate a `post_scale_bias` equivalent to the ALiBi bias. An example of this is the `_get_alibi()` in `transformer_engine.pytorch`. Example in test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfa875",
   "metadata": {},
   "source": [
    "### 3.4 FP8 Attention\n",
    "\n",
    "Transformer Engine supports FP8 attention on the C level () and Python level (). In v1.6, it also added support on the framework level (PyTorch only).\n",
    "\n",
    "fp8_dpa, fp8_mha\n",
    "extra states\n",
    "te.sequencial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52312830",
   "metadata": {},
   "source": [
    "### 3.5 Other Features\n",
    "\n",
    "Users can also mix the forward and backward of cuDNN attention and TriDao attention, by setting `NVTE_FUSED_ATTN_USE_FAv2_BWD=1`. This helps when TriDao attention backward is faster than cuDNN attention backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd1074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
