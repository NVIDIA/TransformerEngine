{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d189d9",
   "metadata": {},
   "source": [
    "# Attention Is All You Need!\n",
    "\n",
    "The core idea behind Transformer models is the attention mechanism [[1]](https://arxiv.org/abs/1706.03762). It identifies the correlation between words, focuses on important parts of the sentence, and allows models to more accurately capture patterns in the data and make predictions. [Transformer Engine](https://github.com/NVIDIA/TransformerEngine.git) supports three frameworks, [PyTorch](https://github.com/pytorch/pytorch), [JAX](https://github.com/google/jax) and [PaddlePaddle](https://github.com/PaddlePaddle/Paddle). Their dot product attention interface are respectively [transformer_engine.pytorch.DotProductAttention](../api/pytorch.rst#transformer_engine.pytorch.DotProductAttention), [transformer_engine.jax.flax.DotProductAttention](../api/jax.rst#transformer_engine.jax.flax.DotProductAttention) and [transformer_engine.paddle.DotProductAttention](../api/paddle.rst#transformer_engine.paddle.DotProductAttention).\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"attn.png\" width=\"70%\">\n",
    "<figcaption> Figure 1: The dot product attention workflow, where pre-softmax operations include scaling, bias, masking and post-softmax operations include dropout (all optional). </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2c4cf",
   "metadata": {},
   "source": [
    "## 1. Attention Backends\n",
    "\n",
    "Transformer Engine provides multiple backends for each of the supported frameworks. While the framework-native implementations offer a robust baseline, other backends provide more computational performance. The full list of backends and their definitions are as follows.\n",
    "\n",
    "| Framework | Backend (Module Name) | Module Location |\n",
    "| :-------- | :-------------------- | :-------------- |\n",
    "| PyTorch   | cuDNN attention (`FusedAttention`)<br> TriDao attention (`FlashAttention`)<br> Native PyTorch implementation (`UnfusedDotProductAttention`) | [transformer_engine.pytorch.attention](../../transformer_engine/pytorch/attention.py)      |\n",
    "| JAX       | cuDNN attention (`_FusedDotProductAttention`)<br> Native JAX implementation (`_UnfusedDotProductAttention`)                                | [transformer_engine.jax.flax.transformer](../../transformer_engine/jax/flax/transformer.py)   |\n",
    "| Paddle    | cuDNN attention (`_te_forward`)<br> Native Paddle implementation (`_pd_forward`)                                                           | [transformer_engine.paddle.layer.attention](../../transformer_engine/paddle/layer/attention.py) |\n",
    "\n",
    "### 1.1 Flash vs Non-Flash\n",
    "\n",
    "The attention calculation has quadratic time and memory complexity to the sequence length, and it poses significant challenges for Transformer models to scale up to longer contexts. The flash attention algorithm [[2]](https://arxiv.org/abs/2205.14135) was proposed to improve the compute efficiency and reduce the memory requirement of attention. Compared to the standard, non-flash algorithm, it employs two distinct techniques:\n",
    "- Tiling: Instead of loading the entire tensors in at the same time, flash attention makes several passes at the input. It decomposes the inputs based on the shared memory size, computes the softmax one block at a time, and combines the results together in a subsequent step.\n",
    "- Recomputation: The standard, non-flash algorithm stores the softmax matrix (qudratic to sequence length) to the global memory, while flash attention only saves the softmax normalization factors (linear to sequence length) from the forward pass. During backward, it recomputes the forward output using these normalization factors. Even though there is added computation, it happens on-chip and the read/write savings still outweigh the extra recomputation.\n",
    "\n",
    "It is proven that the flash algorithm runs faster and requires less memory compared to the non-flash attention algorithm.\n",
    "\n",
    "### 1.2 Public Flash vs cuDNN Flash\n",
    "\n",
    "The popular public implementation, [flash-attention](https://github.com/Dao-AILab/flash-attention), has evolves significantly from version 1 to version 2. It only supports PyTorch, and Transformer Engine has integrated it into its PyTorch attention module `transformer_engine.pytorch.attention.FusedAttention`. `FusedAttention` is a wrapper around `flash-attention`, and it also provides a few miscellaneous functionalities such as converting an `attention_mask` tensor to cumulative sequence lengths `cu_seqlens`. As of v1.7, Transformer Engine supports `flash-attention` 2.0.6+.\n",
    "\n",
    "Another high-performance attention implementation is, [cuDNN attention](https://docs.nvidia.com/deeplearning/cudnn/latest/developer/graph-api.html#fused-flash-attention-fprop), developed at NVIDIA. Even though on the framework level, it has been named \"fused attention\", its sub-backends 1 and 2 (listed below) are also based on the flash algorithm, as `flash-attention` is. cuDNN attention requires [cuDNN](https://developer.nvidia.com/cudnn) and [cudnn-frontend](../../3rdparty/cudnn-frontend) to run, and it has been integrated into Transformer Engine to support all three frameworks (see [Attention Backends](#1.-attention-backends)).\n",
    "\n",
    "| cuDNN Sub-Backend |  Name | Precision | Sequence Length | Algorithm | Architecture |\n",
    "| :---------------- | :------------- | :-------------- | :---------- | :----------- | :-------------- |\n",
    "| 0 | `NVTE_F16_max512_seqlen`       | BF16/FP16       | <=512       | Non-Flash | sm80, 90 |\n",
    "| 1 | `NVTE_F16_arbitrary_seqlen`    | BF16/FP16       | Any         | Flash     | sm80+    |  \n",
    "| 2 | `NVTE_FP8`                     | FP8             | cuDNN pre-9.0: <=512<br>cuDNN 9.0+: any | Flash     | cuDNN pre-9.0: sm90<br>cuDNN 9.0+:  sm90+ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee708d",
   "metadata": {},
   "source": [
    "## 2. Backend Selection\n",
    "\n",
    "Transformer Engine selects the most appropriate attention backend based on user input and backend performance. User input affects whether a backend is eligible, and backend performance heuristics affect which backend to choose when there are multiple  eligible ones.\n",
    "\n",
    "To understand what input parameters are at play, users can set `NVTE_DEBUG=1 NVTE_DEBUG_LEVEL=2` in their run and some example outputs are,\n",
    "```\n",
    "        [DotProductAttention]: using flash-attn 2.1.0\n",
    "        [DotProductAttention]: using cuDNN attention (backend 0)\n",
    "        [DotProductAttention]: dtype=torch.bfloat16, b=2, sq=2048, skv=2048, hq=16, hkv=2, d=64, qkv_layout='bshd', mask_type='padding', bias_type='post_scale_bias', bias_shape='1hss', dropout=0.1, is_training=True, context_parallel=True, sm=80, cudnn_version=9.0\n",
    "        [DotProductAttention]: using cuDNN attention (backend 1)\n",
    "        [DotProductAttention]: dtype=torch.bfloat16, b=2, sq=2048, skv=2048, hq=16, hkv=2, d=64, qkv_layout='bshd', mask_type='padding', bias_type='post_scale_bias', bias_shape='1hss', dropout=0.1, is_training=True, context_parallel=True, sm=80, cudnn_version=9.0\n",
    "        [DotProductAttention]: using cuDNN attention (backend 2)\n",
    "        [DotProductAttention]: fp8_dpa=False, fp8_mha=False, FP8_BWD_ONLY=True...\n",
    "```\n",
    "\n",
    "On the performance side, fused implementations are usually better than the unfused ones, and our general selection order is,\n",
    "\n",
    "| Framework | Selection Order                                                                                                                              |\n",
    "| :--------- | :---------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| PyTorch   | sm90: cuDNN attention > TriDao attention > PyTorch native implementation<br>sm80: TriDao attention > cuDNN attention > PyTorch native implementation |\n",
    "| JAX       | cuDNN attention > JAX native implementation                                                                                                          |\n",
    "| Paddle    | cuDNN attention > Paddle native implementation                                                                                                       |\n",
    "\n",
    "This order may change as we monitor different backends' performance across various model configurations and on different GPU architectures.\n",
    "\n",
    "Users do not need to interfere with the backend selection in Transformer Engine, but if there is a performance regression or convergence issue (or it's simply out of curiousity), these environment variables can be used to turn on/off TriDao attention and cuDNN attention (in PyTorch). \n",
    "```\n",
    "        NVTE_FLASH_ATTN = 0 # disables flash-attention; default = 1\n",
    "        NVTE_FUSED_ATTN = 0 # disables cuDNN attention; default = 1\n",
    "```\n",
    "\n",
    "Users can also *influence* the cuDNN sub-backend selection by\n",
    "```\n",
    "        NVTE_FUSED_ATTN_BACKEND = 0 # or 1, 2\n",
    "```\n",
    "Note that this election only takes effect if the elected backend does have support for the user input and runtime environment. If not, Transformer Engine will still route to the appropriate sub-backend as it would without this user election."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3dd14",
   "metadata": {},
   "source": [
    "## 3. Backend Support\n",
    "\n",
    "### 3.1 Basic Features\n",
    "\n",
    "All attention backends support basic features such as self-/cross-attention and dropout, and they differ in a few other key features. \n",
    "\n",
    "| Backend | Precision      | Architecture | Sliding Window Attention | MQA/GQA | Context Parallelism | Deterministic |\n",
    "| :------ | :------------- | :----------- | :----------------------- | :------ | :------------------ | :------------ |\n",
    "| cuDNN attention            | BF16/FP16/FP8  |  sm80+ | No  | Yes | No (`bshd`, `sbhd`, `thd`) | Sub-backend 0, 2: yes<br>Sub-backend 1: yes if workspace optimization path |\n",
    "| TriDao attention           | BF16/FP16      |  sm80+ | Yes | Yes | Yes (`bshd`)                      | Yes if `deterministic=True`                                                                                    |\n",
    "| Framework-native attention | BF16/FP16/FP32 |  Any   | No unless passed in as a mask  | Yes | No                                  | Yes |\n",
    "\n",
    "For architecture-specific support, please check [here](https://docs.nvidia.com/deeplearning/cudnn/latest/) and [here](https://github.com/Dao-AILab/flash-attention) about cuDNN attention and `flash-attention`.\n",
    "\n",
    "The \"workspace optimization path\" in the table is a feature in cuDNN sub-backend 1 that allows users to trade memory for performance. It uses `batch_size x seqlen_q x seqlen_kv` more memory, but provides 20-30% better performance (available on Hopper only). By default, if the extra memory requirement falls under 256MB, this path is turned on, unless users disable it. The following environment variable allows for more fine-grained control: \n",
    "```\n",
    "# CUDNN_FRONTEND_ATTN_DP_WORKSPACE_LIMIT\n",
    "# - unset: enables workspace optimization when required workspace is <= 256MB\n",
    "#          or when bias gradient needs to be computed\n",
    "# -     n: enables workspace optimization when required workspace is <= n bytes\n",
    "# -    -1: enables workspace optimization always\n",
    "# -     0: disables workspace optimization always\n",
    "```\n",
    "If `deterministic=True` is set for Transformer Engine's `DotProductAttention` module, this path will be turned on as well. The non-workspace optimization path for cuDNN sub-backend 1 is non-deterministic. When choosing the workspace optimization path, please be aware of the Out-Of-Memory risk, due to the increased memory requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d45d35",
   "metadata": {},
   "source": [
    "### 3.2 QKV Layout\n",
    "\n",
    "The query (`q`), key (`k`), value (`v`) tensors that users pass into the `DotProductAttention` module may be in varying memory layouts, and Transformer Engine categorizes them as 15 QKV layouts, 3 QKV formats, or 5 QKV layout groups. Their mapping relationship is as follows.\n",
    "\n",
    "| qkv_layout          | qkv_layout_group=`3hd` | `h3d`   | `hd_2hd`     | `hd_h2d`     | `hd_hd_hd`       |\n",
    "| ------------------- | ------------------------ | --------- | -------------- | -------------- | ------------------ |\n",
    "| qkv_format=`sbhd` | `sb3hd`                | `sbh3d` | `sbhd_sb2hd` | `sbhd_sbh2d` | `sbhd_sbhd_sbhd` |\n",
    "| `bshd`            | `bs3hd`                | `bsh3d` | `bshd_bs2hd` | `bshd_bsh2d` | `bshd_bshd_bshd` |\n",
    "| `thd`             | `t3hd`                 | `th3d`  | `thd_t2hd`   | `thd_th2d`   | `thd_thd_thd`    |\n",
    "\n",
    "Different backends provide different QKV layout support:\n",
    "- TriDao attention: `bshd`, `sbhd` and `thd` formats (`sbhd` layouts require transposes)\n",
    "- cuDNN attention: `bshd`, `sbhd` and `thd` formats\n",
    "- PyTorch-native attention: `bshd` and `sbhd` (`bshd` layouts require transposes)\n",
    "- JAX-native attention: `bshd` and `sbhd` (transposes?)\n",
    "- Paddle-native attention: `bshd` and `sbhd` (transposes?)\n",
    "\n",
    "When RoPE is employed, QKV layout will be automatically converted to the corresponding `hd_hd_hd` layout. For example, `sbh3d` will be converted to `sbhd_sbhd_sbhd`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e68f7f",
   "metadata": {},
   "source": [
    "### 3.3 Attention Mask\n",
    "\n",
    "Transformer Engine supports 5 mask types: `no_mask`, `padding`, `causal`, `padding_causal` (equivalent to `causal_padding`), and `arbitrary`. All masks are defined as `True - masking the corresponding element out` and `False - including the element in attention calculation`.\n",
    "\n",
    "The support matrix for attention mask is:\n",
    "- TriDao attention: `no_mask`, `causal`, `padding`, `padding_causal`\n",
    "- cuDNN attention: `no_mask`, `causal`, `padding`, `padding_causal`\n",
    "- Framework-native attention: `no_mask`, `causal`, `arbitrary` (?)\n",
    "\n",
    "Note that TriDao attention since 2.1 has employed a different causal mask definition than cuDNN attention (see [here](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#21-change-behavior-of-causal-flag)). TriDao attention employs bottom right diagonal and cuDNN attention employs top left diagonal.\n",
    "\n",
    "The `no_mask` and `causal` mask types do not require users to pass in the `attention_mask` tensor, but `padding`, `padding_causal` and `arbitrary` types do, for most of the backends.\n",
    "\n",
    "For `padding` and `padding_causal`, one `attention_mask` tensor should be passed in in the shape of `[batch_size, 1, 1, seqlen_q]` when the attention type is self-attention, and two tensors are required when it's cross attention in shapes of `[batch_size, 1, 1, seqlen_q]` and `[batch_size, 1, 1, seqlen_kv]`. For example, (?)\n",
    "\n",
    "This mask is converted to `cu_seqlens_q` (if self-attention) and `cu_seqlens_kv` (if cross attention). Users can pass in these tensors directly as well, instead of `attention_mask`. Example (?)\n",
    "\n",
    "For `arbitrary` mask type, an `attention_mask` tensor should be passed in in the shape of `[batch_size, 1, 1, seqlen_q]` (?) Example.\n",
    "\n",
    "cuDNN attention does not support `Arbitray` mask. However, users can use its `post_scale_bias` path to apply an `arbitrary` mask. An example script is [here](./arbitrary_mask_to_bias.py). This path is more performant than the unfused path.\n",
    "\n",
    "When `max_seqlen_q` and `max_seqlen_kv` are not set, Transformer Engine calculates the max based on the `query`, `key` and `value` tensors, or `cu_seqlens_q/kv` (?). This incurs a GPU-CPU memcpy and synchronization, to avoid which, users are encouraged (? should?) pass in `max_seqlen_q/kv` directly to Transformer Engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe42d3a",
   "metadata": {},
   "source": [
    "### 3.3 Attention Bias\n",
    "\n",
    "Transformer Engine supports 4 bias types: `no_bias`, `pre_scale_bias`, `post_scale_bias`, and `ALiBi` (with/without custom slopes).\n",
    "\n",
    "| Backend                    | Bias Type                                                                             | Bias Shape                                                                                           | Bias Dtype                                                       | Architecture            |\n",
    "| -------------------------- | ------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | ----------------------- |\n",
    "| TriDao attention           | \\`no_bias\\`, \\`AliBi\\` (with/without slopes) need to convert?                         | NA                                                                                                   | FP32 for AliBi slopes                                            | sm80+                   |\n",
    "| cuDNN attention            | \\`no_bias\\`, \\`post_scale_bias\\`, \\`AliBi\\` (with/without slopes) need to convert?    | BHSS, 1HSS, B1SS, 11SS for \\`post_scale_bias\\` forward,<br>And 1HSS for \\`post_scale_bias\\` backward | Data dtype for \\`post_scale_bias\\`,<br>And FP32 for AliBi slopes | sm80, 90 for cuDNN 9.0+ |\n",
    "| Framework-native attention | \\`no_bias\\`, \\`pre_scale_bias\\`, \\`post_scale_bias\\`, \\`AliBi\\` (with/without slopes) | BHSS, 1HSS, B1SS, 11SS for \\`post_scale_bias\\`                                                       | Data dtype for \\`post_scale_bias\\`,<br>And FP32 for AliBi slopes | sm80+                   |\n",
    "\n",
    "ALiBi vs ALiBi slopes\n",
    "can convert by self? get_alibi(), get_alibi_slopes()\n",
    "Mask + Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa867d0",
   "metadata": {},
   "source": [
    "### 3.4 FP8 Attention\n",
    "fp8_dpa, fp8_mha\n",
    "extra states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a3a4a",
   "metadata": {},
   "source": [
    "### 3.5 Other Features\n",
    "\n",
    "Users can also mix the forward and backward of cuDNN attention and TriDao attention, by setting `NVTE_FUSED_ATTN_USE_FAv2_BWD=1`. This helps when TriDao attention backward is faster than cuDNN attention backward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c7ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
