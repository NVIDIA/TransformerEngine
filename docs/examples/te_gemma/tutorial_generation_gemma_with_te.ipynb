{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8581f0e4",
   "metadata": {},
   "source": [
    "# Accelerating Generation of the Hugging Face Gemma Model with Transformer Engine\n",
    "\n",
    "Generative AI has made remarkable strides in recent years, with Large Language Models (LLMs) like ChatGPT at the forefront. These models have revolutionized how we interact with machine-generated content, providing capabilities that range from writing assistance to complex decision support. The core functionality of these models is the generation process, which involves predicting the next token in a sequence based on the preceding text. This task is critical for applications such as automated content creation, translation, and more, emphasizing the importance of efficient implementation.\n",
    "\n",
    "For those seeking a deeper understanding of text generation mechanisms in Transformers, it is recommended to check out the [HuggingFace generation tutorial](https://huggingface.co/docs/transformers/llm_tutorial).\n",
    "\n",
    "In our previous tutorials on [Llama](../te_llama/tutorial_accelerate_hf_llama_with_te.ipynb) and [Gemma](./tutorial_accelerate_hf_gemma_with_te.ipynb), we demonstrated how finetuning can be accelerated using the Transformer Engine's `TransformerLayer`. Building on this foundation, our current objective is to enhance the generation speed of the Gemma model.\n",
    "\n",
    "This tutorial will introduce and explain several advanced features of the Transformer Engine that contribute to this goal:\n",
    "\n",
    "##### 1. THD Attention Layout.\n",
    "\n",
    "Addressing the challenge of computing attention for sequences with varying lengths, a common method is to pad these sequences and apply an attention mask. The Transformer Engine, however, offers a more optimized approachâ€”by specifying the lengths and offsets of the sequences, attention can be computed directly. Instead of passing the matrix and mask with the shape `[b, s, h, d]`, one can pass a matrix of the shape `[t, h, d]` along with tensors detailing sequence lengths and offsets to run the attention optimized for this case. This specific attention layout is referred to as the **THD layout**.\n",
    "\n",
    "<center>\n",
    "<img src=\"./media/atn1.png\" alt=\"\" width= \"400\"><br>\n",
    "Fig. 1. The sequences and the mask for standard attention layout - padding from the end.<br><br>\n",
    "<img src=\"./media/atn2.png\" alt=\"\" width=\"400\"><br>\n",
    "Fig. 2. The sequences and the mask for standard attention layout - padding from the beginning.<br><br>\n",
    "<img src=\"./media/atn3.png\" alt=\"\" width=\"400\"><br>\n",
    "Fig. 3. An attention with thd layer.<br><br>\n",
    "</center>\n",
    "\n",
    "##### 2. FP8 Weight Calibration.\n",
    "\n",
    "Assuming that we have a model trained in FP32/BF16 precision and we wish to execute it in FP8 precision, the process isn't straightforward due to the absence of appropriate FP8 scaling factors. In this scenario, FP8 calibration becomes essential. By conducting several forward passes on sample data, we can compute the FP8 saling parameters. This calibration allows the model to operate correctly in FP8 precision.\n",
    "\n",
    "We highly recommend familiarizing yourself with the [tutorial](../../examples/fp8_primer.ipynb) on FP8 precision to understand the importance of proper scaling factors.\n",
    "\n",
    "##### 3. CUDA Graphs API.\n",
    "\n",
    "The speed of GPUs is increasing at a rapid pace. It turns out that sometimes the runtime of kernels is shorter than the time it takes for the CPU to submit them, which can lead to significant overhead. CUDA Graphs were developed to address this issue. When certain kernels are executed repeatedly, this tool allows us to record and replay them without CPU involvement. This becomes particularly useful in applications like text generation, where a `TransformerLayer` is run for every token that needs to be generated.\n",
    "\n",
    "We recommend reading further about CUDA Graphs [here](https://developer.nvidia.com/blog/cuda-graphs/).\n",
    "\n",
    "PyTorch exposes graphs via a raw `torch.cuda.CUDAGraphclass` and two convenience wrappers, `torch.cuda.graph` and `torch.cuda.make_graphed_callables`. More information about the cuda graphs in Pytorch can be found [here](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/).\n",
    "\n",
    "Transformer Engine supports cuda graphs from version 1.5.\n",
    "\n",
    "#### Benchmarking\n",
    "\n",
    "We'll evaluate the generation time across three benchmarks:\n",
    "- Long input sequences (up to 256 tokens) with short generation (up to 128 tokens),\n",
    "- Short input sequences (up to 64 tokens) with long generation (up to 1000 tokens).\n",
    "\n",
    "All benchmarks are conducted with a batch size of 64 using the dataset \"timdettmers/openassistant-guanaco\".\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note</b>\n",
    "    \n",
    "This tutorial focuses on showcasing the mentioned features of Transformer Engine in the context of generation. It's important to note, however, that NVIDIA provides another library, [TensorRT](https://developer.nvidia.com/tensorrt), which is optimized for inference tasks and should be considered for such use cases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f91a9",
   "metadata": {},
   "source": [
    "## Dependencies for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5201d77",
   "metadata": {},
   "source": [
    "Following files and media are necessary to effectively run this tutorial:\n",
    "\n",
    "1. `te_gemma.py`\n",
    "    - This file contains the code to load a Hugging Face Gemma checkpoint in Transformer Engine's `TransformerLayer` instead of Hugging Face's `GemmaDecoderLayer`. It does also contain code for generation with THD attention and weight calibration.\n",
    "2. `utils.py`\n",
    "    - This file contains the code related to dataloading, hyperparameters, setting up model/optimizers/accelerator, model training and other miscellaneous tasks like restarting the jupyter notebook from within the cell. \n",
    "3. `media/`\n",
    "    - This directory contains the images used in the following tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfabbf",
   "metadata": {},
   "source": [
    "## [Baseline] Running Hugging Face generation with Gemma model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59560bff",
   "metadata": {},
   "source": [
    "HuggingFace Transformers library offers generation API. We will use HuggingFace generation for the Gemma model as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "\n",
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://huggingface.co/google/gemma-7b\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"  # <== Add model weight location here e.g. \"/path/to/downloaded/gemma/weights\"\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "model = init_te_gemma_model(hyperparams).cuda()\n",
    "\n",
    "generate_sample_text(model)\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3698dc6",
   "metadata": {},
   "source": [
    "We put these times into the table for later comparison.\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf3d47",
   "metadata": {},
   "source": [
    "## [Improvement 1] Speeding up generation by using Transformer Engine with THD attention\n",
    "\n",
    "Similarly to the Gemma tutorial, we substitute `GemmaDecoderLayer` with `TransformerLayer` from Transformer Engine. \n",
    "\n",
    "Input sequences can have various lengths. The most common approach is to use the padding and attention masks in such situation. We will use more straightforward method - using the THD attention layout with offests. \n",
    "\n",
    "<center>\n",
    "<span style=\"display: flex; flex-direction: row; justify-content: center\">\n",
    "<span style=\"display: flex; flex-direction: column; align-items: center\">\n",
    "Query layer   \n",
    "<img src=\"./media/pic1.png\" alt=\"\" height=\"200\">\n",
    "</span>\n",
    "<span style=\"display: flex; flex-direction: column; align-items: center\">\n",
    "Key layer and value layer  \n",
    "<img src=\"./media/pic2.png\" alt=\"\" height=\"200\">\n",
    "</span>\n",
    "</span>\n",
    "cu_seqlens_q = [0, 1, 3, 7, 9, 12] <br>\n",
    "cu_seqlens_kv = [0, 1, 3, 6, 8, 10] <br>\n",
    "seq_offsets_q = [0, 5, 10, 15, 20, 25] * h * d <br>\n",
    "seq_offsets_k = [0, 7, 14, 21, 28, 35] * h * d <br>\n",
    "seq_offsets_v = [0, 7, 14, 21, 28, 35] * h * d <br>\n",
    "</center>\n",
    "\n",
    "The class `transformer_engine.DotProductAttention` supports this format. One need to pass the following things as the arguments to the forward:\n",
    "- `seq_offsets_q`, `seq_offsets_k`, `seq_offsets_v` - which represents the offsets of the beginnings of the next sequences,\n",
    "- `cu_seqlens_q`, `cu_seqlens_kv` - cumulative sum of the lengths of the sequences of query and values,\n",
    "- `max_seqlen_q` - maximum sequence length in query layer,\n",
    "- `max_seqlen_kv` - maximum sequence length in key-value layer.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "Currently, the THD attention for `TransformerLayer` is supported only for inference.\n",
    "</div>\n",
    "\n",
    "Let's look how using TransformerEngine with THD attention impacts the speed of generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://huggingface.co/google/gemma-7b\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"  # <== Add model weight location here e.g. \"/path/to/downloaded/gemma/weights\"\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "hyperparams.fuse_qkv_params = False\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_te_gemma_model(hyperparams).to(torch.bfloat16).cuda()\n",
    "\n",
    "generate_sample_text(model)\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e397a65",
   "metadata": {},
   "source": [
    "By using THD attention we obtained following speedups:\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b171a0",
   "metadata": {},
   "source": [
    "## [Improvement 2] Running generation in FP8 of the model trained in higher precision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80288b",
   "metadata": {},
   "source": [
    "We are now preparing to execute FP8 generation using the Gemma model. However, this process is not straightforward. Since the model was originally trained with BF16 precision, the FP8 scaling factors have not been computed. Operating the model at such low precision without the correct scaling could result in significant numerical errors, which in turn would produce incorrect results.\n",
    "\n",
    "We highly recommend familiarizing yourself with the [tutorial](../../examples/fp8_primer.ipynb) on FP8 precision to understand the necessity of scaling.\n",
    "\n",
    "##### Weight Calibration\n",
    "\n",
    "To address the issue outlined above, we will implement weight calibration. This involves running several forward iterations at BF16 precision within the context `te.fp8_autocast(enabled=False, calibration=True)`. This setup allows the forward pass to operate at higher precision, while we simultaneously collect `amax_history` and other parameters related to the FP8 precision, which is essential for calculating the FP8 scaling factors.\n",
    "\n",
    "The code below outlines the steps to initialize the BF16 model and conduct several forward iterations within the specified context. After these iterations, we save the model, and these weights will be utilized in subsequent chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecee0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and methods\n",
    "import transformer_engine.pytorch as te\n",
    "from utils import *\n",
    "import torch\n",
    "\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=False).cuda()\n",
    "model = model.to(torch.bfloat16)\n",
    "accelerator = Accelerator(\n",
    "        log_with=\"wandb\",\n",
    "        gradient_accumulation_steps=hyperparams.gradient_accumulation_steps,\n",
    "        mixed_precision=hyperparams.mixed_precision\n",
    "    )\n",
    "train_dataloader = get_dataloaders(accelerator, hyperparams)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "\n",
    "print(\"Calibration started\")\n",
    "with te.fp8_autocast(enabled=False, calibrating=True):\n",
    "    model.train()\n",
    "    train_dataloader = enumerate(train_dataloader)\n",
    "\n",
    "    for i in range(100):\n",
    "        step, batch = next(train_dataloader)\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].cuda()\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=10\n",
    "        )\n",
    "        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(\"calibration_finished\")\n",
    "\n",
    "print(\"scale_fwd computation started\")\n",
    "with te.fp8_autocast(enabled=True):\n",
    "    for i in range(10):\n",
    "        step, batch = next(train_dataloader)\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].cuda()\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=1\n",
    "        )\n",
    "print(\"scale_fwd_computation ended\")\n",
    "\n",
    "print(\"Casting weights...\")\n",
    "model_fp8 = init_te_gemma_model(hyperparams, fp8_model_init=True).cuda()\n",
    "model_fp8.load_state_dict(model.state_dict())\n",
    "print(\"Weights casted\")\n",
    "\n",
    "print(\"Saving model...\")\n",
    "torch.save(model_fp8.state_dict(), 'model_fp8_state_dict.pth')\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcd135",
   "metadata": {},
   "source": [
    "#### Generation in FP8\n",
    "\n",
    "Now we are ready to run FP8 inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "\n",
    "from utils import *\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=True, qkv_format=\"thd\").cuda()\n",
    "\n",
    "# Load weights of the model with the proper scaling factors.\n",
    "model.load_state_dict(torch.load('model_fp8_state_dict.pth'))\n",
    "\n",
    "generate_sample_text(model, fp8=True)\n",
    "benchmark_generation(model, fp8=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbb56c",
   "metadata": {},
   "source": [
    "We add the speedups to the table:\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         | \n",
    "| THD attention + FP8 with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a89d9c",
   "metadata": {},
   "source": [
    "## [Improvement 3] Speeding up generation with CUDA Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d53e7b",
   "metadata": {},
   "source": [
    "TransformerEngine includes a function `transformer_engine.pytorch.make_graphed_callables`, which functions similarly to the corresponding feature in PyTorch. It is capable of recording any modules from the Transformer Engine. Below is a code excerpt from `te_gemma.py`:\n",
    "```\n",
    "        generator = GemmaGenerator(\n",
    "            lm_head=self.lm_head,\n",
    "            model=self.model, \n",
    "            inference_params=inference_params, \n",
    "            generation_config=generation_config, \n",
    "            dtype=hidden_states.dtype,\n",
    "        )\n",
    "\n",
    "        (...)\n",
    "        if use_cuda_graphs:\n",
    "            fp8_format = Format.HYBRID\n",
    "            fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=32, amax_compute_algo=\"max\")\n",
    "            graphed_generator = te.pytorch.make_graphed_callables(\n",
    "                generator, \n",
    "                args, \n",
    "                fp8_enabled=True, \n",
    "                fp8_recipe=fp8_recipe, \n",
    "                allow_unused_input=True,\n",
    "                num_warmup_iters=10\n",
    "            )\n",
    "            \n",
    "        (...)\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "            next_tokens = graphed_generator(*args) if use_cuda_graphs else generator(*args)\n",
    "            output_tokens.append(next_tokens.clone())\n",
    "```\n",
    "\n",
    "Let us now proceed to evaluate the performance improvement offered by CUDA Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "\n",
    "from utils import *\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=True, qkv_format=\"thd\").cuda()\n",
    "\n",
    "# Load weights of the model with the proper scaling factors.\n",
    "model.load_state_dict(torch.load('model_fp8_state_dict.pth'))\n",
    "\n",
    "generate_sample_text(model, fp8=True, use_cuda_graphs=True)\n",
    "benchmark_generation(model, fp8=True, use_cuda_graphs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb430f",
   "metadata": {},
   "source": [
    "We finally obtained the **??%** speedup.\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         | \n",
    "| THD attention + FP8 with TE                                               | -      | -                         |  \n",
    "| THD attention + FP8 + Cuda Graphs with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd87e6",
   "metadata": {},
   "source": [
    "We can also see how use of graphs reduced CPU overhead. Here are two screenshots from the profiler:\n",
    "\n",
    "<center>\n",
    "<img src=\"./media/pic2.png\" alt=\"Logo Pythona\" height=\"200\">\n",
    "<br>\n",
    "Generation without CUDA Graphs\n",
    "<br>\n",
    "\n",
    "<img src=\"./media/pic2.png\" alt=\"Logo Pythona\" height=\"200\">\n",
    "<br>\n",
    "Generation with CUDA Graphs\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e87275",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2452d",
   "metadata": {},
   "source": [
    "In this tutorial, we've explored three features of the Transformer Engine:\n",
    "1. Support for the THD attention layout,\n",
    "2. FP8 weights calibration,\n",
    "3. Integration with CUDA Graphs.\n",
    "\n",
    "Each of these features can be applied in various contexts, and here we demonstrated their use for achieving fast inference. It's important to note that the fastest possible inference speeds can be achieved using NVIDIA's inference-optimized [TensorRT](https://developer.nvidia.com/tensorrt) library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
