{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8581f0e4",
   "metadata": {},
   "source": [
    "# Speeding up the Hugging Face Gemma model generation with Cuda Graphs and THD attention with FP8 precision\n",
    "\n",
    "As it can be seen in the [tutorial for Llama](../te_llama/tutorial_accelerate_hf_llama_with_te.ipynb) or [tutorial for Gemma](./tutorial_accelerate_hf_gemma_with_te.ipynb), transformer models can be accelerated by using Transformer's Engine `TransformerLayer`. In this tutorial we want to present few more advanced features, namely\n",
    "1. THD attention layout.\n",
    "2. FP8 weight calibration - for doing inference in FP8 precisions for models, which were trained in higher precisions.\n",
    "3. CUDA Graphs API.\n",
    "\n",
    "We will compare generation time at 3 benchmarks:\n",
    "- long input sequences (max 256 tokens), short generation part (max 128 tokens),\n",
    "- short input sequences (max 64 tokens), long generation (max 100 tokens),\n",
    "\n",
    "All benchmarks above run with batch size 64 and on the dataset \"timdettmers/openassistant-guanaco\".\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "    \n",
    "This tutorial aims to demonstrate features of TransformerEngine mentioned above on the example of generation. It's important to note though, that NVIDIA offers other library to use for inference - namely [TensorRT](https://developer.nvidia.com/tensorrt), which should be used in such cases.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f91a9",
   "metadata": {},
   "source": [
    "## Dependencies for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5201d77",
   "metadata": {},
   "source": [
    "Following files and media are necessary to effectively run this tutorial:\n",
    "\n",
    "1. `te_gemma.py`\n",
    "    - This file contains the code to load a Hugging Face Gemma checkpoint in Transformer Engine's `TransformerLayer` instead of Hugging Face's `GemmaDecoderLayer`. It does also contain code for generation with THD attention and weight calibration.\n",
    "2. `utils.py`\n",
    "    - This file contains the code related to dataloading, hyperparameters, setting up model/optimizers/accelerator, model training and other miscellaneous tasks like restarting the jupyter notebook from within the cell. \n",
    "3. `media/`\n",
    "    - This directory contains the images used in the following tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bfbe6c",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c29e7",
   "metadata": {},
   "source": [
    "1. [Baseline] Running Hugging Face generation with Gemma model\n",
    "2. [Improvement 1] Speeding up generation by using Transformer Engine THD attention.\n",
    "3. [Improvement 2] Running generation of the model trained in hign precision in FP8.\n",
    "4. [Improvement 3] Speeding up generation with CudaGraphs.\n",
    "5. Conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfabbf",
   "metadata": {},
   "source": [
    "## [Baseline] Running Hugging Face generation with Gemma model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59560bff",
   "metadata": {},
   "source": [
    "Hugging Face Transformers library offers generation API. We will treat this as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "\n",
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://llama.meta.com/llama-downloads/\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"  # <== Add model weight location here e.g. \"/path/to/downloaded/llama/weights\"\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_baseline_model(hyperparams).cuda()\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"Some random initial str \", \"Another string ... \"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(duration)\n",
    "\n",
    "# Decode the output tensor to text\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Display the generated text\n",
    "for text in generated_texts:\n",
    "    print(text)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3698dc6",
   "metadata": {},
   "source": [
    "We will put these times into the table for later comparison.\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf3d47",
   "metadata": {},
   "source": [
    "## [Improvement 1] Speeding up generation by using Transformer Engine THD attention\n",
    "\n",
    "Similarly to the Gemma tutorial, we substitute `GemmaDecoderLayer` with `TransformerLayer` from Transformer Engine. Since initial sequences have different lengths, we have following choices:\n",
    "1. Use padding from the beginning and then use standard attention with `\"bshd\"` or `\"sbhd\"` layout.\n",
    "2. Do not pad from the beginning and use THD attention.\n",
    "\n",
    "In this tutorial we will show the second option. We illustrate THD attention idea on the two pictures below.\n",
    "\n",
    "<center>\n",
    "<img src=\"./media/pic1.png\" alt=\"Logo Pythona\" width=\"200\" height=\"200\">\n",
    "<img src=\"./media/pic2.png\" alt=\"Logo Pythona\" width=\"200\" height=\"200\">\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://llama.meta.com/llama-downloads/\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"  # <== Add model weight location here e.g. \"/path/to/downloaded/llama/weights\"\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "hyperparams.fuse_qkv_params = False\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_te_gemma_model(hyperparams).cuda()\n",
    "#accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "model = model.to(torch.bfloat16).cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"I love when \", \"I \"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "import time\n",
    "\n",
    "# PoczÄ…tek pomiaru czasu\n",
    "start_time = time.time()\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=40\n",
    ")\n",
    "\n",
    "# Koniec pomiaru czasu\n",
    "end_time = time.time()\n",
    "\n",
    "# Obliczamy czas trwania operacji\n",
    "duration = end_time - start_time\n",
    "print(f\"Generation time: {duration} seconds\")\n",
    "\n",
    "\n",
    "# Decode the output tensor to text\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Display the generated text\n",
    "for text in generated_texts:\n",
    "    print(text)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e397a65",
   "metadata": {},
   "source": [
    "By using THD attention we obtained following speedups:\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b171a0",
   "metadata": {},
   "source": [
    "## [Improvement 2] Running generation of the model trained in high precision in FP8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80288b",
   "metadata": {},
   "source": [
    "Now we want to run FP8 generation with Gemma model. But it's not that simple! Since model was trained in BF16 precision, the FP8 scaling factors are not computed. Running the model with such low precision without proper scaling will lead to serious numerical divergence, which will lead to wrong output.\n",
    "\n",
    "##### Weight calibration\n",
    "\n",
    "The wieght calibration is solution of the problem mentioned above. We will run few forward iterations on BF16 precision within context `te.fp8_autocast(enabled=False, calibration=True)`. This means that the forward pass will be done in higher precision, but we will store `amax_history`, which will be used to compute FP8 scaling factors. \n",
    "\n",
    "In the code below, we initialize BF16 model, run few iterations of forward passes within mentioned context. Then, we save model - we will also use these weights in the next chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecee0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and methods\n",
    "import transformer_engine.pytorch as te\n",
    "from utils import *\n",
    "import accelerate\n",
    "from transformer_engine.pytorch import fp8_model_init\n",
    "from transformer_engine.common.recipe import Format, DelayedScaling\n",
    "import torch\n",
    "\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=False).cuda()\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "\n",
    "accelerator = Accelerator(\n",
    "        log_with=\"wandb\",\n",
    "        gradient_accumulation_steps=hyperparams.gradient_accumulation_steps,\n",
    "        mixed_precision=hyperparams.mixed_precision\n",
    "    )\n",
    "train_dataloader = get_dataloaders(accelerator, hyperparams)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "\n",
    "print(\"Calibration started\")\n",
    "with te.fp8_autocast(enabled=False, calibrating=True):\n",
    "    model.train()\n",
    "    train_dataloader = enumerate(train_dataloader)\n",
    "\n",
    "    for i in range(100):\n",
    "        step, batch = next(train_dataloader)\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].cuda()\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=10\n",
    "        )\n",
    "        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        print(generated_texts[0][:50])\n",
    "print(\"calibration_finished\")\n",
    "\n",
    "print(\"scale_fwd computation started\")\n",
    "with te.fp8_autocast(enabled=True):\n",
    "    for i in range(10):\n",
    "        step, batch = next(train_dataloader)\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].cuda()\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=1\n",
    "        )\n",
    "print(\"scale_fwd_computation ended\")\n",
    "\n",
    "print(\"Casting weights...\")\n",
    "model_fp8 = init_te_gemma_model(hyperparams, fp8_model_init=True).cuda()\n",
    "model_fp8.load_state_dict(model.state_dict())\n",
    "print(\"Weights casted\")\n",
    "\n",
    "\n",
    "print(\"Saving model...\")\n",
    "torch.save(model_fp8.state_dict(), 'model_fp8_state_dict.pth')\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcd135",
   "metadata": {},
   "source": [
    "Now we are ready to run FP8 inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "import transformer_engine.pytorch as te\n",
    "\n",
    "import os\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "from transformer_engine.pytorch import fp8_model_init\n",
    "from transformer_engine.common.recipe import Format, DelayedScaling\n",
    "\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=True, qkv_format=\"thd\").cuda()\n",
    "\n",
    "print(\"Loading model\")\n",
    "model_state_dict = torch.load('model_fp8_state_dict.pth')\n",
    "model.load_state_dict(model_state_dict)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"Some random initial str \", \"Another string ... \"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "fp8_format = Format.HYBRID\n",
    "fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=32, amax_compute_algo=\"max\")\n",
    "torch.manual_seed(1234)\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "    with autocast(dtype=torch.bfloat16, cache_enabled=False):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=40,\n",
    "                use_cuda_graphs=False\n",
    "            )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for text in generated_texts[:12]:\n",
    "    print(\"-\" * 50)\n",
    "    print(text)\n",
    "\n",
    "print(f\"Duration = {duration}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbb56c",
   "metadata": {},
   "source": [
    "We add the speedups to the table:\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         | \n",
    "| THD attention + FP8 with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a89d9c",
   "metadata": {},
   "source": [
    "## [Improvement 3] Speeding up generation with CudaGraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d53e7b",
   "metadata": {},
   "source": [
    "The inference code is run by CPU which starts GPU kernels. When GPU kernels are fast enough, it can result in overhead caused by the CPU. That's where Cuda Graphs come in. When some series of kernels starts is repeatable, then it can be recorded and then repeated without usage of the CPU. You can read more about the Cuda Graphs [here](https://developer.nvidia.com/blog/cuda-graphs/).\n",
    "\n",
    "Pytorch supports Cuda Graphs with `torch.cuda` API. Neverthless, there are some requirements for sequence of tensor operations to be able of being captured and repeated correctly. Namely, all the operations need to be static - meaning that tensors should not \"move\" between iterations. Pytorch offers also simpler way of using cuda graphs - the method `torch.cuda.make_graphed_callables`. We can easily record every pytorch module.\n",
    "\n",
    "Transformer Engine from version 1.6 also `make_graphed_callables` API. In the following code I run generate method from `te_gemma.py`. This is the code responsible for making graphed part:\n",
    "\n",
    "```\n",
    "graphed_generator = TeGraphed(...)\n",
    "(...)\n",
    "    if use_cuda_graphs:\n",
    "        fp8_format = Format.HYBRID\n",
    "        fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=32, amax_compute_algo=\"max\")\n",
    "        graphed_layers = te.pytorch.make_graphed_callables(\n",
    "                graphed_generator, \n",
    "                args, \n",
    "                fp8_enabled=True, \n",
    "                fp8_recipe=fp8_recipe, \n",
    "                allow_unused_input=True,\n",
    "                num_warmup_iters=10\n",
    "            )\n",
    "            \n",
    "    for i in range(max_new_tokens):\n",
    "        next_tokens = graphed_layers(*args) if use_cuda_graphs else graphed_generator(*args)\n",
    "        output_tokens.append(next_tokens.clone())\n",
    "```\n",
    "\n",
    "Now, let's see how big the speedup is going to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDNN_LOGLEVEL_DBG'] = '3'\n",
    "os.environ['CUDNN_LOGDEST_DBG'] = 'backlog.txt'\n",
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "import transformer_engine.pytorch as te\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "from transformer_engine.pytorch import fp8_model_init\n",
    "from transformer_engine.common.recipe import Format, DelayedScaling\n",
    "\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=True, qkv_format=\"thd\").cuda()\n",
    "\n",
    "print(\"Loading model\")\n",
    "model_state_dict = torch.load('model_fp8_state_dict.pth')\n",
    "model.load_state_dict(model_state_dict)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"Some random initial str \", \"Another string ... \"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "fp8_format = Format.HYBRID\n",
    "fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=32, amax_compute_algo=\"max\")\n",
    "torch.manual_seed(1234)\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "    with autocast(dtype=torch.bfloat16, cache_enabled=False):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                use_cuda_graphs=True\n",
    "            )\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for text in generated_texts[:12]:\n",
    "    print(\"-\" * 50)\n",
    "    print(text)\n",
    "\n",
    "print(f\"Duration = {duration}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb430f",
   "metadata": {},
   "source": [
    "We finally obtained the **??%** speedup.\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         | \n",
    "| THD attention + FP8 with TE                                               | -      | -                         |  \n",
    "| THD attention + FP8 + Cuda Graphs with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e87275",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2452d",
   "metadata": {},
   "source": [
    "In this tutorial we showed three features of Transformer Engine:\n",
    "1. Support of THD attention layout,\n",
    "2. FP8 weights calibration.\n",
    "3. Support of Cuda Graphs.\n",
    "\n",
    "Each one of them can be used in different context, here we showed how to use them to obtain fast inference. We remind though, that this is not the fastest possible way of doing inference - for doing do we reccommend looking at the [TensorRT](https://developer.nvidia.com/tensorrt) library from NVIDIA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
