{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating token generation of the Hugging Face Gemma Model with Transformer Engine\n",
    "\n",
    "Generative AI has made remarkable strides in recent years, with Large Language Models (LLMs) like ChatGPT at the forefront. These models have revolutionized how we interact with machine-generated content, providing capabilities that range from writing assistance to complex decision support. The core functionality of these models is the generation process, which involves predicting the next token in a sequence based on the preceding text. This task is critical for applications such as automated content creation, translation, and more, emphasizing the importance of efficient implementation.\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./media/generation_animation.gif\" alt=\"\"><br>\n",
    "Animation 1. Hugging Face Gemma model token generation.\n",
    "</center>\n",
    "\n",
    "For those seeking a deeper understanding of text generation mechanisms in Transformers, it is recommended to check out the [HuggingFace generation tutorial](https://huggingface.co/docs/transformers/llm_tutorial).\n",
    "\n",
    "In the previous tutorials on [Llama](../te_llama/tutorial_accelerate_hf_llama_with_te.ipynb) and [Gemma](./tutorial_accelerate_hf_gemma_with_te.ipynb), it was demonstrated how finetuning can be accelerated using the Transformer Engine's `TransformerLayer`. Building on this foundation, the current objective is to enhance the generation speed of the Gemma model.\n",
    "\n",
    "This tutorial will introduce and explain several advanced features of the Transformer Engine that contribute to this goal:\n",
    "\n",
    "##### 1. THD Attention Layout.\n",
    "\n",
    "Addressing the challenge of computing attention for sequences with varying lengths, a common method is to pad these sequences and apply an attention mask. The Transformer Engine, however, offers a more optimized approach—by specifying the lengths and offsets of the sequences, attention can be computed directly. Instead of passing the matrix and mask with the shape `[b, s, h, d]`, one can pass a matrix of the shape `[t, h, d]` along with tensors detailing cumulative sequence lengths and offsets to run the attention optimized for this case. This specific attention layout is referred to as the **THD layout**.\n",
    "\n",
    "<center>\n",
    "<img src=\"./media/thd_bshd.png\" alt=\"\"><br>\n",
    "Fig. 1. The difference between BSDH (default) and THD attention layouts is as follows: with BSDH, we need to provide the attention mask, while with THD, we need to provide cumulative sequence lengths and sequence offsets.<br><br>\n",
    "</center>\n",
    "\n",
    "##### 2. CUDA Graphs API.\n",
    "\n",
    "The speed of GPUs is increasing at a rapid pace. It turns out that sometimes the runtime of kernels is shorter than the time it takes for the CPU to submit them, which can lead to significant overhead. CUDA Graphs can address this issue. When certain kernels are executed repeatedly, it allows us to record and replay them without less CPU involvement. This becomes particularly useful in applications like token generation, where a `TransformerLayer` is run for every token that needs to be generated.\n",
    "\n",
    "We recommend reading further about CUDA Graphs [here](https://developer.nvidia.com/blog/cuda-graphs/).\n",
    "\n",
    "PyTorch exposes graphs via a raw `torch.cuda.CUDAGraph` class and two convenience wrappers, `torch.cuda.graph` and `torch.cuda.make_graphed_callables`. More information about the cuda graphs in Pytorch can be found [here](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/).\n",
    "\n",
    "<center>\n",
    "<img src=\"./media/graphs.png\" alt=\"\"><br>\n",
    "Fig. 2. CUDA Graphs allow us to reduce the overhead generated by the long time it takes to launch a single kernel. They enable the recording and replaying of subsequent launches, thus reducing the total time used by the CPU. <br><br>\n",
    "</center>\n",
    "\n",
    "\n",
    "##### 3. FP8 Weights Calibration.\n",
    "\n",
    "Assuming that the model is trained in FP32/BF16 precision and the goal is to execute it in FP8 precision, the process isn't straightforward due to the absence of appropriate FP8 scaling factors. In this scenario, FP8 calibration becomes essential. By conducting several forward passes on sample data, the FP8 scaling parameters can be computed. This calibration allows the model to operate correctly in FP8 precision.\n",
    "\n",
    "It is highly recommended to familiarize oneself with the [tutorial](../../examples/fp8_primer.ipynb) on FP8 precision to understand the importance of proper scaling factors.\n",
    "\n",
    "<center>\n",
    "<img src=\"./media/calibration.png\" alt=\"\" ><br>\n",
    "Fig. 3. \n",
    "If the model is trained in BF16/FP32, it does not include the computed FP8 scaling factors. When it is run under <b>fp8_autocast()</b>, the value of these scaling factors will default to their initial values, which can cause numerical errors. Weight calibration involves calculating FP8 scaling factors from higher precision forward passes. Once these factors are computed, the numerical errors should be resolved. <br><br>\n",
    "</center>\n",
    "\n",
    "##### 4. FP8 Model Weights.\n",
    "\n",
    "The typical approach is to store weights in higher precision and then cast them to fp8 before operations. This may prevent accuraccy drops in training. However, for inference, this level of precision is not necessary.\n",
    "\n",
    "The TransformerEngine includes a wrapper `fp8_model_​init`, which allows for the creation of models that store only the FP8 copy of the weights. This eliminates the need to cast from higher precision to BF16, saving time in this casting process. \n",
    "\n",
    "<center>\n",
    "<img src=\"./media/fp8_model_init.png\" alt=\"\" ><br>\n",
    "Fig. 6. Model under <b>fp8_autocast()</b> stores weights in high precision by default, and casts them if needed. It can leads to slowdown and increased memory usage. Using <i>fp8_model_init()</i> results in storing weight in FP8. <br><br>\n",
    "</center>\n",
    "\n",
    "#### Benchmarking\n",
    "\n",
    "We'll evaluate the generation time across one benchmark: generation with context phase max sequence length = 128, batch size = 64 and number of generated tokens = 896 on random texts with random lengths.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note</b>\n",
    "    \n",
    "This tutorial focuses on showcasing the mentioned features of Transformer Engine in the context of generation. It's important to note, however, that NVIDIA provides [TensorRT](https://developer.nvidia.com/tensorrt), which is optimized for inference tasks and should be considered for such use cases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f91a9",
   "metadata": {},
   "source": [
    "## Dependencies for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5201d77",
   "metadata": {},
   "source": [
    "Following files and media are necessary to effectively run this tutorial:\n",
    "\n",
    "1. `te_gemma.py`\n",
    "    - This file contains the code to load a Hugging Face Gemma checkpoint in Transformer Engine's `TransformerLayer` instead of Hugging Face's `GemmaDecoderLayer`. It does also contain code for generation with THD attention and weight calibration.\n",
    "2. `te_gemma_loading_weights.py`\n",
    "    - This file contains logic of mapping the parameters from `GemmaDecoderLayer` into the `TransformerLayer`.\n",
    "3. `utils.py`\n",
    "    - This file contains the code related to dataloading, hyperparameters, setting up model/optimizers/accelerator, model training and other miscellaneous tasks like restarting the jupyter notebook from within the cell. \n",
    "4. `requirements.txt`\n",
    "    - Contains necessary Python packages for this tutorial\n",
    "4. `media/`\n",
    "    - This directory contains the images used in the following tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31390c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.tst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfabbf",
   "metadata": {},
   "source": [
    "## [Baseline] Running Hugging Face generation with Gemma model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59560bff",
   "metadata": {},
   "source": [
    "HuggingFace Transformers library offers generation API. \n",
    "We will use HuggingFace generation for the Gemma model as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7477e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Generation example 1 ==============================\n",
      "Tell me something about GPUs:\n",
      "\n",
      "1. What is the difference between a GPU and a CPU?\n",
      "2. What is a GPU used for?\n",
      "3. What is a GPU used for in a computer?\n",
      "4. What is a GPU used for in a computer game\n",
      "============================== Generation example 2 ==============================\n",
      "Tell me something about NVIDIA:\n",
      "\n",
      "NVIDIA is a global technology company that designs and develops graphics processing units (GPUs) for the gaming, professional visualization, and data center markets. The company was founded in 1993 and is headquartered in Santa Clara, California.\n",
      "\n",
      "\n",
      "============================== Benchmarking ==============================\n",
      "Benchmarking for batch_size = 64 and max total tokens = 1024\n",
      "Time: 82.04 s.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://huggingface.co/google/gemma-7b\n",
    "hyperparams.model_name = \"\"  # <== Add model weight location here e.g. \"/path/to/downloaded/gemma/weights\"\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "model = init_baseline_model(hyperparams).cuda()\n",
    "\n",
    "print_sample_of_generated_texts(model)\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3698dc6",
   "metadata": {},
   "source": [
    "We put these times into the table for later comparison.\n",
    "\n",
    "| Models                                                      | Time | Speedup |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | 82,04 sec      | 1                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb40f45",
   "metadata": {},
   "source": [
    "## [Iprovement 1] Using TransformerLayer from Transformer Engine instead of GemmaDecoderLayer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecde0c0",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./media/substitution.png\" alt=\"\">\n",
    "Fig. Each GemmaDecoderLayer is substituted by a tuned TransformerLayer from the Transformer Engine.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b40f2",
   "metadata": {},
   "source": [
    "As in the [Gemma](./tutorial_accelerate_hf_gemma_with_te.ipynb) finetuning tutorial, we substitute GemmaDecoderLayer by a tuned TransformerLayer from the Transformer Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dceef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "hyperparams.model_name = \"\"\n",
    "\n",
    "model = init_te_gemma_model(hyperparams)\n",
    "\n",
    "print_sample_of_generated_texts(model)\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d40836",
   "metadata": {},
   "source": [
    "We have obtained speedup of **x%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d18e8",
   "metadata": {},
   "source": [
    "\n",
    "| Models                                                      | Time (s) | Speedup |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | 82.04     | 1                         |\n",
    "| TE                                               |       |                          | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf3d47",
   "metadata": {},
   "source": [
    "## [Improvement 2] Use of THD attention layout.\n",
    "\n",
    "Input sequences can have various lengths. Hugging Face generation - as can be seen in Animation 1 - pads the sequences and then uses attention mask. The THD attention layout is faster, but less flexible. Instead of attention mask, cumulative sequence lengths and offsets need to be provided.\n",
    "\n",
    "<center>\n",
    "<span style=\"display: flex; flex-direction: row; justify-content: center\">\n",
    "<span style=\"display: flex; flex-direction: column; align-items: center\">\n",
    "Query layer   \n",
    "<img src=\"./media/thd_dimensions_1.png\" alt=\"\" height=\"200\">\n",
    "</span>\n",
    "<span style=\"display: flex; flex-direction: column; align-items: center\">\n",
    "Key layer and value layer  \n",
    "<img src=\"./media/thd_dimensions_2.png\" alt=\"\" height=\"200\">\n",
    "</span>\n",
    "</span>\n",
    "cu_seqlens_q = [0, 1, 3, 7, 9, 12] <br>\n",
    "cu_seqlens_kv = [0, 1, 3, 6, 8, 10] <br>\n",
    "seq_offsets_q = [0, 5, 10, 15, 20, 25] * h * d <br>\n",
    "seq_offsets_k = [0, 7, 14, 21, 28, 35] * h * d <br>\n",
    "seq_offsets_v = [0, 7, 14, 21, 28, 35] * h * d <br>\n",
    "<br><br>\n",
    "Fig. Example of arguments related to THD attention layout that need to be passed to <i>transformer_engine.pytorch.DotProductAttention().</i>\n",
    "</center>\n",
    "\n",
    "The class `transformer_engine.pytorch.DotProductAttention` supports this format. One need to pass the following things as the arguments to the forward:\n",
    "- `seq_offsets_q`, `seq_offsets_k`, `seq_offsets_v` – which represent the offsets of the beginnings of the next sequences,\n",
    "- `cu_seqlens_q`, `cu_seqlens_kv` – cumulative sum of the lengths of the sequences of query and values,\n",
    "- `max_seqlen_q` – maximum sequence length in query layer,\n",
    "- `max_seqlen_kv` – maximum sequence length in key-value layer.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "Currently, the THD attention for `TransformerLayer` is supported only for token generation.\n",
    "</div>\n",
    "\n",
    "Let's look how using TransformerEngine with THD attention impacts the speed of token generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc5e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device memory hasn't been flushed, try manually restarting the Jupyter kernel!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Generation example 1 ==============================\n",
      "Tell me something about GPUs:\n",
      "\n",
      "1. What is the difference between a GPU and a CPU?\n",
      "2. What is the difference between a GPU and a graphics card?\n",
      "3. What is the difference between a graphics card and a video card?\n",
      "4. What is the\n",
      "============================== Generation example 2 ==============================\n",
      "Tell me something about NVIDIA:\n",
      "\n",
      "NVIDIA is a global technology company that designs and develops graphics processing units (GPUs) for the gaming and professional markets.\n",
      "\n",
      "What is the difference between a CPU and a GPU?\n",
      "\n",
      "A CPU (Central Processing Unit) is a computer chip that is\n",
      "============================== Benchmarking ==============================\n",
      "Benchmarking for batch_size = 64 and max total tokens = 1024\n",
      "Time: 28.19 s.\n"
     ]
    }
   ],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "restart_jupyter_notebook()\n",
    "\n",
    "from utils import *\n",
    "\n",
    "hyperparams.model_name = \"\"  # <== Add model weight location here.\n",
    "hyperparams.qkv_format = \"thd\"\n",
    "\n",
    "model = init_te_gemma_model(hyperparams)\n",
    "\n",
    "print_sample_of_generated_texts(model)\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e397a65",
   "metadata": {},
   "source": [
    "By using THD attention we obtained following speedup:\n",
    "\n",
    "| Models                                                      | Time | Speedup |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | 82.04 sec     | 1                         |\n",
    "| THD attention with TE                                               | 28.19      | 2.91                         | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a89d9c",
   "metadata": {},
   "source": [
    "## [Improvement 3] Speeding up generation with CUDA Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d53e7b",
   "metadata": {},
   "source": [
    "TransformerEngine includes a function `transformer_engine.pytorch.make_graphed_callables`, which functions similarly to the corresponding feature in PyTorch. It is capable of recording any modules from the Transformer Engine. Below is a code excerpt from `te_gemma.py` from class `TEGemmaForCausalLMCudaGraphs`:\n",
    "```\n",
    "    def __init__(self, config : GemmaConfig):\n",
    "            (...)\n",
    "            \n",
    "            # Here \"the trick\" happens. We override methods from TEGemmaForCausalLM\n",
    "            # with their recorded version. After invocation of each of them,\n",
    "            # captured graph will be replayed with minimal usage of CPU,\n",
    "            # what will lead to huge speedup.\n",
    "            (...)\n",
    "            self._model_context_phase = self.record_graph(self._model_context_phase, self.hidden_states_buffer) # CUDA Graphs recording\n",
    "\n",
    "            (...)        \n",
    "            self._model_generation_phase = self.record_graph(self._model_generation_phase, self.generation_buffer) # CUDA Graphs recording\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def record_graph(self, function, input_tensor):\n",
    "        # function is invoked on argument (self.hidden_states,) and all kernels are recorded.\n",
    "        # record_graph() returns captured function, which can be run later with minimal use of th CPU.\n",
    "        fp8_format = Format.HYBRID\n",
    "        fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=32, amax_compute_algo=\"max\")\n",
    "        with autocast(dtype=torch.bfloat16, cache_enabled=False):\n",
    "            graphed_function = te.pytorch.make_graphed_callables(\n",
    "                function, \n",
    "                (input_tensor,), \n",
    "                fp8_enabled=True, \n",
    "                fp8_recipe=fp8_recipe, \n",
    "                allow_unused_input=True,\n",
    "                num_warmup_iters=3\n",
    "            )\n",
    "        return graphed_function\n",
    "```\n",
    "\n",
    "It is strongly reccomended to review the entire code of the class `TEGemmaForCausalLMCudaGraphs`. Let's now proceed to evaluate the performance improvement offered by CUDA Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a3a8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Generation example 1 ==============================\n",
      "Tell me something about GPUs:\n",
      "\n",
      "1. What is the difference between a GPU and a CPU?\n",
      "2. What is the difference between a GPU and a graphics card?\n",
      "3. What is the difference between a graphics card and a video card?\n",
      "4. What is the\n",
      "============================== Generation example 2 ==============================\n",
      "Tell me something about NVIDIA:\n",
      "\n",
      "NVIDIA is a global technology company that designs and develops graphics processing units (GPUs) for the gaming and professional markets.\n",
      "\n",
      "What is the difference between a CPU and a GPU?\n",
      "\n",
      "A CPU (Central Processing Unit) is a computer chip that is\n",
      "============================== Benchmarking ==============================\n",
      "Benchmarking for batch_size = 64 and max total tokens = 1024\n",
      "Time: 16.81 s.\n"
     ]
    }
   ],
   "source": [
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "restart_jupyter_notebook()\n",
    "\n",
    "from utils import *\n",
    "\n",
    "hyperparams.model_name = \"\"   # <== Add model weight location here e.g. \"/path/to/downloaded/gemma/weights\"\n",
    "hyperparams.qkv_format = \"thd\"\n",
    "\n",
    "hyperparams.generation_cuda_graphs = True\n",
    "\n",
    "# It is necessary to preallocate a static buffer.\n",
    "# CUDA graphs require static input tensors for every kernel.\n",
    "# This approach may result in a slight increase in memory consumption;\n",
    "# however, the substantial speedup achieved makes it worthwhile.\n",
    "hyperparams.cuda_graphs_static_batch_size = 64\n",
    "hyperparams.cuda_graphs_static_max_seq_len = 1024\n",
    "hyperparams.cuda_graphs_static_max_context_len = 128\n",
    "model = init_te_gemma_model(hyperparams)\n",
    "\n",
    "print_sample_of_generated_texts(model)\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb430f",
   "metadata": {},
   "source": [
    "We obtained the **4.88x** speedup!\n",
    "\n",
    "| Models                                                      | Time | Speedup |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | 82.04      | 1                         |\n",
    "| THD attention with TE                                               | 28.19      | 2.91                         | \n",
    "| THD attention +  Cuda Graphs with TE                                               | 16.81      | 4.88                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a11b75c",
   "metadata": {},
   "source": [
    "Let's look at the screenshots from *NVIDIA Nsight System* profiler to see where this speedup comes from:\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "<span style=\"\"> \n",
    "<img src=\"./media/graphs-1.png\" alt=\"\" height=\"200\"><br>\n",
    "    Fig. 7. Without CUDA Graphs. We can see that GPU (blue) is idle for most of the time.\n",
    "    <br><br><br>\n",
    "<img src=\"./media/graphs_2.png\" alt=\"\" height=\"200\"><br>\n",
    "    Fig. 8. With CUDA Graphs. We can see that GPU (orange) is utilized.\n",
    "</span>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b171a0",
   "metadata": {},
   "source": [
    "## [Improvement 4] Running generation in FP8 of the model trained in higher precision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80288b",
   "metadata": {},
   "source": [
    "Implementing FP8 generation with the Gemma model is not straightforward, because it was initially trained using BF16 precision, and the necessary FP8 scaling factors are missing. Running the model at this lower precision without proper scaling could lead to significant errors and incorrect results.\n",
    "\n",
    "It is highly recommended to familiarize oneself with the [tutorial](../../examples/fp8_primer.ipynb) on FP8 precision to understand the necessity of scaling.\n",
    "\n",
    "##### Weight Calibration\n",
    "\n",
    "To address the issue outlined above, weight calibration will be used. This involves running several forward iterations at BF16 precision within the context `te.fp8_autocast(enabled=False, calibration=True)`. This setup allows the forward pass to operate at higher precision, while simultaneously collecting `amax_history` and other parameters related to the FP8 precision, which are essential for calculating the FP8 scaling well.\n",
    "\n",
    "The code below outlines the steps to initialize the BF16 model and conduct several forward iterations within the specified context. After these iterations, the model is saved, and these weights will be utilized in subsequent chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecee0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import transformer_engine.pytorch as te\n",
    "\n",
    "hyperparams.model_name = \"\"  # <== Add model weight location here e.g. \"/path/to/downloaded/llama/weights\"\n",
    "hyperparams.fuse_qkv_params = True # This is needed by the last improvement.\n",
    "\n",
    "model = init_te_gemma_model(hyperparams)\n",
    "\n",
    "# Calibration\n",
    "with te.fp8_autocast(enabled=False, calibrating=True), \\\n",
    "    torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    model.train()\n",
    "    run_forward_pass(model, hyperparams, num_iters=512)\n",
    "\n",
    "# Compute scale_fwd with enabled fp8 autocast\n",
    "with te.fp8_autocast(enabled=True), \\\n",
    "    torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    run_forward_pass(model, hyperparams, 10)\n",
    "\n",
    "# Some parameters are in pointing to the same tensors, we do not want to double save them.\n",
    "dict_to_save = {k: v for k, v in model.state_dict().items() \\\n",
    "                if (\"_context_phase\" not in k and \"_generation_phase\" not in k)}\n",
    "torch.save(dict_to_save, '<calibrated_weights_path>') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcd135",
   "metadata": {},
   "source": [
    "#### Generation in FP8\n",
    "\n",
    "Now we are ready to run FP8 inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a913f54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Generation example 1 ==============================\n",
      "Tell me something about GPUs:\n",
      "\n",
      "* What is a GPU?\n",
      "* What is a GPU used for?\n",
      "* What is a GPU used for in machine learning?\n",
      "* What is a GPU used for in deep learning?\n",
      "* What is a GPU used for in computer vision\n",
      "============================== Generation example 2 ==============================\n",
      "Tell me something about NVIDIA:\n",
      "\n",
      "NVIDIA Corporation is an American multinational technology company headquartered in Santa Clara, California, that designs graphics processing units (GPUs) for the gaming and professional markets, as well as system on a chip units (SoCs) for the mobile computing and automotive market\n",
      "============================== Benchmarking ==============================\n",
      "Benchmarking for batch_size = 64 and max total tokens = 1024\n",
      "Time: 19.32 s.\n",
      "Peak GPU memory usage: 63.82 GB\n"
     ]
    }
   ],
   "source": [
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "restart_jupyter_notebook()\n",
    "\n",
    "from utils import *\n",
    "\n",
    "hyperparams.model_name = \"\"   # <== Add model weight location here e.g. \"/path/to/downloaded/gemma/weights\"\n",
    "hyperparams.qkv_format = \"thd\"\n",
    "hyperparams.fuse_qkv_params = True # This is needed by the last improvement.\n",
    "\n",
    "hyperparams.fp8 = True \n",
    "# We load calibrated fp8 weights directly from the file.\n",
    "hyperparams.fp8_model_weights_filename = \"<calibrated_weights_path>\"\n",
    "\n",
    "hyperparams.generation_cuda_graphs = True\n",
    "hyperparams.cuda_graphs_static_batch_size = 64\n",
    "hyperparams.cuda_graphs_static_max_seq_len = 1024\n",
    "hyperparams.cuda_graphs_static_max_context_len = 128\n",
    "model = init_te_gemma_model(hyperparams)\n",
    "\n",
    "print_sample_of_generated_texts(model)\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbb56c",
   "metadata": {},
   "source": [
    "One can observe that the outputs are coherent; however, the generation time has increased. Why is this the case?\n",
    "\n",
    "Running the model in FP8 does not imply that all weights are stored in FP8. By default, they are stored in higher precision and are cast to FP8, using saved scaling factors, before operations such as GEMMs.\n",
    "\n",
    "This approach is beneficial during training: one can perform one cast for both backward and forward passes, leading to speedups. However, performing a single cast for each forward pass introduces too much overhead to achieve a speedup. This issue will be addressed in the next section of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3945e3",
   "metadata": {},
   "source": [
    "### Use of only FP8 model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0cba9",
   "metadata": {},
   "source": [
    "TransformerEngine stores parameters in higher precision and only casts them to FP8. It may be necessary to maintain accucacy during training. However, we can get rid of high precision weights when doing inference. \n",
    "\n",
    "Transformer Engine supports maintaining only FP8 weights with `fp8_model_init` decorator. Let's see an example\n",
    "```\n",
    "with te.fp8_model_init(enabled=True):\n",
    "    linear = te.Linear((1024, 1024)) # this module is initialized only with fp8 weights\n",
    "```\n",
    "\n",
    "Let's run the code with `fp8_model_init`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96264b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Generation example 1 ==============================\n",
      "Tell me something about GPUs:\n",
      "\n",
      "* What is a GPU?\n",
      "* What is a GPU used for?\n",
      "* What is a GPU used for in machine learning?\n",
      "* What is a GPU used for in deep learning?\n",
      "* What is a GPU used for in computer vision\n",
      "============================== Generation example 2 ==============================\n",
      "Tell me something about NVIDIA:\n",
      "\n",
      "NVIDIA Corporation is an American multinational technology company headquartered in Santa Clara, California, that designs graphics processing units (GPUs) for the gaming and professional markets, as well as system on a chip units (SoCs) for the mobile computing and automotive market\n",
      "============================== Benchmarking ==============================\n",
      "Benchmarking for batch_size = 64 and max total tokens = 1024\n",
      "Time: 12.18 s.\n",
      "Peak GPU memory usage: 56.60 GB\n"
     ]
    }
   ],
   "source": [
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "restart_jupyter_notebook()\n",
    "\n",
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "hyperparams.model_name = \"\" # <== Add model weight location here e.g. \"/path/to/downloaded/gemma/weights\"\n",
    "hyperparams.fuse_qkv_params = True # Needed for fp8_model_init().\n",
    "hyperparams.qkv_format = \"thd\"\n",
    "\n",
    "hyperparams.fp8 = True\n",
    "hyperparams.fp8_model_init = True # This will result in storing only fp8 weights.\n",
    "hyperparams.fp8_model_weights_filename = \"/root/model_calibrated_weights.pth\"\n",
    "\n",
    "hyperparams.generation_cuda_graphs = True\n",
    "hyperparams.cuda_graphs_static_batch_size = 64\n",
    "hyperparams.cuda_graphs_static_max_seq_len = 1024\n",
    "hyperparams.cuda_graphs_static_max_context_len = 128\n",
    "model = init_te_gemma_model(hyperparams)\n",
    "\n",
    "print_sample_of_generated_texts(model)\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30ca5a",
   "metadata": {},
   "source": [
    "| Models                                                      | Time | Speedup |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | 82.04      | 1                         |\n",
    "| THD attention with TE                                               | 28.19      | 2.91                         | \n",
    "| THD attention +  Cuda Graphs with TE                                               | 16.81      | 4.88                         |  \n",
    "| THD attention + FP8 with TE + fp8_model_init()                                             | 12.18      | 6.74                         |  \n",
    "\n",
    "We finally obtained the **6.74x** speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e87275",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2452d",
   "metadata": {},
   "source": [
    "In this tutorial, we've explored three features of the Transformer Engine:\n",
    "1. Support for the THD attention layout,\n",
    "2. Integration with CUDA Graphs,\n",
    "3. FP8 weights calibration,\n",
    "4. Models containing only FP8 version of their parameters.\n",
    "\n",
    "Each of these features can be applied in various contexts, and here we demonstrated their use for achieving fast token generation. It's important to note that the fastest possible inference speeds can be achieved using NVIDIA's inference-optimized [TensorRT](https://developer.nvidia.com/tensorrt) library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
