{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8581f0e4",
   "metadata": {},
   "source": [
    "# CUDA Graphs, THD Attention, and FP8 Weight Calibration\n",
    "\n",
    "In tutorials such as [Llama](../te_llama/tutorial_accelerate_hf_llama_with_te.ipynb) and [Gemma](./tutorial_accelerate_hf_gemma_with_te.ipynb), we've demonstrated how transformer models can be accelerated using the Transformer Engine's `TransformerLayer`. This tutorial introduces a few more advanced features:\n",
    "1. THD attention layout.\n",
    "2. FP8 weight calibration - enabling inference in FP8 precision for models originally trained in higher precisions.\n",
    "3. CUDA Graphs API.\n",
    "We will explore how these features enhance the performance of the Gemma model during generation tasks.\n",
    "\n",
    "#### Benchmarking\n",
    "\n",
    "We'll evaluate the generation time across three benchmarks:\n",
    "- Long input sequences (up to 256 tokens) with short generation (up to 128 tokens),\n",
    "- Short input sequences (up to 64 tokens) with long generation (up to 1000 tokens).\n",
    "\n",
    "All benchmarks are conducted with a batch size of 64 using the dataset \"timdettmers/openassistant-guanaco\".\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note</b>\n",
    "    \n",
    "This tutorial focuses on showcasing the mentioned features of Transformer Engine in the context of generation. It's important to note, however, that NVIDIA provides another library, [TensorRT](https://developer.nvidia.com/tensorrt), which is optimized for inference tasks and should be considered for such use cases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f91a9",
   "metadata": {},
   "source": [
    "## Dependencies for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5201d77",
   "metadata": {},
   "source": [
    "Following files and media are necessary to effectively run this tutorial:\n",
    "\n",
    "1. `te_gemma.py`\n",
    "    - This file contains the code to load a Hugging Face Gemma checkpoint in Transformer Engine's `TransformerLayer` instead of Hugging Face's `GemmaDecoderLayer`. It does also contain code for generation with THD attention and weight calibration.\n",
    "2. `utils.py`\n",
    "    - This file contains the code related to dataloading, hyperparameters, setting up model/optimizers/accelerator, model training and other miscellaneous tasks like restarting the jupyter notebook from within the cell. \n",
    "3. `media/`\n",
    "    - This directory contains the images used in the following tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfabbf",
   "metadata": {},
   "source": [
    "## [Baseline] Running Hugging Face generation with Gemma model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59560bff",
   "metadata": {},
   "source": [
    "Hugging Face Transformers library offers generation API. We will treat this as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "\n",
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://llama.meta.com/llama-downloads/\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"  # <== Add model weight location here e.g. \"/path/to/downloaded/llama/weights\"\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_baseline_model(hyperparams).cuda()\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"Some random initial str \", \"Another string ... \"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for text in generated_texts:\n",
    "    print(text)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3698dc6",
   "metadata": {},
   "source": [
    "We put these times into the table for later comparison.\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf3d47",
   "metadata": {},
   "source": [
    "## [Improvement 1] Speeding up generation by using Transformer Engine with THD attention\n",
    "\n",
    "Similarly to the Gemma tutorial, we substitute `GemmaDecoderLayer` with `TransformerLayer` from Transformer Engine. \n",
    "\n",
    "Input sequences can have various lengths. The most common approach is to use the padding and attention masks in such situation. We will use more straightforward method - using the THD attention layout with offests. \n",
    "\n",
    "<center>\n",
    "<span style=\"display: flex; flex-direction: row; justify-content: center\">\n",
    "<span style=\"display: flex; flex-direction: column; align-items: center\">\n",
    "Query layer   \n",
    "<img src=\"./media/pic1.png\" alt=\"Logo Pythona\" height=\"200\">\n",
    "</span>\n",
    "<span style=\"display: flex; flex-direction: column; align-items: center\">\n",
    "Key layer and value layer  \n",
    "<img src=\"./media/pic2.png\" alt=\"Logo Pythona\" height=\"200\">\n",
    "</span>\n",
    "</span>\n",
    "cu_seqlens_q = [0, 1, 3, 7, 9, 12] <br>\n",
    "cu_seqlens_kv = [0, 1, 3, 6, 8, 10] <br>\n",
    "seq_offsets_q = [0, 5, 10, 15, 20, 25] * h * d <br>\n",
    "seq_offsets_k = [0, 7, 14, 21, 28, 35] * h * d <br>\n",
    "seq_offsets_v = [0, 7, 14, 21, 28, 35] * h * d <br>\n",
    "</center>\n",
    "\n",
    "The class `transformer_engine.DotProductAttention` supports this format. One need to pass the following things as the arguments to the forward:\n",
    "- `seq_offsets_q`, `seq_offsets_k`, `seq_offsets_v` - which represents the offsets of the beginnings of the next sequences,\n",
    "- `cu_seqlens_q`, `cu_seqlens_kv` - cumulative sum of the lengths of the sequences of query and values,\n",
    "- `max_seqlen_q` - maximum sequence length in query layer,\n",
    "- `max_seqlen_kv` - maximum sequence length in key-value layer.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "Currently, the THD attention for `TransformerLayer` is supported only for inference.\n",
    "</div>\n",
    "\n",
    "Let's look how using TransformerEngine with THD attention impacts the speed of generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://llama.meta.com/llama-downloads/\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"  # <== Add model weight location here e.g. \"/path/to/downloaded/llama/weights\"\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "hyperparams.fuse_qkv_params = False\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_te_gemma_model(hyperparams).cuda()\n",
    "\n",
    "model = model.to(torch.bfloat16).cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"I love when \", \"I \"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "# Method .generate is overriden in the file te_gemma.py - look there for the implementation.\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=40\n",
    ")\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for text in generated_texts:\n",
    "    print(text)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e397a65",
   "metadata": {},
   "source": [
    "By using THD attention we obtained following speedups:\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b171a0",
   "metadata": {},
   "source": [
    "## [Improvement 2] Running generation in FP8 of the model trained in higher precision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80288b",
   "metadata": {},
   "source": [
    "We are now preparing to execute FP8 generation using the Gemma model. However, this process is not straightforward. Since the model was originally trained with BF16 precision, the FP8 scaling factors have not been computed. Operating the model at such low precision without the correct scaling could result in significant numerical errors, which in turn would produce incorrect results.\n",
    "\n",
    "We highly recommend familiarizing yourself with the [tutorial](../../examples/fp8_primer.ipynb) on FP8 precision to understand the necessity of scaling.\n",
    "\n",
    "##### Weight Calibration\n",
    "\n",
    "To address the issue outlined above, we will implement weight calibration. This involves running several forward iterations at BF16 precision within the context `te.fp8_autocast(enabled=False, calibration=True)`. This setup allows the forward pass to operate at higher precision, while we simultaneously collect `amax_history` and other parameters related to the FP8 precision, which is essential for calculating the FP8 scaling factors.\n",
    "\n",
    "The code below outlines the steps to initialize the BF16 model and conduct several forward iterations within the specified context. After these iterations, we save the model, and these weights will be utilized in subsequent chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecee0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and methods\n",
    "import transformer_engine.pytorch as te\n",
    "from utils import *\n",
    "import torch\n",
    "\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=False).cuda()\n",
    "model = model.to(torch.bfloat16)\n",
    "accelerator = Accelerator(\n",
    "        log_with=\"wandb\",\n",
    "        gradient_accumulation_steps=hyperparams.gradient_accumulation_steps,\n",
    "        mixed_precision=hyperparams.mixed_precision\n",
    "    )\n",
    "train_dataloader = get_dataloaders(accelerator, hyperparams)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "\n",
    "print(\"Calibration started\")\n",
    "with te.fp8_autocast(enabled=False, calibrating=True):\n",
    "    model.train()\n",
    "    train_dataloader = enumerate(train_dataloader)\n",
    "\n",
    "    for i in range(100):\n",
    "        step, batch = next(train_dataloader)\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].cuda()\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=10\n",
    "        )\n",
    "        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(\"calibration_finished\")\n",
    "\n",
    "print(\"scale_fwd computation started\")\n",
    "with te.fp8_autocast(enabled=True):\n",
    "    for i in range(10):\n",
    "        step, batch = next(train_dataloader)\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].cuda()\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=1\n",
    "        )\n",
    "print(\"scale_fwd_computation ended\")\n",
    "\n",
    "print(\"Casting weights...\")\n",
    "model_fp8 = init_te_gemma_model(hyperparams, fp8_model_init=True).cuda()\n",
    "model_fp8.load_state_dict(model.state_dict())\n",
    "print(\"Weights casted\")\n",
    "\n",
    "print(\"Saving model...\")\n",
    "torch.save(model_fp8.state_dict(), 'model_fp8_state_dict.pth')\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcd135",
   "metadata": {},
   "source": [
    "#### Generation in FP8\n",
    "\n",
    "Now we are ready to run FP8 inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "import transformer_engine.pytorch as te\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from utils import *\n",
    "from transformer_engine.common.recipe import Format, DelayedScaling\n",
    "\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=True, qkv_format=\"thd\").cuda()\n",
    "\n",
    "print(\"Loading model\")\n",
    "model_state_dict = torch.load('model_fp8_state_dict.pth')\n",
    "model.load_state_dict(model_state_dict)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"Some random initial str \", \"Another string ... \"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "fp8_format = Format.HYBRID\n",
    "fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=32, amax_compute_algo=\"max\")\n",
    "torch.manual_seed(1234)\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "    with autocast(dtype=torch.bfloat16, cache_enabled=False):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=40,\n",
    "                use_cuda_graphs=False\n",
    "            )\n",
    "\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for text in generated_texts[:2]:\n",
    "    print(\"-\" * 50)\n",
    "    print(text)\n",
    "\n",
    "benchmark_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbb56c",
   "metadata": {},
   "source": [
    "We add the speedups to the table:\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         | \n",
    "| THD attention + FP8 with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a89d9c",
   "metadata": {},
   "source": [
    "## [Improvement 3] Speeding up generation with CUDA Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d53e7b",
   "metadata": {},
   "source": [
    "The speed of the GPU is increasing at very fast pace. It turns out that sometimes kernels runtime is shorter that time it takes CPU to submit them. It can result in serious overhead as we can see at the two pictures below.\n",
    "\n",
    "<center>\n",
    "<img src=\"./media/pic2.png\" alt=\"Logo Pythona\" height=\"200\">\n",
    "<br>\n",
    "Generation without CUDA Graphs\n",
    "<br>\n",
    "\n",
    "<img src=\"./media/pic2.png\" alt=\"Logo Pythona\" height=\"200\">\n",
    "<br>\n",
    "Generation with CUDA Graphs\n",
    "</center>\n",
    "\n",
    "CUDA Graphs were developed to address this issue. When certain kernels are executed repeatedly, this tool enables us to record and replay them without CPU involvement.\n",
    "\n",
    "We recommend reading further about CUDA Graphs [here](https://developer.nvidia.com/blog/cuda-graphs/).\n",
    "\n",
    "PyTorch supports CUDA Graphs through the `torch.cuda` API. However, there are specific requirements for a sequence of tensor operations to be captured and replayed correctly. Specifically, all operations must be static, meaning that tensors should not change locations between iterations.\n",
    "\n",
    "PyTorch also provides a simpler method for utilizing CUDA Graphs: the `torch.cuda.make_graphed_callables`. This allows easy recording of any PyTorch module. Starting from version 1.5, the Transformer Engine also supports the `make_graphed_callables` API. Below is the code that executes the generate method from `te_gemma.py`, which is responsible for creating the graphed part:\n",
    "\n",
    "```\n",
    "graphed_generator = TeGraphed(...)\n",
    "(...)\n",
    "    if use_cuda_graphs:\n",
    "        fp8_format = Format.HYBRID\n",
    "        fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=32, amax_compute_algo=\"max\")\n",
    "        graphed_layers = te.pytorch.make_graphed_callables(\n",
    "                graphed_generator, \n",
    "                args, \n",
    "                fp8_enabled=True, \n",
    "                fp8_recipe=fp8_recipe, \n",
    "                allow_unused_input=True,\n",
    "                num_warmup_iters=3\n",
    "            )\n",
    "            \n",
    "    for i in range(max_new_tokens):\n",
    "        next_tokens = graphed_layers(*args) if use_cuda_graphs else graphed_generator(*args)\n",
    "        output_tokens.append(next_tokens.clone())\n",
    "```\n",
    "If you want to use CUDA Graphs with the Transformer Engine (TE), we recommend looking into the `TeGraphed` class. This class is similar to `TEGemmaDecoderLayer`, but it includes specific functionalities required to make CUDA Graphs work effectively.\n",
    "\n",
    "Now, let's proceed to measure the speedup provided by CUDA Graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "\n",
    "import transformer_engine.pytorch as te\n",
    "from torch.cuda.amp import autocast\n",
    "from utils import *\n",
    "from transformer_engine.common.recipe import Format, DelayedScaling\n",
    "\n",
    "\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"\n",
    "hyperparams.fuse_qkv_params = True\n",
    "model = init_te_gemma_model(hyperparams, fp8_model_init=True, qkv_format=\"thd\").cuda()\n",
    "\n",
    "print(\"Loading model\")\n",
    "model_state_dict = torch.load('model_fp8_state_dict.pth')\n",
    "model.load_state_dict(model_state_dict)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"Some random initial str \", \"Another string ... \"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "fp8_format = Format.HYBRID\n",
    "fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=32, amax_compute_algo=\"max\")\n",
    "torch.manual_seed(1234)\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "    with autocast(dtype=torch.bfloat16, cache_enabled=False):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                use_cuda_graphs=True\n",
    "            )\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for text in generated_texts[:12]:\n",
    "    print(\"-\" * 50)\n",
    "    print(text)\n",
    "\n",
    "benchmark_generation(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb430f",
   "metadata": {},
   "source": [
    "We finally obtained the **??%** speedup.\n",
    "\n",
    "| Models                                                      | max_input_len=64, max_new_tokens=1000 | max_input_len=128, max_new_tokens=128 |  \n",
    "|-------------------------------------------------------------|---------------------------------------|--------------------------------------|\n",
    "| HF (baseline)                                               | -      | -                         |\n",
    "| THD attention with TE                                               | -      | -                         | \n",
    "| THD attention + FP8 with TE                                               | -      | -                         |  \n",
    "| THD attention + FP8 + Cuda Graphs with TE                                               | -      | -                         |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e87275",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2452d",
   "metadata": {},
   "source": [
    "In this tutorial, we've explored three features of the Transformer Engine:\n",
    "1. Support for the THD attention layout,\n",
    "2. FP8 weights calibration,\n",
    "3. Integration with CUDA Graphs.\n",
    "\n",
    "Each of these features can be applied in various contexts, and here we demonstrated their use for achieving fast inference. However, it's important to note that this isn't the fastest possible method for performing inference. For achieving optimal speed, we recommend exploring NVIDIA's [TensorRT](https://developer.nvidia.com/tensorrt) library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
