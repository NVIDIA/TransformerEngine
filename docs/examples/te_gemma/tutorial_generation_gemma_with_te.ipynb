{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cac9d39",
   "metadata": {},
   "source": [
    "# Accelerating a Hugging Face Gemma model generation with Transformer Engine\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Goal</b>\n",
    "\n",
    "This tutorial showcases how to accelerate generation done by a full Gemma model from [Hugging Face](https://huggingface.co/google/gemma-7b-it) by using `TransformerLayer` from the [Transformer Engine library](https://github.com/NVIDIA/TransformerEngine) in `BF16` precision.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f7fb1",
   "metadata": {},
   "source": [
    "## Dependencies for this tutorial\n",
    "\n",
    "Following files and media are necessary to effectively run this tutorial:\n",
    "\n",
    "1. `te_gemma.py`\n",
    "    - This file contains the code to load a Hugging Face Gemma checkpoint in Transformer Engine's `TransformerLayer` instead of Hugging Face's `LlamaDecoderLayer`. Also it contains the logic of the generation using TransformerEngine. \n",
    "2. `utils.py`\n",
    "    - This file contains the code related to dataloading, hyperparameters, setting up model/optimizers/accelerator, model training and other miscellaneous tasks like restarting the jupyter notebook from within the cell. \n",
    "3. `media/`\n",
    "    - This directory contains the images used in the following tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564503c",
   "metadata": {},
   "source": [
    "## Baseline HuggingFace Gemma generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8d0a5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "    \n",
    "This tutorial loads and trains a Llama 2 7B model which takes up most of the GPU memory and therefore, we need to restart the jupyter notebook each time before running the following sections. A small utility method `restart_jupyter_notebook` is defined in the accompanying `utils.py` file. This function restarts the jupyter notebook so that the GPU memory is flushed before the model is loaded again from the checkpoint in order to avoid running into OOM (Out Of Memory) errors.\n",
    "\n",
    "If the utility doesn't work, comment this line `restart_jupyter_notebook()` in the following cell and manually restart the jupyter notebook before running the cell. Repeat the same for other sections in this tutorial.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36ff380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 26.482454538345337 seconds\n",
      "I like the new look of the app. I like the new features. I like the new look of \n",
      "==============================\n",
      "I do not like the way the new version of the app is set up. I do not like the fa\n"
     ]
    }
   ],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "\n",
    "\n",
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "import torch\n",
    "\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://llama.meta.com/llama-downloads/\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"  # <== Add model weight location here e.g. \"/path/to/downloaded/llama/weights\"\n",
    "hyperparams.mixed_precision = \"no\"\n",
    "\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_baseline_model(hyperparams).cuda()\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"I like\", \"I do not like\"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=400\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Generation time: {duration} seconds\")\n",
    "\n",
    "\n",
    "# Decode the output tensor to text\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Display the first two samples of the generated text\n",
    "print(generated_texts[0][:80])\n",
    "print(30 * \"=\")\n",
    "print(generated_texts[1][:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f0f33",
   "metadata": {},
   "source": [
    "Let's add this information in a table and keep comparing it with a few possible improvements in future sections:\n",
    "\n",
    "| Models                                                      | Precision | Generation time | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 26.48                       | 1                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb88e9",
   "metadata": {},
   "source": [
    "## [Improvement] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` and use generation within TE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fefac",
   "metadata": {},
   "source": [
    "```\n",
    "@torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        max_new_tokens = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        num_heads = self.model.config.num_attention_heads\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        max_seq_len = seq_len + max_new_tokens\n",
    "        generation_config, _ = self._prepare_generation_config(generation_config, **kwargs)\n",
    "        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        # inference_params object is a cache, where keys and values of previous tokens are stored\n",
    "        inference_params = te.pytorch.InferenceParams(\n",
    "            max_batch_size=batch_size, \n",
    "            max_sequence_length=seq_len+max_new_tokens+1) \n",
    "\n",
    "        # mask has shape [batch_size, num_heads, 1, max_seq_len] and contains False \n",
    "        # when coressponding token is padding and True otherwise.\n",
    "        pad_attention_mask = input_ids.ne(generation_config.pad_token_id)\n",
    "        mask = torch.ones((batch_size, num_heads, 1, max_seq_len), dtype=torch.bool).cuda()\n",
    "        mask[..., :seq_len] = mask[..., :seq_len] & pad_attention_mask.unsqueeze(1).unsqueeze(2).expand(-1, num_heads, -1, -1)\n",
    "\n",
    "        hidden_states = self.model.embed_tokens(input_ids)\n",
    "        output_tokens = []\n",
    "        for i in range(max_new_tokens):\n",
    "            normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n",
    "            hidden_states = hidden_states * normalizer\n",
    "            for decoder_layer in self.model.layers:\n",
    "                hidden_states = decoder_layer(\n",
    "                            hidden_states,\n",
    "                            # In the case of arbiutrary mask, the meaning of True and False is switched, so negation is needed.\n",
    "                            attention_mask=pad_attention_mask if i == 0 else ~mask[..., :seq_len],\n",
    "                            self_attn_mask_type=\"padding_causal\" if i == 0 else \"arbitrary\",\n",
    "                            inference_params=inference_params\n",
    "                        )[0]\n",
    "\n",
    "            # inference_params.sequence_len_offset should contain position of the current token in the sequence.\n",
    "            inference_params.sequence_len_offset += hidden_states.shape[1]\n",
    "\n",
    "            hidden_states = self.model.norm(hidden_states)\n",
    "            logits = self.lm_head(hidden_states)\n",
    "            logits = logits.float()\n",
    "            logits = logits[:, -1, :]\n",
    "            next_tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Sequences, which are finished should contain padding - taken from huggingface transformers.\n",
    "            next_tokens = next_tokens * unfinished_sequences + generation_config.pad_token_id * (1 - unfinished_sequences)\n",
    "            output_tokens.append(next_tokens)\n",
    "\n",
    "            unfinished_sequences = unfinished_sequences & ~(next_tokens == generation_config.eos_token_id)\n",
    "\n",
    "            hidden_states = self.model.embed_tokens(next_tokens).unsqueeze(1)\n",
    "            seq_len += 1\n",
    "\n",
    "        result = torch.cat((input_ids, torch.stack(output_tokens).permute([1, 0])), dim=1)\n",
    "        return result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2b752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 16.87099289894104 seconds\n",
      "I like the idea of a \"re-do\" of the original \"The Man from U.N.C.L.E.\" movie. I \n",
      "==============================\n",
      "I do not like the way the \"new\" (2011) version of the 1099-MISC is set up.  I ha\n"
     ]
    }
   ],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "#restart_jupyter_notebook()\n",
    "\n",
    "\n",
    "# Import necessary packages and methods\n",
    "from utils import *\n",
    "import accelerate\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "## Weights can be downloaded from: https://llama.meta.com/llama-downloads/\n",
    "hyperparams.model_name = \"../../../../gemma-weights\"  # <== Add model weight location here e.g. \"/path/to/downloaded/llama/weights\"\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_te_gemma_model(hyperparams)\n",
    "#accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "model = model.to(torch.bfloat16).cuda()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.model_name)\n",
    "inputs = tokenizer([\"I like\", \"I do not like\"] * 32, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=400\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Generation time: {duration} seconds\")\n",
    "\n",
    "\n",
    "# Decode the output tensor to text\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Display the first two samples of the generated text\n",
    "print(generated_texts[0][:80])\n",
    "print(30 * \"=\")\n",
    "print(generated_texts[1][:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec126c",
   "metadata": {},
   "source": [
    "| Models                                                      | Precision | Generation time | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 26.48                         | 1                       |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | BF16      | 16.87                         | 1.56                    |\n",
    "\n",
    "\n",
    "\n",
    "After converting to TE Transformer Layers, we obtained the speedup of **56%**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b80b0f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using `TransformerLayer` module from Transformer Engine as a substitute for Hugging Face's `LlamaDecoderLayer` provides a speedup over Hugging Face's native Gemma generation implementation. `TransformerLayer` provides a speedup over the baseline implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
