{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48b8ac0",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "## Overview\n",
    "\n",
    "Transformer Engine (TE) is a library for accelerating Transformer models on NVIDIA GPUs, providing better performance with lower memory utilization in both training and inference. It provides support for 8-bit floating point (FP8) precision on Hopper GPUs, implements a collection of highly optimized building blocks for popular Transformer architectures, and exposes an automatic-mixed-precision-like API that can be used seamlessly with your PyTorch code. It also includes a framework-agnostic C++ API that can be integrated with other deep learning libraries to enable FP8 support for Transformers.\n",
    "\n",
    "This is specifically a guide for getting started to use Transformer Engine with JAX. We recommend you to try understanding the basics of JAX first, using these resources:\n",
    "- Thinking in JAX: https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html\n",
    "- JAX 101: https://docs.jax.dev/en/latest/jax-101.html\n",
    "- Key concepts in JAX: https://docs.jax.dev/en/latest/key-concepts.html#jax-arrays-jax-array\n",
    "- Flax 101: https://flax-linen.readthedocs.io/en/latest/guides/flax_fundamentals/index.html\n",
    "\n",
    "If you do not wish to learn/use JAX, there is another guide in this same director, called quickstart.ipynb, that is to get started with PyTorch.\n",
    "\n",
    "## Let's build a Transformer layer (*)!\n",
    "<small>(*) _This was based upon the GPT decoder layer, but for the sake of simplicity and mirroring the PyTorch tutorial whose defaults are without any attention mask, we are setting attention mask here also to 0, making the attention basically an encoder, which does not exist in the GPT architecture. However, since the code support any attention mask here in the TransformerLayer (later in this guide), we will leave it to the audience to try experimenting with different attention masks._</small>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "We build a basic Transformer layer using regular JAX modules. This will be our baseline for later comparisons with Transformer Engine.\n",
    "\n",
    "</div>\n",
    "\n",
    "Let's start with creating the transformer layer using plain JAX/Flax. Figure 1 shows the overall structure.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"transformer_layer.png\" width=\"20%\">\n",
    "<figcaption> Figure 1: Structure of a GPT encoder layer.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We construct the components as follows:\n",
    "\n",
    "- `LayerNorm`: `nn.LayerNorm` (JAX/Flax)\n",
    "- `QKV Projection`: `nn.Dense` (conceptually three `Dense` layers for Q, K, and V separately, but we fuse into a single `Dense` layer that is three times larger)\n",
    "- `DotProductAttention`: `DotProductAttention` from [quickstart_jax_utils.py](quickstart_jax_utils.py)\n",
    "- `Projection`: `nn.Dense` (JAX/Flax)\n",
    "- `Dropout`: `nn.Dropout` (JAX/Flax)\n",
    "- `MLP`: `BasicMLP` from [quickstart_jax_utils.py](quickstart_jax_utils.py)\n",
    "\n",
    "Over the course of this tutorial we will use a few modules and helper functions defined in [quickstart_jax_utils.py](quickstart_jax_utils.py). Putting it all together:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "881fd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from datasets import load_dataset\n",
    "except ModuleNotFoundError:\n",
    "    %pip install --quiet datasets\n",
    "    from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5284a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import quickstart_jax_utils as utils\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4d1cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTransformerLayer(nn.Module):\n",
    "    \"\"\"Basic Transformer layer using plain JAX/Flax modules\n",
    "    \n",
    "    This is the JAX/Flax equivalent of the PyTorch BasicTransformerLayer\n",
    "    from the quickstart.ipynb notebook.\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden_size: int\n",
    "    ffn_hidden_size: int\n",
    "    num_attention_heads: int\n",
    "    layernorm_eps: float = 1e-5\n",
    "    attention_dropout: float = 0.1\n",
    "    hidden_dropout: float = 0.1\n",
    "    \n",
    "    def setup(self):\n",
    "        self.kv_channels = self.hidden_size // self.num_attention_heads\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, \n",
    "        x: jnp.ndarray, \n",
    "        attention_mask: Optional[jnp.ndarray] = None,\n",
    "        deterministic: bool = False\n",
    "    ) -> jnp.ndarray:\n",
    "        res = x\n",
    "        x = nn.LayerNorm(epsilon=self.layernorm_eps)(x)\n",
    "        \n",
    "        # Fused QKV projection\n",
    "        qkv = nn.Dense(features=3 * self.hidden_size, use_bias=True)(x)\n",
    "        qkv = qkv.reshape(qkv.shape[0], qkv.shape[1], self.num_attention_heads, 3 * self.kv_channels)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=3)  # qkv.shape[3] = 3?\n",
    "        \n",
    "        # Attention self-implemented. Comment out if not used\n",
    "        attention = utils.DotProductAttention(\n",
    "            num_attention_heads=self.num_attention_heads,\n",
    "            kv_channels=self.kv_channels,\n",
    "            attention_dropout=self.attention_dropout,\n",
    "        )\n",
    "        x = attention(q, k, v, attention_mask, deterministic=deterministic)\n",
    "        \n",
    "        # Attention built-in. Comment out if not used\n",
    "        # attention = flax.nnx.MultiheadAttention(\n",
    "        #     num_heads=self.num_attention_heads,\n",
    "        #     in_features=self.hidden_size,\n",
    "        #     qkv_features=self.kv_channels,\n",
    "        #     dropout_rate=self.attention_dropout,\n",
    "        #     deterministic=True\n",
    "        # )\n",
    "        # x = attention(q, k, v, attention_mask, deterministic=deterministic)\n",
    "\n",
    "        # Projection and dropout\n",
    "        x = nn.Dense(features=self.hidden_size, use_bias=True)(x)\n",
    "        x = nn.Dropout(rate=self.hidden_dropout)(x, deterministic=deterministic)\n",
    "        x = res + x\n",
    "        \n",
    "        # Second residual connection\n",
    "        res = x\n",
    "        x = nn.LayerNorm(epsilon=self.layernorm_eps)(x)\n",
    "        \n",
    "        # MLP\n",
    "        mlp = utils.BasicMLP(\n",
    "            hidden_size=self.hidden_size,\n",
    "            ffn_hidden_size=self.ffn_hidden_size,\n",
    "        )\n",
    "        x = mlp(x)\n",
    "        \n",
    "        return x + res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69efd5",
   "metadata": {},
   "source": [
    "That's it! We now have a simple Transformer layer in JAX/Flax. Let's test it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3510b",
   "metadata": {},
   "source": [
    "## Testing Performance\n",
    "\n",
    "Now let's test the performance of our BasicTransformerLayer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b44649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer configuration\n",
    "hidden_size = 4096\n",
    "sequence_length = 2048\n",
    "batch_size = 4\n",
    "ffn_hidden_size = 16384\n",
    "num_attention_heads = 32\n",
    "dtype = jnp.bfloat16\n",
    "\n",
    "# Synthetic data\n",
    "key, dropout_key = jax.random.split(jax.random.PRNGKey(42))\n",
    "x = jax.random.normal(key, (sequence_length, batch_size, hidden_size)).astype(dtype)\n",
    "dy = jax.random.normal(key, (sequence_length, batch_size, hidden_size)).astype(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e44ed26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Flax BasicTransformerLayer initialized successfully!\n",
      "Parameter shapes: {'params': {'BasicMLP_0': {'Dense_0': {'bias': (16384,), 'kernel': (4096, 16384)}, 'Dense_1': {'bias': (4096,), 'kernel': (16384, 4096)}}, 'Dense_0': {'bias': (12288,), 'kernel': (4096, 12288)}, 'Dense_1': {'bias': (4096,), 'kernel': (4096, 4096)}, 'LayerNorm_0': {'bias': (4096,), 'scale': (4096,)}, 'LayerNorm_1': {'bias': (4096,), 'scale': (4096,)}}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BasicTransformerLayer\n",
    "basic_transformer = BasicTransformerLayer(\n",
    "    hidden_size=hidden_size,\n",
    "    ffn_hidden_size=ffn_hidden_size,\n",
    "    num_attention_heads=num_attention_heads,\n",
    ")\n",
    "\n",
    "# Initialize parameters\n",
    "params = basic_transformer.init(key, x, attention_mask=None, deterministic=False)\n",
    "\n",
    "print(\"Pure Flax BasicTransformerLayer initialized successfully!\")\n",
    "print(f\"Parameter shapes: {jax.tree_util.tree_map(lambda x: x.shape, params)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de91af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2048, 4, 4096)\n",
      "Output shape: (2048, 4, 4096)\n",
      "Output dtype: float32\n",
      "Forward pass completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "y = basic_transformer.apply(params, x, attention_mask=None, deterministic=True)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output dtype: {y.dtype}\")\n",
    "print(\"Forward pass completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "037bc8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 28.229827880859375 ms\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import quickstart_jax_utils\n",
    "importlib.reload(quickstart_jax_utils)\n",
    "\n",
    "utils.speedometer(\n",
    "    model_apply_fn=basic_transformer.apply,\n",
    "    variables=params,  # Ensure the correct `params` is passed\n",
    "    input=x,\n",
    "    output_grad=dy,\n",
    "    dropout_key=dropout_key,\n",
    "    forward_kwargs={\"attention_mask\": None, \"deterministic\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb16f31",
   "metadata": {},
   "source": [
    "## Meet Transformer Engine\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "Now that we have a basic Transformer layer in JAX/Flax, let's use Transformer Engine to speed up the training. The following examples show how to use TE modules.\n",
    "\n",
    "</div>\n",
    "\n",
    "The JAX/Flax BasicTransformerLayer above is equivalent to the PyTorch version in the main quickstart.ipynb notebook. It uses:\n",
    "\n",
    "- `nn.LayerNorm`: JAX/Flax LayerNorm\n",
    "- `nn.Dense`: JAX/Flax Dense layer for QKV projection  \n",
    "- `DotProductAttention`: Custom attention from [quickstart_jax_utils.py] (**)(quickstart_jax_utils.py)\n",
    "- `nn.Dense`: JAX/Flax Dense layer for projection\n",
    "- `nn.Dropout`: JAX/Flax Dropout\n",
    "- `BasicMLP`: Custom MLP from [quickstart_jax_utils.py](quickstart_jax_utils.py)\n",
    "\n",
    "<small> (**) _The code below also shows how to use the built-in attention sub-layer from either pure Flax or TE Flax in commented code if you wish to use those instead of the custom attention in [quickstart_jax_utils.py]. The implementation is there for your reference of how attention is roughly implemented in our source_</small>\n",
    "\n",
    "Below we show how to use Transformer Engine JAX/Flax modules for better performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bed20d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_engine.jax as te\n",
    "import transformer_engine.jax.flax as te_flax\n",
    "from transformer_engine.jax.quantize import is_fp8_available, ScalingMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28cb444",
   "metadata": {},
   "source": [
    "TE provides a set of JAX modules that can be used to build Transformer layers. The simplest of the provided modules are the `Linear ` and `LayerNorm` layers, which we can use instead of `flax.linen.Linear` and ` flax.linen.LayerNorm`. Let's modify our `BasicTransformLayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56105579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicTEMLP(nn.Module):\n",
    "    hidden_size : int\n",
    "    ffn_hidden_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = te_flax.DenseGeneral(features=self.ffn_hidden_size, use_bias=True) (x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = te_flax.DenseGeneral(features=self.hidden_size, use_bias=True) (x)\n",
    "        return x\n",
    "\n",
    "class BasicTETransformerLayer(nn.Module):\n",
    "    hidden_size: int\n",
    "    ffn_hidden_size: int \n",
    "    num_attention_heads: int  \n",
    "    layernorm_eps: float = 1e-5\n",
    "    attention_dropout: float = 0.1 \n",
    "    hidden_dropout: float = 0.1\n",
    "\n",
    "    def setup(self):\n",
    "        self.kv_channels = self.hidden_size // self.num_attention_heads\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, \n",
    "        x: jnp.ndarray,\n",
    "        attention_mask: Optional[jnp.ndarray] = None,\n",
    "        deterministic: bool = False\n",
    "    ) -> jnp.ndarray:\n",
    "        res = x\n",
    "        x = te_flax.LayerNorm(epsilon=self.layernorm_eps)(x)\n",
    "\n",
    "        # Fused QKV projection\n",
    "        qkv = te_flax.DenseGeneral(features=3 * self.hidden_size, use_bias=True)(x)\n",
    "        qkv = qkv.reshape(qkv.shape[0], qkv.shape[1], self.num_attention_heads, 3 * self.kv_channels)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=3)\n",
    "\n",
    "        attention = utils.DotProductAttention(\n",
    "            num_attention_heads=self.num_attention_heads,\n",
    "            kv_channels=self.kv_channels,\n",
    "            attention_dropout=self.attention_dropout,\n",
    "        )\n",
    "        x = attention(q, k, v, attention_mask, deterministic=deterministic)\n",
    "        \n",
    "        # If you'd like to use the built-in Attention layer from JAX/Flax, uncomment the below\n",
    "        # If used, please remove the subsequent Dense layer that is projecting\n",
    "        # the concatenated QKVoutput to hidden size.\n",
    "        # attention = flax.nnx.MultiheadAttention(\n",
    "        #     num_heads=self.num_attention_heads,\n",
    "        #     qkv_features=self.kv_channels,\n",
    "        #     dropout_rate=self.attention_dropout,\n",
    "        #     attention_mask_type='no_mask'\n",
    "        # )\n",
    "        # x = attention(q, k, v, attention_mask, deterministic=deterministic)\n",
    "\n",
    "        # If you'd like to use the built-in Attention layer from TE JAX, uncomment the below\n",
    "        # If used, please remove the subsequent Dense layer that is projecting \n",
    "        # the concatenated QKVoutput to hidden size.\n",
    "        # attention = te_flax.MultiHeadAttention(\n",
    "        #     num_attention_heads = self.num_attention_heads,\n",
    "        #     head_dim=self.kv_channels,\n",
    "        #     attention_dropout=self.attention_dropout,\n",
    "        #     attention_mask_type='no_mask',\n",
    "        # )\n",
    "\n",
    "         # x = attention(q, k, v, attention_mask, deterministic=deterministic)\n",
    "\n",
    "        # Projection concatenated QKVoutput back to hidden size.\n",
    "        # Delete this if use the buiil-in MultiheadAttention module from either flax or te_flax\n",
    "        x = te_flax.DenseGeneral(features=self.hidden_size, use_bias=True)(x)\n",
    "        x = nn.Dropout(rate=self.hidden_dropout)(x, deterministic=deterministic)\n",
    "        x = res + x\n",
    "\n",
    "        # Second residual connection\n",
    "        res = x\n",
    "        x = te_flax.LayerNorm(epsilon=self.layernorm_eps)(x)\n",
    "\n",
    "        # MLP\n",
    "        mlp = BasicTEMLP(\n",
    "            hidden_size=self.hidden_size,\n",
    "            ffn_hidden_size=self.ffn_hidden_size\n",
    "        )\n",
    "\n",
    "        x = mlp(x)\n",
    "\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5146cd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 17.390952110290527 ms\n"
     ]
    }
   ],
   "source": [
    "import quickstart_jax_utils\n",
    "importlib.reload(quickstart_jax_utils)\n",
    "\n",
    "basic_te_transformer = BasicTETransformerLayer(\n",
    "    hidden_size, \n",
    "    ffn_hidden_size, \n",
    "    num_attention_heads,\n",
    ")\n",
    "\n",
    "te_params = basic_te_transformer.init(key, x, attention_mask=None, deterministic=False)\n",
    "\n",
    "# Test forward pass\n",
    "y = basic_te_transformer.apply(te_params, x, attention_mask=None, deterministic=True)\n",
    "\n",
    "utils.speedometer(\n",
    "    model_apply_fn=basic_te_transformer.apply,\n",
    "    variables=te_params,  # Ensure the correct `params` is passed\n",
    "    input=x,\n",
    "    output_grad=dy,\n",
    "    dropout_key=dropout_key,\n",
    "    forward_kwargs={\"attention_mask\": None, \"deterministic\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801b201",
   "metadata": {},
   "source": [
    "\n",
    "## Fused TE Modules\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "We optimize the example Transformer layer with TE modules for fused operations.\n",
    "\n",
    "</div>\n",
    "\n",
    "The `DenseGeneral` layer is enough to build any Transformer model and it enables usage of Transformer Engine even for very custom Transformers. However, having more knowledge about the model allows for additional optimizations like kernel fusion, increasing the achievable speedup.\n",
    "\n",
    "Transformer Engine therefore provides coarser modules that span multiple layers:\n",
    "\n",
    "* `LayerNormDenseGeneral`\n",
    "* `LayerNormMLP`\n",
    "* `TransformerLayer`\n",
    "\n",
    "To see a complete list of all the functions TE Flax support, you can view it here: https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/jax.html#modules\n",
    "\n",
    "Building a third iteration of our Transformer layer with `LayerNormDenseGeneral` and `LayerNormMLP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11203785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedTETransformerLayer(nn.Module):\n",
    "    hidden_size: int\n",
    "    ffn_hidden_size: int \n",
    "    num_attention_heads: int  \n",
    "    layernorm_eps: float = 1e-5\n",
    "    attention_dropout: float = 0.1 \n",
    "    hidden_dropout: float = 0.1\n",
    "\n",
    "    def setup(self):\n",
    "        self.kv_channels = self.hidden_size // self.num_attention_heads\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, \n",
    "        x: jnp.ndarray,\n",
    "        attention_mask: Optional[jnp.ndarray] = None,\n",
    "        deterministic: bool = False\n",
    "    ) -> jnp.ndarray:\n",
    "\n",
    "        res = x\n",
    "\n",
    "        # Fused QKV projection\n",
    "        qkv, x_norm = te_flax.LayerNormDenseGeneral(features=3 * self.hidden_size, epsilon=self.layernorm_eps, use_bias=True)(x)\n",
    "\n",
    "        qkv = qkv.reshape(qkv.shape[0], qkv.shape[1], self.num_attention_heads, 3 * self.kv_channels)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=3)\n",
    "\n",
    "        attention = utils.DotProductAttention(\n",
    "            num_attention_heads=self.num_attention_heads,\n",
    "            kv_channels=self.kv_channels,\n",
    "            attention_dropout=self.attention_dropout,\n",
    "        )\n",
    "        x = attention(q, k, v, attention_mask, deterministic=deterministic)\n",
    "\n",
    "        # If you'd like to use the built-in Attention layer from TE JAX, uncomment the below\n",
    "        # If used, please remove the subsequent Dense layer that is projecting \n",
    "        # the concatenated QKVoutput to hidden size.\n",
    "        # attention = te_flax.MultiHeadAttention(\n",
    "        #     num_attention_heads = self.num_attention_heads,\n",
    "        #     head_dim=self.kv_channels,\n",
    "        #     attention_dropout=self.attention_dropout,\n",
    "        #     attention_mask_type='no_mask',\n",
    "        # )\n",
    "        # x = attention(q, k, v, attention_mask, deterministic=deterministic)\n",
    "\n",
    "        # Projection concatenated QKVoutput back to hidden size.\n",
    "        # Delete this if use the buiil-in MultiheadAttention module from either flax or te_flax\n",
    "        x = te_flax.DenseGeneral(features=self.hidden_size, use_bias=True)(x)\n",
    "        x = nn.Dropout(rate=self.hidden_dropout)(x, deterministic=deterministic)\n",
    "        x = res + x\n",
    "\n",
    "        # Second residual connection\n",
    "        res = x\n",
    "        x,_ = te_flax.LayerNormMLP(intermediate_dim=self.ffn_hidden_size, \n",
    "                                 epsilon=self.layernorm_eps,\n",
    "                                 use_bias=True\n",
    "                                 )(x, deterministic=deterministic)\n",
    "\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "114de14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_te_transformer = FusedTETransformerLayer(\n",
    "    hidden_size, \n",
    "    ffn_hidden_size, \n",
    "    num_attention_heads\n",
    ")\n",
    "\n",
    "fused_te_params = fused_te_transformer.init(key, x, attention_mask=None, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b0c705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 18.087706565856934 ms\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "y = fused_te_transformer.apply(fused_te_params, x, attention_mask=None, deterministic=True)\n",
    "\n",
    "utils.speedometer(\n",
    "    model_apply_fn=fused_te_transformer.apply,\n",
    "    variables=fused_te_params,  # Ensure the correct `params` is passed\n",
    "    input=x,\n",
    "    output_grad=dy,\n",
    "    dropout_key=dropout_key,\n",
    "    forward_kwargs={\"attention_mask\": None, \"deterministic\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c12c8",
   "metadata": {},
   "source": [
    "Finally, the `TransformerLayer` module is convenient for creating standard Transformer architectures and it provides the highest degree of performance optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7496b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_transformer = te_flax.TransformerLayer(\n",
    "    hidden_size=hidden_size,\n",
    "    mlp_hidden_size=ffn_hidden_size, \n",
    "    num_attention_heads=num_attention_heads,\n",
    "    mlp_activations=(\"gelu\",),\n",
    "    self_attn_mask_type='no_mask',\n",
    "    layernorm_epsilon=1e-5,\n",
    "    use_bias=True\n",
    "    )\n",
    "\n",
    "te_transformer_params = te_transformer.init(key, x, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ec0f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 12.37576961517334 ms\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "y = te_transformer.apply(te_transformer_params, x, attention_mask=None, deterministic=True)\n",
    "\n",
    "utils.speedometer(\n",
    "    model_apply_fn=te_transformer.apply,\n",
    "    variables=te_transformer_params,  # Ensure the correct `params` is passed\n",
    "    input=x,\n",
    "    output_grad=dy,\n",
    "    dropout_key=dropout_key,\n",
    "    forward_kwargs={\"attention_mask\": None, \"deterministic\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a101d3",
   "metadata": {},
   "source": [
    "## Enabling FP8\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "We configure a TE module to perform compute in FP8.\n",
    "\n",
    "</div>\n",
    "\n",
    "Enabling FP8 support is very simple in Transformer Engine. We just need to wrap the modules within an [fp8_autocast](.../api/jax.rst#transformer_engine.jax.fp8_autocast) context manager. Note that fp8_autocast should only be used to wrap the forward pass and must exit before starting a backward pass. See the [FP8 tutorial](fp8_primer.ipynb) (currently only available in PyTorch) for a detailed explanation of FP8 recipes and the supported options.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Important: FP8 Metadata Initialization</b>\n",
    "\n",
    "When using FP8, the model **must be initialized within the `fp8_autocast` context**. This creates a special collection called `fp8_metas` that contains scaling factors and other metadata required for FP8 computation. If you initialize a model outside of `fp8_autocast` and then try to use it with FP8, you will get a `ScopeCollectionNotFound` error because the `fp8_metas` collection was never created.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2aaa8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_engine.common.recipe import Format, DelayedScaling\n",
    "\n",
    "te_transformer = te_flax.TransformerLayer(\n",
    "    hidden_size=hidden_size,\n",
    "    mlp_hidden_size=ffn_hidden_size, \n",
    "    num_attention_heads=num_attention_heads,\n",
    "    mlp_activations=(\"gelu\",),\n",
    "    self_attn_mask_type='no_mask',\n",
    "    layernorm_epsilon=1e-5,\n",
    "    use_bias=True\n",
    ")\n",
    "\n",
    "fp8_format = Format.HYBRID\n",
    "fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=16, amax_compute_algo=\"max\")\n",
    "\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "    te_transformer_params = te_transformer.init(key, x, deterministic=False)\n",
    "    y = te_transformer.apply(te_transformer_params, x, attention_mask=None, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9cdbf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 7.956786155700684 ms\n"
     ]
    }
   ],
   "source": [
    "utils.speedometer(\n",
    "    model_apply_fn=te_transformer.apply,\n",
    "    model_init_fn=te_transformer.init,\n",
    "    variables=te_transformer_params,  # Includes both params and fp8_metas\n",
    "    input=x,\n",
    "    output_grad=dy,\n",
    "    dropout_key=dropout_key,\n",
    "    forward_kwargs={\"attention_mask\": None, \"deterministic\": False},\n",
    "    fp8_autocast_kwargs = { \"enabled\": True, \"fp8_recipe\": fp8_recipe }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
