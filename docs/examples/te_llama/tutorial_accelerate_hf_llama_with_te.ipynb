{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3d41f2",
   "metadata": {},
   "source": [
    "# Accelerate HF Llama model with TransformerEngine\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Goal</b>\n",
    "\n",
    "This tutorial showcases three incrementally efficient ways to use [TransformerEngine library](https://github.com/NVIDIA/TransformerEngine) to finetune (full) a Llama2 model from [Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-hf).\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d32532",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "    \n",
    "This tutorial showcases finetuning a full 7B [Llama2 model](https://huggingface.co/meta-llama/Llama-2-7b-hf) on h100 GPUs (which have 80GB of HBM). Therefore, running the following individual portions (Baseline, Improvement 1, Improvement 2 and Improvement 3) for perf benchmarking will require restarting the Jupyter notebook kernel each time.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3da0e",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. From \"Transformer\" to \"Llama\"\n",
    "2. Hugging Face's `LlamaModel`\n",
    "    - Hugging Face's `LlamaDecoderLayer`\n",
    "3. TransformerEngine's `TransformerLayer`\n",
    "    - `TransformerLayer` options explained\n",
    "4. Necessary Imports\n",
    "5. [Baseline] Running HF `LlamaModel` (Precision: `BF16`)\n",
    "6. [Improvement 1] Replace `nn.Linear` with TE's `Linear` layers (Precision: FP8)\n",
    "7. [Improvement 2] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `BF16`)\n",
    "8. [Improvement 3] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `FP8`)\n",
    "9. Benchmarks revisited and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14656fdd",
   "metadata": {},
   "source": [
    "## From \"Transformer\" to \"Llama\" \n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/transformer_llama.png\" width=\"50%\">\n",
    "    <figcaption> Fig 1: Llama visualized as a transformer. (generated with <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/sdxl\">Nvidia's AI-foundation models</a>)</figcaption>\n",
    "</figure>\n",
    "\n",
    "A flashback:\n",
    "- 2017: [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) paper introduced pioneering \"Transformer\" architecture and changed the NLP field forever.\n",
    "- 2018-2020: Emergence of GPT model series that showed causal decoder architectures are great fit for pretraining, few-shot and zero-shot learning.\n",
    "- Fast forward to 2023-2024: Following GPT-3/GPT-4 success stories, researchers and companies raced to produce the next best pretrained model that could further be finetuned for application-specific use-cases. \n",
    "- One of the latest in this line of pretrained models which is also open source is Meta's [Llama 2](https://llama.meta.com/llama2) models (Large Language Model Meta AI). \n",
    "    - These models range from 7B to 65B parameters.\n",
    "    - LLaMA 2 was pretrained on 2 trillion tokens.\n",
    "\n",
    "For more information on Llama 2 consider reading the [Huggingface tutorial](https://huggingface.co/blog/llama2). As a quick summary, here are some of the important differences b/w the conventional transformer decoder architecture vs Llama2 architecture:\n",
    "\n",
    "1. Decoder only model (causal language modeling and next word prediction)\n",
    "2. RMSNorm in place of the LayerNorm\n",
    "3. SwiGLU activation function\n",
    "4. RoPE as positional embeddings \n",
    "5. Grouped Query Attention\n",
    "6. Trained on 4K context length\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/transformers_vs_llama.png\" width=\"100%\">\n",
    "    <figcaption> Fig 2: Comparing conventional Transformer architecture with Llama architecture. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c57f2",
   "metadata": {},
   "source": [
    "## Hugging Face's `LlamaModel`\n",
    "Hugging Face provides an open-source implementation of `Llama` model in [`modeling_llama.py`](https://github.com/huggingface/transformers/blob/3d2900e829ab16757632f9dde891f1947cfc4be0/src/transformers/models/llama/modeling_llama.py#L4).\n",
    "\n",
    "Here's a block diagram that shows how Llama model is implemented in the Hugging Face repo. Notice the modular encapsulated form and `LlamaDecoderLayer` at the core of the model implementation. This core layer chunk is targeted for optimizations in a couple of the improvements later in this tutorial (Improvement 2 and Improvement 3). \n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/llama_for_causal_lm.png\" width=\"40%\">\n",
    "    <figcaption> Fig 3: Causal Llama Model Block Diagram. </figcaption>\n",
    "</figure>\n",
    "\n",
    "The above diagram translates to the following text output of the model in pytorch. Notice that the core of the model has 32 `LlamaDecoderLayer`s. \n",
    "\n",
    "```\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "    (layers): ModuleList(\n",
    "      (0-31): 32 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaFlashAttention2(\n",
    "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (rotary_emb): LlamaRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm()\n",
    "        (post_attention_layernorm): LlamaRMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "#### HF `LlamaDecoderLayer`\n",
    "\n",
    "Let's take a closer look at `LlamaDecoderLayer`. It's composed of `input_layernorm`, `self_attn`, `post_attention_layernorm` and `mlp` modules. Each module has associated weights as shown in the diagram.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/llama_zoom.png\" width=\"70%\">\n",
    "    <figcaption> Fig 4: Causal Llama Model Block Diagram (with simplified illustration of the <a href=\"https://github.com/huggingface/transformers/blob/e770f0316d2a9b787c9d1440f204fcb65e176682/src/transformers/models/llama/modeling_llama.py#L695\">LlamaDecoderLayer</a>). </figcaption>\n",
    "</figure>\n",
    "\n",
    "##### Self_Attn Layer\n",
    "For simplicity in the block diagram illustration of the \"self_attn\" box, we omit the \"Grouped Query Attention\" operation and only showcase the modules which have associated weights.\n",
    "   \n",
    "##### MLP Layer\n",
    "\n",
    "SwiGLU is an activation defined as follows in the [modeling_llama.py](https://github.com/huggingface/transformers/blob/7c4995f93d8d24aae05e1e43279c96dce736e5c8/src/transformers/models/llama/modeling_llama.py#L236) file in the Hugging Face github repo:\n",
    "```\n",
    "down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "```\n",
    "It requires a set of 3 weights as compared to 2 weights in conventional \"MLP\" layers e.g. in the traditional transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae844f",
   "metadata": {},
   "source": [
    "## [Baseline] Running HF `LlamaModel` (Precision: `BF16`)\n",
    "\n",
    "Llama2 weights are loaded into the Hugging Face native implementation `LlamaForCausalLM` (refer to [modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)). \n",
    "\n",
    "`batch_size` is `8` and precision is `BF16`\n",
    "\n",
    "The `LlamaDecoderLayer` is left unchanged in the baseline as follows:\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/llamadecoderlayer.png\" width=\"30%\">\n",
    "    <figcaption> Fig 5: Revisiting \"LlamaDecoderLayer\". </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ddb92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_warn_always(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56195c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]\n",
      "Map: 100%|██████████| 9846/9846 [00:00<00:00, 12152.20 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 finetuning steps complete!\n",
      "Average time taken per step: 289 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "restart_jupyter_notebook()\n",
    "\n",
    "\n",
    "# Minimize the bloat by wrapping all the imports in a function and return\n",
    "from utils import *\n",
    "\n",
    "\n",
    "## Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "hyperparams.model_name = \"\" # <== Add model weight location here\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "\n",
    "## Init the model and accelerator wrapper\n",
    "model = init_baseline_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "## Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36075341",
   "metadata": {},
   "source": [
    "Let's add this information in a table and keep comparing it with a few possible improvements in future sections:\n",
    "\n",
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 289                         | 1                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273932e3",
   "metadata": {},
   "source": [
    "## [Improvement 1] Replace `nn.Linear` with TE's `Linear` layers (Precision: `FP8`)\n",
    "\n",
    "[Hugging Face accelerate](https://github.com/huggingface/accelerate) provides a [`convert_model`](https://github.com/huggingface/accelerate/blob/97d2168e5953fe7373a06c69c02c5a00a84d5344/src/accelerate/utils/transformer_engine.py#L24) method to replace `torch.nn.Linear` layers with `transformer_engine.pytorch.module.Linear` layers. The following diagram illustrates this visually.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/llamadecoderlayer_replace_with_telinear.png\" width=\"70%\">\n",
    "    <figcaption> Fig 6: Replacing \"nn.Linear\" with \"TE.Linear\". </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "This is the most straightforward way to use TransformerEngine's `FP8` precision during training/finetuning for HF Llama model. Notice that the entire `LlamaDecoderLayer` is mostly left unchanged, it's only the `nn.Linear` layers that get replaced with `TE.Linear` layers. \n",
    "\n",
    "#### How to run the model in `FP8` precision\n",
    "\n",
    "After the substition, these layers can be run in `FP8` precision by the following change over the previous BF16 runs. (For more information, refer the corresponding `te_llama.py` file in this tutorial, especially the `wrap_with_accelerator` function).\n",
    "\n",
    "```\n",
    "# Specify the `FP8RecipeKwargs` (additional argument required to run in `fp8` precision)\n",
    "fp8_kwarg_handler = [FP8RecipeKwargs(backend=\"te\")]\n",
    "\n",
    "# Pass the `FP8RecipeKwargs` to the `Accelerator` init call\n",
    "accelerator = Accelerator(\n",
    "    ...\n",
    "    kwargs_handlers=fp8_kwarg_handler\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0e6ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 finetuning steps complete!\n",
      "Average time taken per step: 310 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "restart_jupyter_notebook()\n",
    "\n",
    "\n",
    "# Minimize the bloat by wrapping all the imports in a function and return\n",
    "from utils import *\n",
    "\n",
    "\n",
    "## Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "hyperparams.model_name = \"\" # <== Add model weight location here\n",
    "hyperparams.mixed_precision = \"fp8\"\n",
    "\n",
    "\n",
    "## Init the model and accelerator wrapper\n",
    "model = init_baseline_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "## Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39d951",
   "metadata": {},
   "source": [
    "Based on the above run, the performance of the baseline implementation is as follows:\n",
    "\n",
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 289                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 310                         | _**0.93**_                 \n",
    "\n",
    "The performance with TE `Linear` layers has actually decreased by a factor of **0.93** (or **7% slower**). Let's try to understand the reason of the slow down. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbfced",
   "metadata": {},
   "source": [
    "### Understanding the aberration with Improvement 2 (CPU Overheads)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "\n",
    "<a href=\"https://developer.nvidia.com/nsight-systems\">NVIDIA Nsight Systems</a> allows taking a closer look at the underlying CPU and GPU activity of an application or workload. This process is called \"profiling\" and the graphical visualization produced after running this process is called a \"profile\".\n",
    "\n",
    "In the following explanation, profiles are annotated manually to allow the reader to better comprehend the time spent in various computations. \n",
    "\n",
    "</div>\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_bf16.png\" width=\"100%\">\n",
    "    <figcaption> Fig 7: Profile of HF Llama baseline implementation. (Top) A cross-section of the CPU/GPU activity showing a single transformer layer (mainly \"SelfAttention\" and \"MLP\" modules) forward. (Middle) An emphasized cross-section of a portion of the \"SelfAttention\" module that shows the \"Q\", \"K\" and \"V\" projection operations (basically `nn.Linear` layers). (Bottom) A further emphasized cross-section of only the \"Q\" projection operation which shows the corresponding CPU and GPU activity for a single `nn.Linear` layer. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_fp8.png\" width=\"100%\">\n",
    "    <figcaption> Fig 8: Profile of HF Llama with \"Improvement 1\" (replacing `nn.Linear` layers with TE's `Linear` layers) implementation. (Top) A cross-section of the CPU/GPU activity showing a single transformer layer (mainly \"MultiheadAttention\" and \"LayerNormMLP\" modules) forward. (Middle) An emphasized cross-section of a portion of the \"MultiheadAttention\" module that shows the Q, K and V projection operations (basically TE's `Linear` layers). (Bottom) A further emphasized cross-section of only the Q projection operation which shows the CPU and GPU activity for a single TE's `Linear` layer.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<div class=\"alert alert-light\">\n",
    "\n",
    "<b>Insight</b>\n",
    "    \n",
    "In the profiles above, whenever the GPU activity is absent, it generally indicates that the GPU is waiting for CPU to dispatch a kernel i.e. the GPU is idle and waiting for more work. In general, we'd want the GPU to be active as much as possible and wait less for the CPU.\n",
    "    \n",
    "One thing clearly noticeable is that the GPU kernels in the \"baseline\" implementation occupy GPU more of the time than the \"Improvement 1\" (replacing `nn.Linear` with TE's `Linear`) implementation. Let's dig into why that is the case.\n",
    "\n",
    "</div>\n",
    "\n",
    "To simplify the information from the above profiles, consider the following table that compares the two implementations:\n",
    "\n",
    "|                                                     |              |                        |              |                   | Baseline (microseconds) | Improvement 1 (replace `nn.Linear` with TE's `Linear`)(microseconds) | Speedup |\n",
    "|-----------------------------------------------------|--------------|------------------------|--------------|-------------------|-------------------------|----------------------------------------------------------------------|---------|\n",
    "| Single transformer layer forward (\"attn\" and \"mlp\") |              |                        |              |                   | 2443                    | 6299                                                                 | -       |\n",
    "|                                                     | \"attn\" layer |                        |              |                   | 1470                    | 3842                                                                 | -       |\n",
    "|                                                     |              | Q, K and V projections |              |                   | 326                     | 2027                                                                 | -       |\n",
    "|                                                     |              |                        | Q projection |                   | 117                     | 1001                                                                 | -       |\n",
    "|                                                     |              |                        |              | Amax/Scale update | -                       | 72                                                                   | -       |\n",
    "|                                                     |              |                        |              | Buffer allocation | -                       | 49                                                                   | -       |\n",
    "|                                                     |              |                        |              | Cast+Transpose    | -                       | 35 (10 + 25)                                                         | -       |\n",
    "|                                                     |              |                        |              | MatMul            | 106                     | 52                                                                   | **2.03x**   |\n",
    "|                                                     | \"mlp\" layer  |                        |              |                   | 790                     | 2057                                                                 | -       |\n",
    "\n",
    "\n",
    "Now let's make a few observations:\n",
    "\n",
    "1. For a single transformer layer, Improvement 1 implementation (`nn.Linear` layers replaced with TE's `Linear` layers and with FP8 precision) takes more time than the baseline implementation (BF16 precision). \n",
    "2. If we keep zooming in the profile to individual \"attn\" (`SelfAttention` for baseline and `MultiheadAttention` for Improvement 1) or \"mlp\" (`MLP` for baseline or `LayerNormMLP` for Improvement 1) layers, the trend is similar that Improvement 1 is slower than the baseline implementation.\n",
    "3. At its core the `Linear` layers in Improvement 1 are slower than the `nn.Linear` layers in the baseline implementation (1001 microseconds vs 117 microseconds, i.e. _slower by a factor of **8.5x**_)\n",
    "\n",
    "#### Why is TE's `Linear` slower than `nn.Linear` layer?\n",
    "\n",
    "If we look closely, TE's `Linear` layer contains more kernels for the following tasks: \n",
    "1. Amax and scale update \n",
    "2. FP8 weights and transpose buffer allotment\n",
    "3. Cast+Transpose kernels for inputs and weights (to cast BF16 inputs and weights to their FP8 counterparts)\n",
    "4. Matrix Multiplication kernel (in FP8 precision)\n",
    "\n",
    "While the GPU is idle, it's usually waiting for CPU to finish doing its work and dispatch a kernel that can run on the GPU! Further, all those kernels are pretty short for the workload for the current finetuning tutorial (`batch_size=8`, `max_seq_length=256`). Therefore, overall the time taken by the TE's `Linear` layer is **1001 microseconds**. \n",
    "\n",
    "Compare this to `nn.Linear` layer (in Fig: ???) which contains only a Matrix Multiplication kernel (in BF16 precision). Almost all of the work inside the linear layer is running that kernel. Further, as the GPU doesn't have to wait for the CPU for more kernels, the `nn.Linear` layer itself takes less time to run, only **117 microseconds**, almost a tenth of the fraction of TE's `Linear` layer. \n",
    "\n",
    "<div class=\"alert alert-light\">\n",
    "\n",
    "<b>Insight</b>\n",
    "    \n",
    "Note that in Improvement 1, the Matrix Multiplication operation in FP8 precision itself is faster than the Matrix Multiplication in BF16 precision in the baseline implementation!\n",
    "    \n",
    "What if we could force the GPU to spend more time in Matrix Multiplication?\n",
    "\n",
    "</div>\n",
    "\n",
    "#### How to make TE's `Linear` faster than `nn.Linear` layer?\n",
    "\n",
    "As we noted earlier, the workload for the current finetuning tutorial (`batch_size=8`, `max_seq_length=256`) is small and therefore isn't able to fully utilize the capability of TE's `Linear` layers.\n",
    "\n",
    "_Generally, we'd want to increase the workload so that GPU is active more of the time than the CPU._\n",
    "\n",
    "As a small experiment, let's see how the profiles look like for the \"Q\" projection operation when we increase the `batch_size` from `8` to `128`. Since this will result in GPU running of memory, let's simultaneously decrease the size of our model from `32` layers to just `4` (i.e. `config=num_hidden_layers=4`).\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_bf16_bs_128.png\" width=\"100%\">\n",
    "    <figcaption> Fig 9: (baseline implementation) Profile of the \"Q\" projection, especially the `nn.Linear` layer when batch_size=128 and num_hidden_layers=4. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_fp8_bs_128.png\" width=\"100%\">\n",
    "    <figcaption> Fig 10: (Improvement 1 implementation) Profile of the \"Q\" projection, especially the TE's `Linear` layer when  batch_size=128 and num_hidden_layers=4. </figcaption>\n",
    "</figure>\n",
    "\n",
    "As is visible in the figures above, the \"Q Projection\" with TE's `Linear` layer (**928 microseconds**) is faster as compared to the case with `nn.Linear` layer (**1356 microseconds**). It is mainly due to the following reasons:\n",
    "1. The FP8 Matrix Multiplication (**691 microseconds**) in the TE's `Linear` layer is faster than the BF16 Matrix Multiplication (**1353 microseconds**) in the `nn.Linear` layer case. That's **almost 2x** speedup. Further, the FP8 Matrix Multiplication also takes more time as compared to the smaller tutorial workload (**52 microseconds**) i.e. more time is spent in the GPU instead of the CPU.\n",
    "2. Other kernels in the TE's `Linear` layer (especially the transposes) also take more time, but the speedup provided by the Matrix Multiplication more than compensates for this increased time, thereby providing an overall speedup of **46%** (**1356** for `nn.Linear` vs **928** for TE `Linear`)!\n",
    "\n",
    "<div class=\"alert alert-light\">\n",
    "<b>Insight</b>\n",
    "    \n",
    "As the workload is increased, more time is spend in the GPU as compared to CPU and the speedup provided by FP8 Matrix Multiplication dominates the time spent in CPU and other GPU kernels (e.g. cast transposes).\n",
    "\n",
    "In the above example, `batch_size` was increased to increase the workload size. Another way is to increase the `sequence_length` which will also have a similar effect of increasing the workload size!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8d1a8",
   "metadata": {},
   "source": [
    "## [Improvement 2] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `BF16`)\n",
    "\n",
    "In addition to basic layers like `Linear` and `LayerNorm`, TransformerEngine offers larger modules like `MultiheadAttention` (combines \"LayerNorm\" and \"Self Attention\") and `LayerNormMLP` (combines \"LayerNorm\" and \"MLP\") that could replace their counterparts in the `LlamaDecoderLayer` and potentially provide more speedup. Further, TransformerEngine also offers a full `TransformerLayer` (which further combines `MultiheadAttention` and `LayerNormMLP` layers) which could be substituted for `LlamaDecoderLayer` (with careful mapping of the weights since the name of the weights are different for those two layers). Let's take a closer look at TransformerEngine's `TransformerLayer`. \n",
    "\n",
    "### TransformerEngine's `TransformerLayer`\n",
    "\n",
    "At a higher level, TE's `TransformerLayer` could be visualized as an apt replacement for the `LlamaDecoderLayer`. But the internals of the `TransformerLayer` are organized a bit differently. \n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/tellamadecoderlayer.png\" width=\"30%\">\n",
    "    <figcaption> Fig 11: TransformerEngine's `TransformerLayer` </figcaption>\n",
    "</figure>\n",
    "\n",
    "Just like Hugging Face's `LlamaDecoderLayer`, TransformerEngine's `TransformerLayer` encapsulates `self_attention` (as `MultiheadAttention`) and `mlp` (as `LayerNormMLP`). A major difference is that the two `Norm`s are included in the `MultiheadAttention` and `LayerNormMLP` layers as shown in the following output prompt:\n",
    "\n",
    "```\n",
    "TransformerLayer(\n",
    "    (self_attention): MultiheadAttention(\n",
    "      (layernorm_qkv): LayerNormLinear()\n",
    "      (core_attention): DotProductAttention()\n",
    "      (proj): Linear()\n",
    "    )\n",
    "    (layernorm_mlp): LayerNormMLP()\n",
    ")\n",
    "```\n",
    "\n",
    "### `TransformerLayer` options explained\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "    \n",
    "Here, we go over some of the options in `TransformerLayer` that are needed for the tutorial. For a complete list of options, refer the [TransformerLayer API documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/pytorch.html?highlight=transformerlayer#transformer_engine.pytorch.TransformerLayer).\n",
    "\n",
    "</div>\n",
    "\n",
    "In the accompanying `te_llama.py` file, `TELlamaDecoderLayer` is defined as a wrapper over TE's `TransformerLayer` with a few needed options that make `TransformerLayer` as a plug-in replacement for the HF's `LlamaDecoderLayer`.\n",
    "\n",
    "```\n",
    "class TELlamaDecoderLayer(te.pytorch.TransformerLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(\n",
    "            config.hidden_size,\n",
    "            config.intermediate_size,\n",
    "            config.num_attention_heads,\n",
    "            bias=False,\n",
    "            layernorm_epsilon=config.rms_norm_eps,\n",
    "            hidden_dropout=0,\n",
    "            attention_dropout=0,\n",
    "            fuse_qkv_params=False,\n",
    "            normalization=\"RMSNorm\",\n",
    "            activation=\"swiglu\",\n",
    "            attn_input_format=\"bshd\",\n",
    "        )\n",
    "        te_rope = RotaryPositionEmbedding(config.hidden_size//config.num_attention_heads)\n",
    "        self.te_rope_emb = te_rope(max_seq_len=config.max_position_embeddings).cuda()\n",
    "```\n",
    "\n",
    "Here's a list summarizing each option briefly:\n",
    "\n",
    "1. `hidden_size`: size of each input sample.\n",
    "2. `ffn_hidden_size`: intermediate size to which samples are projected.\n",
    "3. `num_attention_heads`: number of attention heads in the transformer layer.\n",
    "4. `bias`: switch to add additive biases to the submodule layers.\n",
    "5. `layernorm_epsilon`: a value added to the denominator of layer normalization for numerical stability. Default is `1e-5`.\n",
    "6. `hidden_dropout`: dropout probability for the dropout op after FC2 layer (fully connected layer no. 2). Default is `0.1`.\n",
    "7. `attention_dropout`: dropout probability for the dropout op during multi-head attention. Default is `0.1`. \n",
    "8. `fuse_qkv_params`:  if set to True, TransformerLayer module exposes a single fused parameter for query-key-value. This enables optimizations such as QKV fusion without concatentations/splits and also enables the argument fuse_wgrad_accumulation.\n",
    "9. `normalization`: type of normalization applied. Default is `LayerNorm`.\n",
    "10. `activation`: type of activation used in the MLP block. Default is `gelu`.\n",
    "11. `attn_input_format`: controls whether the dimensions of the intermediate hidden states is 'batch first' ('bshd') or 'sequence first' ('sbhd'). `s` stands for the sequence length, `b` batch size, `h` the number of heads, `d` head size. Note that these formats are very closely related to the `qkv_format` in the `MultiHeadAttention` and `DotProductAttention` modules. \n",
    "\n",
    "\n",
    "Further, note that `RotaryPositionEmbedding` is defined as part of the TE's `TransformerLayer` itself since it expects this rope cache if RoPE is used in the model. \n",
    "\n",
    "### Comparing Hugging Face's `LlamaDecoderLayer` with TranformerEngine's `TransformerLayer`\n",
    "\n",
    "Let's revisit how `LlamaDecoderLayer`s form the core of the decoder layer stack in HF's llama implementation:\n",
    "```\n",
    "ModuleList(\n",
    "  (0-31): 32 x LlamaDecoderLayer(\n",
    "    (self_attn): LlamaAttention(\n",
    "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (rotary_emb): LlamaRotaryEmbedding()\n",
    "    )\n",
    "    (mlp): LlamaMLP(\n",
    "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "      (act_fn): SiLU()\n",
    "    )\n",
    "    (input_layernorm): LlamaRMSNorm()\n",
    "    (post_attention_layernorm): LlamaRMSNorm()\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "A major portion of the Hugging Face model implementation (`LlamaDecoderLayer`) could be potentially replaced with TransformerEngine's implementation.\n",
    "\n",
    "\n",
    "### Mapping weights from HF's `LlamaDecoderLayer` to TE's `TransformerLayer`\n",
    "\n",
    "Refer the accompanying file `te_llama.py` which provides a reference to create a Llama 2 model with TE's `TransformerLayer` after replacing HF's `LlamaDecoderLayer`.\n",
    "\n",
    "Briefly, following pieces of code are put together:\n",
    "\n",
    "1. `TELlamaDecoderLayer` is added as a wrapper for `TransformerLayer`. \n",
    "```\n",
    "class TELlamaDecoderLayer(te.pytorch.TransformerLayer):\n",
    "    \"\"\"\n",
    "    Wrapper class over TE's `TransformerLayer`. This makes the wrapper very\n",
    "    similar to HF's `LlamaDecoderLayer` and easier to replace it in the code.\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "        args: positional args (for compatibility with `LlamaDecoderLayer`)\n",
    "        kwargs: keyword args (for compatibility with `LlamaDecoderLayer`)\n",
    "    \"\"\"\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            hidden_size=config.hidden_size,\n",
    "            ffn_hidden_size=config.intermediate_size,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            bias=False,\n",
    "            layernorm_epsilon=config.rms_norm_eps,\n",
    "            hidden_dropout=0,\n",
    "            attention_dropout=0,\n",
    "            fuse_qkv_params=False,\n",
    "            normalization=\"RMSNorm\",\n",
    "            activation=\"swiglu\",\n",
    "            attn_input_format=\"bshd\",\n",
    "        )\n",
    "        te_rope = RotaryPositionEmbedding(config.hidden_size//config.num_attention_heads)\n",
    "        self.te_rope_emb = te_rope(max_seq_len=config.max_position_embeddings).cuda()\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states,\n",
    "                *args,\n",
    "                attention_mask,\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Custom forward to make sure we only pass relevant arguments to the\n",
    "        forward pass of the `TransformerLayer`. Also, make sure the output\n",
    "        format matches the output of the HF's `LlamaDecoderLayer`.\n",
    "        \"\"\"\n",
    "        return (super().forward(hidden_states, attention_mask=attention_mask, rotary_pos_emb=self.te_rope_emb),)\n",
    "```\n",
    "\n",
    "2. Before creating a `LlamaForCausalLM`, `replace_decoder` context manager is used to monkey-patch `LlamaDecoderLayer` with `TELlamaDecoderLayer`.\n",
    "\n",
    "```\n",
    "@contextmanager\n",
    "def replace_decoder(te_decodder_cls):\n",
    "    \"\"\"\n",
    "    Replace `LlamaDecoderLayer` with custom `TELlamaDecoderLayer`.\n",
    "    \"\"\"\n",
    "    original_llama_decoder_cls = transformers.models.llama.modeling_llama.LlamaDecoderLayer\n",
    "    transformers.models.llama.modeling_llama.LlamaDecoderLayer = te_decodder_cls\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        transformers.models.llama.modeling_llama.LlamaDecoderLayer = original_llama_decoder_cls\n",
    ".\n",
    ".\n",
    ".\n",
    "class TELlamaForCausalLM:\n",
    "    \"\"\"\n",
    "    Causal LM created with `LlamaModel`. The underlying `LlamaDecoderLayer`\n",
    "    class is monkey-patched with `TELlamaDecoderLayer` class before\n",
    "    initializing the causal LM with `LlamaForCausalLM`.\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, config: LlamaConfig):\n",
    "        with replace_decoder(te_decodder_cls=TELlamaDecoderLayer):\n",
    "            llama_for_causal_lm = LlamaForCausalLM(config)\n",
    "        return llama_for_causal_lm\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "\n",
    "3. A custom `pretrained_from_local` method is added that copies the weights from the checkpoint (which is meant for HF Llama implementation) to the modified `TELlamaForCausalLM` by carefully mapping the weights from the `LlamaDecoderLayer` (HF) to `TransformerLayer` (TE). The method `replace_params` maps and copies apt weights from `LlamaDecoderLayer` to the `TransformerLayer`. Refer to the following diagram for more details.\n",
    "\n",
    "```\n",
    "def replace_params(hf_state_dict, te_state_dict):\n",
    "    # collect all layer prefixes to update\n",
    "    all_layer_prefixes = set()\n",
    "    for param_key in hf_state_dict.keys():\n",
    "        layer_prefix_pat = 'model.layers.\\d+.'\n",
    "        m = re.match(layer_prefix_pat, param_key)\n",
    "        if m is not None:\n",
    "            all_layer_prefixes.add(m.group())\n",
    "\n",
    "    for layer_prefix in all_layer_prefixes:\n",
    "        # When loading weights into models with less number of layers, skip the\n",
    "        # copy if the corresponding layer doesn't exist in TE model\n",
    "        if layer_prefix + 'self_attention.layernorm_qkv.layer_norm_weight' in te_state_dict:\n",
    "            te_state_dict[layer_prefix + 'self_attention.layernorm_qkv.layer_norm_weight'].data[:] = hf_state_dict[layer_prefix + 'input_layernorm.weight'].data[:]\n",
    "\n",
    "        if layer_prefix + 'self_attention.layernorm_qkv.query_weight' in te_state_dict:\n",
    "            te_state_dict[layer_prefix + 'self_attention.layernorm_qkv.query_weight'].data[:] = hf_state_dict[layer_prefix + 'self_attn.q_proj.weight'].data[:]\n",
    "\n",
    "        if layer_prefix + 'self_attention.layernorm_qkv.key_weight' in te_state_dict:\n",
    "            te_state_dict[layer_prefix + 'self_attention.layernorm_qkv.key_weight'].data[:] = hf_state_dict[layer_prefix + 'self_attn.k_proj.weight'].data[:]\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "\n",
    "    return all_layer_prefixes\n",
    "```\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/weight_swap.png\" width=\"70%\">\n",
    "    <figcaption> Fig 12: Replace `LlamaDecoderLayer` with `TransformerLayer`. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "After initializing the modified Llama model this way, the core decoder layers get changed to `TELlamaDecoderLayer` (wrapper around `TransformerLayer`) as shown in the following output:\n",
    "```\n",
    "ModuleList(\n",
    "  (0-31): 32 x TELlamaDecoderLayer(\n",
    "    (self_attention): MultiheadAttention(\n",
    "      (layernorm_qkv): LayerNormLinear()\n",
    "      (core_attention): DotProductAttention(\n",
    "        (flash_attention): FlashAttention()\n",
    "        (fused_attention): FusedAttention()\n",
    "        (unfused_attention): UnfusedDotProductAttention(\n",
    "          (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
    "          (attention_dropout): Dropout(p=0, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (proj): Linear()\n",
    "    )\n",
    "    (layernorm_mlp): LayerNormMLP()\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "In summary, the model gets changed as follows with a bigger chunk of the implementation (core decoder layers) coming from TransformerEngine.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/model_change.png\" width=\"80%\">\n",
    "    <figcaption> Fig 13: Language model after the HF's `LlamaDecoderLayer`s are replaced with TE's `TransformerLayer`s. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note</b>\n",
    "\n",
    "Let's first run this \"TELlama\" implementation in `BF16` precision.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a6e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 finetuning steps complete!\n",
      "Average time taken per step: 242 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "restart_jupyter_notebook()\n",
    "\n",
    "\n",
    "# Minimize the bloat by wrapping all the imports in a function and return\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "hyperparams.model_name = \"\" # <== Add model weight location here\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_te_llama_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "# Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69d7e0",
   "metadata": {},
   "source": [
    "Compared to the \"baseline\" implementation, we see that with replacing larger chunks of Transformer Engine's layers gives a speedup of **19%** even when using only BF16 precision!\n",
    "\n",
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 289                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 310                         | 0.93                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | BF16      | 242                         | 1.19                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81f916",
   "metadata": {},
   "source": [
    "## [Improvement 3] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `FP8`)\n",
    "\n",
    "Now that most of the HF Llama model implementation (`LlamaDecoderLayer`s) has been swapped with TransformerEngine implementation (`TELlamaDecoderLayer` or `TransformerLayer`), let's see how `FP8` training helps improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49d8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 finetuning steps complete!\n",
      "Average time taken per step: 231 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Restart the notebook (to flush the GPU memory)\n",
    "from utils import restart_jupyter_notebook\n",
    "restart_jupyter_notebook()\n",
    "\n",
    "\n",
    "# Minimize the bloat by wrapping all the imports in a function and return\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "hyperparams.model_name = \"\" # <== Add model weight location here\n",
    "hyperparams.mixed_precision = \"fp8\"\n",
    "\n",
    "\n",
    "## Init the model and accelerator wrapper\n",
    "model = init_te_llama_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "## Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2441c3",
   "metadata": {},
   "source": [
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 289                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 310                         | 0.93                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | BF16      | 242                         | 1.19                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | FP8       | 231                         | 1.25                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c419912",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Major takeaways from the above tutorial are:\n",
    "1. Transformer Engine offers layers with differing level of granularity \n",
    "    - Basic modules: `Linear` and `LayerNormLinear` (Also provides the core attention operation in `DotProductAttention`)\n",
    "    - Combine basic modules into larger modules - `MultiheadAttention` and `LayerNormMLP`\n",
    "    - Combine `MultiheadAttention` and `LayerNormMLP` into `TransformerLayer`, the most performance efficient implementation.\n",
    "2. Replacing basic modules like `nn.Linear` with TE's `Linear` layer should be done keeping in mind the workload size since at small workload sizes, the time spent in the CPU can dominate the time spent in the GPU which could actually result in a slowdown.\n",
    "3. Using larger `TransformerLayer` module from Transformer Engine provides speedup over Hugging Face's native Llama 2 implementation. This needs careful initializing of model such that the HF's `LlamaDecoderLayer` are substituted with TE's `TransformerLayer` and then model weights (which are meant for `LlamaDecoderLayer`) are correctly mapped to their counterparts in TE's `TransformerLayer`.\n",
    "4. NVIDIA Nsight Systems allows to peek into the CPU and GPU activity while running a workload (this tutorial in our case) and reason about making appropriate optimizations.  \n",
    "    - In this tutorial, we demonstrated that increasing the workload size makes even basic TE modules like `Linear` run faster (in FP8 precision) than `nn.Linear` (in BF16 precision)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
