{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5461b1ad",
   "metadata": {},
   "source": [
    "# Accelerate HF Llama model with Transformer-Engine\n",
    "\n",
    "Goal: This tutorial showcases three incrementally efficient ways to use [Transformer-Engine library](https://github.com/NVIDIA/TransformerEngine) to finetune (full) a Llama2 model from [HuggingFace](https://huggingface.co/meta-llama/Llama-2-7b-hf).\n",
    "\n",
    "#### Things to note about this tutorial:\n",
    "1. It showcases finetuning a full 7B Llama2 model (https://huggingface.co/meta-llama/Llama-2-7b-hf).\n",
    "2. The GPU requirements are therefore larger (this tutorial targets h100 GPUs which have 80GB of HBM). \n",
    "3. Therefore, running each following individual portions for perf benchmarking will require restarting the Jupyter notebook kernel each time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12beb80",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. From \"Transformer\" to \"Llama\"\n",
    "2. HuggingFace's `LlamaModel`\n",
    "    - HF `LlamaDecoderLayer`\n",
    "3. Transformer-Engine's `TransformerLayer`\n",
    "    - `TransformerLayer` options explained\n",
    "1. Tutorial overview and Benchmarks preview\n",
    "2. Necessary Imports\n",
    "3. Tutorial part 1\n",
    "4. Tutorial part 2\n",
    "5. Tutorial part 3\n",
    "6. Tutorial part 4\n",
    "7. Benchmarks revisited and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8d5b8",
   "metadata": {},
   "source": [
    "## From \"Transformer\" to \"Llama\" \n",
    "![Transformers to Llama](media/transformer_llama.png \"transformer llama\")\n",
    "\n",
    "A flashback:\n",
    "- 2017: [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) paper introduced pioneering \"Transformers\" architecture and changed the NLP field forever.\n",
    "- 2018-2020: Emergence of GPT model series that showed causal decoder architectures are great fit for pretraining, few-shot and zero-shot learning.\n",
    "- Fast forward to 2023-2024: Following GPT-3/GPT-4 success stories, researchers and companies raced to produce the next best pretrained model that could further be finetuned for application-specific use-cases. \n",
    "- One of the latest in this line of pretrained models which is also open source is Meta's Llama-1/Llama-2 models (Large Language Model Meta AI). \n",
    "    - These models range from 7B to 65B parameters.\n",
    "    - LLaMA-2 was pretrained on 2 trillion tokens.\n",
    "\n",
    "A lot is already available on the web about llama (we consider llama v2, [checkout meta ai for more details](https://llama.meta.com/)). A few important details are:\n",
    "1. Decoder only model (causal language modeling and next word prediction)\n",
    "2. RMSNorm in place of the LayerNorm\n",
    "3. SwiGLU activation function\n",
    "4. RoPE as positional embeddings \n",
    "5. Grouped Query Attention\n",
    "6. Trained on 4K context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a238d8a",
   "metadata": {},
   "source": [
    "## HuggingFace's `LlamaModel`\n",
    "Huggingface is the go to place for open source NLP model implementations and also has an implementation of `Llama` model in [`modeling_llama.py`](https://github.com/huggingface/transformers/blob/3d2900e829ab16757632f9dde891f1947cfc4be0/src/transformers/models/llama/modeling_llama.py#L4).\n",
    "\n",
    "Here's a block diagram that shows how Llama model is implemented in the HuggingFace repo. Notice the modular encapsulated form and `LlamaDecoderLayer` at the core of the model implementation. We will target this core layer chunk for optimizations in one of the improvements that we talk about in this tutorial. \n",
    "\n",
    "![Causal Llama Model Block Diagram](media/llama_for_causal_lm.png \"LlamaForCausalLM\")\n",
    "\n",
    "The above diagram translates to the following text output of the model in pytorch. Notice the bunch of `LlamaDecoderLayer`s at the core of the model. \n",
    "\n",
    "```\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "    (layers): ModuleList(\n",
    "      (0-31): 32 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaFlashAttention2(\n",
    "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (rotary_emb): LlamaRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm()\n",
    "        (post_attention_layernorm): LlamaRMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "#### HF `LlamaDecoderLayer`\n",
    "\n",
    "Let's take a closer look at `LlamaDecoderLayer`. It's composed of `input_layernorm`, `self_attn`, `post_attention_layernorm` and `mlp` modules. Each module has associated weights as shown in the diagram.\n",
    "\n",
    "![LlamaDecoderLayer](media/llama_zoom.png \"LlamaDecoderLayer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fff45b",
   "metadata": {},
   "source": [
    "## Transformer-Engine's `TransformerLayer`\n",
    "\n",
    "At a higher level, TE's `TransformerLayer` could be visualized as an apt replacement for the `LlamaDecoderLayer`. But the internals of the `TransformerLayer` are organized a bit differently. \n",
    "\n",
    "![TransformerLayer](media/tellamadecoderlayer.png \"TELlamaDecoderLayer\")\n",
    "\n",
    "Just like HuggingFace's `LlamaDecoderLayer`, TE's `TransformerLayer` majorly encapsulates `self_attention` and `mlp`. A major difference is that the two `Norm`s are combined with `self_attention` and `mlp` layers.\n",
    "\n",
    "```\n",
    "TransformerLayer(\n",
    "    (self_attention): MultiheadAttention(\n",
    "      (layernorm_qkv): LayerNormLinear()\n",
    "      (core_attention): DotProductAttention()\n",
    "      (proj): Linear()\n",
    "    )\n",
    "    (layernorm_mlp): LayerNormMLP()\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Just like HF's `LlamaDecoderLayer`, it is possible to create a custom model (let's call it `MyGPT`) with only `TransformerLayer`s as follows:\n",
    "```\n",
    "# Define the model with only TransformerLayer\n",
    "class MyGPT(torch.nn.Module):\n",
    "    def __init__(self, num_layers = 1, hidden_size = 128, ffn_hidden_size=512, num_attention_heads=16):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([TransformerLayer(hidden_size, ffn_hidden_size, num_attention_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "...\n",
    "# Init the model\n",
    "mygpt = MyGPT()\n",
    "...\n",
    "```\n",
    "```\n",
    "# Training loop\n",
    "for data, labels in dataset:\n",
    "    ...\n",
    "    out = mygpt(data)\n",
    "    loss = loss_fn(out, labels)\n",
    "    ...\n",
    "```\n",
    "\n",
    "### `TransformerLayer` options explained\n",
    "[Refer the [Transformer-Engine's docs](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/pytorch.html?highlight=transformerlayer#transformer_engine.pytorch.TransformerLayer) for more details]\n",
    "\n",
    "In the accompanying `te_llama.py` file, `TELlamaDecoderLayer` (a wrapper over TE's `TransformerLayer`) is defined as follows with a bunch of options:\n",
    "\n",
    "```\n",
    "class TELlamaDecoderLayer(te.pytorch.TransformerLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(\n",
    "            config.hidden_size,\n",
    "            config.intermediate_size,\n",
    "            config.num_attention_heads,\n",
    "            bias=False,\n",
    "            layernorm_epsilon=config.rms_norm_eps,\n",
    "            hidden_dropout=0,\n",
    "            attention_dropout=0,\n",
    "            fuse_qkv_params=False,\n",
    "            normalization=\"RMSNorm\",\n",
    "            activation=\"swiglu\",\n",
    "            attn_input_format=\"bshd\",\n",
    "        )\n",
    "        te_rope = RotaryPositionEmbedding(config.hidden_size//config.num_attention_heads)\n",
    "        self.te_rope_emb = te_rope(max_seq_len=config.max_position_embeddings).cuda()\n",
    "```\n",
    "\n",
    "1. `hidden_size`: size of each input sample\n",
    "2. `ffn_hidden_size`: intermediate size to which samples are projected.\n",
    "3. `num_attention_heads`: number of attention heads in the transformer layer.\n",
    "4. `bias`: switch to add additive biases to the submodule layers.\n",
    "5. `layernorm_epsilon`: a value added to the denominator of layer normalization for numerical stability. Default is `1e-5`.\n",
    "6. `hidden_dropout`: dropout probability for the dropout op after FC2 layer (fully connected layer no. 2). Default is `0.1`.\n",
    "7. `attention_dropout`: dropout probability for the dropout op during multi-head attention. Default is `0.1`. \n",
    "8. `fuse_qkv_params`:  if set to True, TransformerLayer module exposes a single fused parameter for query-key-value. This enables optimizations such as QKV fusion without concatentations/splits and also enables the argument fuse_wgrad_accumulation.\n",
    "9. `normalization`: type of normalization applied. Default is `LayerNorm`.\n",
    "10. `activation`: type of activation used in the MLP block. Default is `gelu`.\n",
    "11. `attn_input_format`: controls whether the dimensions of the intermediate hidden states is 'batch first' ('bshd') or 'sequence first' ('sbhd'). `s` stands for the sequence length, `b` batch size, `h` the number of heads, `d` head size. Note that these formats are very closely related to the `qkv_format` in the `MultiHeadAttention` and `DotProductAttention` modules. \n",
    "\n",
    "\n",
    "Further, note that `RotaryPositionEmbedding` is defined as part of the TE's `TransformerLayer` itself since it expects this rope cache if RoPE is used in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e5c11",
   "metadata": {},
   "source": [
    "## Tutorial Overview and Benchmarks Preview\n",
    "\n",
    "#### Part 1 (Baseline):  HF Llama2 (precision: `BF16`)\n",
    "Llama2 weights are loaded into the HuggingFace native implementation `LlamaForCausalLM` (refer to [modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)) \n",
    "\n",
    "#### Part 2 (Improvement 1): HF Llama2 (replace `nn.Linear` with `TE.Linear` | precision: `FP8`)\n",
    "[HuggingFace accelerate](https://github.com/huggingface/accelerate/blob/main/src/accelerate/utils/transformer_engine.py) provides a way to replace `torch.nn.Linear` layers with `transformer_engine.pytorch.module.Linear` layers which could be run with `FP8` precision. This is the most straightforward way to use Transformer-Engine's `FP8` precision during training/finetuning for HF Llama model.\n",
    "\n",
    "#### Part 3 (Improvement 2): TE Llama2 (replace `LlamaDecoderLayer` with `TE.TransformerLayer` | precision: `BF16`)\n",
    "```\n",
    "ModuleList(\n",
    "  (0-31): 32 x LlamaDecoderLayer(\n",
    "    (self_attn): LlamaAttention(\n",
    "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (rotary_emb): LlamaRotaryEmbedding()\n",
    "    )\n",
    "    (mlp): LlamaMLP(\n",
    "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "      (act_fn): SiLU()\n",
    "    )\n",
    "    (input_layernorm): LlamaRMSNorm()\n",
    "    (post_attention_layernorm): LlamaRMSNorm()\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "A major portion of the HuggingFace model implementation (`LlamaDecoderLayer`) could be replaced with TransformerEngine's implementation (`TransformerLayer` which is wrapped in `TELlamaDecoderLayer`). \n",
    "\n",
    "```\n",
    "ModuleList(\n",
    "  (0-31): 32 x TELlamaDecoderLayer(\n",
    "    (self_attention): MultiheadAttention(\n",
    "      (layernorm_qkv): LayerNormLinear()\n",
    "      (core_attention): DotProductAttention(\n",
    "        (flash_attention): FlashAttention()\n",
    "        (fused_attention): FusedAttention()\n",
    "        (unfused_attention): UnfusedDotProductAttention(\n",
    "          (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
    "          (attention_dropout): Dropout(p=0, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (proj): Linear()\n",
    "    )\n",
    "    (layernorm_mlp): LayerNormMLP()\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "This needs some special care when handling the weight keys being loaded from the Llama2 model checkpoint as those keys are named differently in HF and TE implementaions. This tutorial comes with a `te_llama.py` file which contains the necessary reference wrappers for the larger model chunk replacement and also handles the weight keys mapping b/w the two layer implementaions.\n",
    "\n",
    "[NOTE: This improvement still runs in `BF16` for reference and the next improvement is switching on `FP8` precision in addition to this improvement]\n",
    "\n",
    "#### Part 4 (Improvement 3): TE Llama2 (replace `LlamaDecoderLayer` with `TE.TransformerLayer` | precision: `FP8`)\n",
    "Same as improvement 3, but the precision is `FP8`.\n",
    "\n",
    "\n",
    "#### Benchmarks Preview\n",
    "A discussion is provided at the end of the tutorial.\n",
    "\n",
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 288                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 307                         | 0.94                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | BF16      | 243                         | 1.19                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | FP8       | 231                         | 1.24                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d7496",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a15736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup, AutoConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils.dataclasses import FP8RecipeKwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a0c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(accelerator:Accelerator, batch_size:int = 8):\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            element[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=False\n",
    "        )\n",
    "        return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        dataset = dataset.map(\n",
    "            tokenize,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "\n",
    "    pad_to_multiple_of = 16\n",
    "    if accelerator.mixed_precision == \"fp8\":\n",
    "        pad_to_multiple_of = 16\n",
    "    elif accelerator.mixed_precision != \"no\":\n",
    "        pad_to_multiple_of = 8\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=pad_to_multiple_of,\n",
    "    )\n",
    "\n",
    "    dataloader_params = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"collate_fn\": data_collator,\n",
    "        \"drop_last\": True,\n",
    "    }\n",
    "    train_dataloader = DataLoader(dataset, **dataloader_params)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510535bf",
   "metadata": {},
   "source": [
    "#### Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3ff7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision = \"bf16\"\n",
    "model_name = \"\" # <== Add model weight location here\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "dataset_text_field = \"text\"\n",
    "learning_rate = 1.41e-5\n",
    "batch_size = 8\n",
    "max_seq_length = 256\n",
    "gradient_accumulation_steps = 1\n",
    "num_training_steps=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b61e2c",
   "metadata": {},
   "source": [
    "## [Baseline] Running HF `LlamaModel` (Precision: `BF16`)\n",
    "\n",
    "Llama2 weights are loaded into the HuggingFace native implementation `LlamaForCausalLM` (refer to [modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)). \n",
    "\n",
    "`batch_size` is `8` and precision is `BF16`\n",
    "\n",
    "To recap, the `LlamaDecoderLayer` is left unchanged in the baseline as follows:\n",
    "\n",
    "![LlamaDecoderLayer](media/llamadecoderlayer.png \"LlamaDecoderLayer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae385bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config._attn_implementation = \"flash_attention_2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# Needed for the cases when using TELlamaForCausalLM. So adding here for 1:1 comparison\n",
    "model.config.use_cache=False\n",
    "\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e69014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:394: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Init HF accelerator that's used for training\n",
    "accelerator = Accelerator(log_with=\"wandb\", gradient_accumulation_steps=gradient_accumulation_steps, mixed_precision=mixed_precision)\n",
    "accelerator.print(f'State: {accelerator.state}')\n",
    "train_dataloader = get_dataloaders(accelerator, batch_size)\n",
    "\n",
    "# Wrap model, optimizer/scheduler, dataloaders in accelerate\n",
    "optimizer = AdamW(params = model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "accelerator.init_trackers(\"fp8-benchmarks\", config={\n",
    "    \"model_name\": model_name,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"accelerator_state\": accelerator.state,\n",
    "    \"mixed_precision\": accelerator.mixed_precision,\n",
    "},\n",
    "init_kwargs={\"wandb\": {\"name\": f'{accelerator.mixed_precision}_bs_{batch_size}_{accelerator.num_processes}_gpus'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc47e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaFlashAttention2(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a14b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 1.742400050163269, batch shape: torch.Size([8, 256]), peak gpu mem: 25.39 GB\n",
      "Step 0 time 0.7563114166259766\n",
      "Step 1: loss: 2.0216784477233887, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 1 time 0.2901787757873535\n",
      "Step 2: loss: 2.2615396976470947, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 2 time 0.2868986129760742\n",
      "Step 3: loss: 2.092174530029297, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 3 time 0.289764404296875\n",
      "Step 4: loss: 2.0941758155822754, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 4 time 0.28885555267333984\n",
      "Step 5: loss: 2.010141372680664, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 5 time 0.287872314453125\n",
      "Step 6: loss: 2.3474209308624268, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 6 time 0.2875847816467285\n",
      "Step 7: loss: 1.8907099962234497, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 7 time 0.28859663009643555\n",
      "Step 8: loss: 1.9204421043395996, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 8 time 0.289294958114624\n",
      "Step 9: loss: 1.9386579990386963, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 9 time 0.28856849670410156\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for _ in range(10):\n",
    "    if completed_steps >= num_training_steps:\n",
    "        break\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(f\"Step {step}: loss: {loss.item()}, batch shape: {batch['input_ids'].shape}, peak gpu mem: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            completed_steps += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Step {step} time {total_time}\")\n",
    "        accelerator.log({\"batch_time\": total_time, \"input_ids\": batch[\"input_ids\"].cpu().numpy(), \"attention_mask\": batch[\"attention_mask\"].cpu().numpy()})\n",
    "        start_time = end_time\n",
    "\n",
    "        if completed_steps >= num_training_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160960a1",
   "metadata": {},
   "source": [
    "## [Improvement 1] Replace `nn.Linear` with TE's `Linear` layers (Precision: `FP8`)\n",
    "\n",
    "[HuggingFace accelerate](https://github.com/huggingface/accelerate/blob/main/src/accelerate/utils/transformer_engine.py) provides a way to replace `torch.nn.Linear` layers with `transformer_engine.pytorch.module.Linear` layers as shown in the diagram below: \n",
    "\n",
    "![Replace `nn.Linear` with `TE.Linear`](media/llamadecoderlayer_replace_with_telinear.png \"llamadecoderlayer_replace_with_telinear\")\n",
    "\n",
    "\n",
    "This is the most straightforward way to use Transformer-Engine's `FP8` precision during training/finetuning for HF Llama model. Notice that the entire `LlamaDecoderLayer` is mostly left unchanged, it's only the `nn.Linear` layers that get replaced with `TE.Linear` layers. After replacing, these layers can be run in `FP8` precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a10a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config._attn_implementation = \"flash_attention_2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# Needed for the cases when using TELlamaForCausalLM. So adding here for 1:1 comparison\n",
    "model.config.use_cache=False\n",
    "\n",
    "batch_size = 8\n",
    "mixed_precision=\"fp8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26cf4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:394: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Init HF accelerator that's used for training. \n",
    "# Notice the use of `fp8` recipe\n",
    "fp8_kwarg_handler = [FP8RecipeKwargs(backend=\"te\")] if mixed_precision == \"fp8\" else None\n",
    "accelerator = Accelerator(\n",
    "    log_with=\"wandb\", gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "    mixed_precision=mixed_precision, \n",
    "    kwargs_handlers=fp8_kwarg_handler\n",
    ")\n",
    "accelerator.print(f'State: {accelerator.state}')\n",
    "\n",
    "# Wrap model, optimizer/lr-scheduler, dataloaders in accelerator\n",
    "train_dataloader = get_dataloaders(accelerator, batch_size)\n",
    "optimizer = AdamW(params = model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "accelerator.init_trackers(\"fp8-benchmarks\", config={\n",
    "    \"model_name\": model_name,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"accelerator_state\": accelerator.state,\n",
    "    \"mixed_precision\": accelerator.mixed_precision,\n",
    "},\n",
    "init_kwargs={\"wandb\": {\"name\": f'{accelerator.mixed_precision}_bs_{batch_size}_{accelerator.num_processes}_gpus'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83db1d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaFlashAttention2(\n",
       "      (q_proj): Linear()\n",
       "      (k_proj): Linear()\n",
       "      (v_proj): Linear()\n",
       "      (o_proj): Linear()\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear()\n",
       "      (up_proj): Linear()\n",
       "      (down_proj): Linear()\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7142363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 1.7408778667449951, batch shape: torch.Size([8, 256]), peak gpu mem: 30.95 GB\n",
      "Step 0 time 1.494196891784668\n",
      "Step 1: loss: 2.0046780109405518, batch shape: torch.Size([8, 256]), peak gpu mem: 68.52 GB\n",
      "Step 1 time 0.926837682723999\n",
      "Step 2: loss: 2.2610206604003906, batch shape: torch.Size([8, 256]), peak gpu mem: 68.52 GB\n",
      "Step 2 time 0.30959415435791016\n",
      "Step 3: loss: 2.0951247215270996, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 3 time 0.3102715015411377\n",
      "Step 4: loss: 2.0994796752929688, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 4 time 0.30820536613464355\n",
      "Step 5: loss: 2.005664587020874, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 5 time 0.3083922863006592\n",
      "Step 6: loss: 2.3457205295562744, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 6 time 0.30889034271240234\n",
      "Step 7: loss: 1.8973335027694702, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 7 time 0.30847978591918945\n",
      "Step 8: loss: 1.9132524728775024, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 8 time 0.30858278274536133\n",
      "Step 9: loss: 1.9362869262695312, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 9 time 0.30936670303344727\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for _ in range(10):\n",
    "    if completed_steps >= num_training_steps:\n",
    "        break\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(f\"Step {step}: loss: {loss.item()}, batch shape: {batch['input_ids'].shape}, peak gpu mem: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            completed_steps += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Step {step} time {total_time}\")\n",
    "        accelerator.log({\"batch_time\": total_time, \"input_ids\": batch[\"input_ids\"].cpu().numpy(), \"attention_mask\": batch[\"attention_mask\"].cpu().numpy()})\n",
    "        start_time = end_time\n",
    "\n",
    "        if completed_steps >= num_training_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b23afd",
   "metadata": {},
   "source": [
    "## [Improvement 2] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `BF16`)\n",
    "\n",
    "In addition to basic layers like `Linear` and `Layernorm`, Transformer-Engine offers larger modules like `Attention` and `MLP` that could replace their counterparts in the `LlamaDecoderLayer` and potentially provide more speedup. Further Transformer-Engine also offers a full-on `TransformerLayer` which could be substituted for `LlamaDecoderLayer`. \n",
    "\n",
    "More concretely, here's how `LlamaDecoderLayer`s form the core of the decoder layer stack in HF's llama implementation:\n",
    "```\n",
    "ModuleList(\n",
    "  (0-31): 32 x LlamaDecoderLayer(\n",
    "    (self_attn): LlamaAttention(\n",
    "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (rotary_emb): LlamaRotaryEmbedding()\n",
    "    )\n",
    "    (mlp): LlamaMLP(\n",
    "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "      (act_fn): SiLU()\n",
    "    )\n",
    "    (input_layernorm): LlamaRMSNorm()\n",
    "    (post_attention_layernorm): LlamaRMSNorm()\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "As is clear above, a major portion of the HuggingFace model implementation (`LlamaDecoderLayer`) could be replaced with TransformerEngine's implementation.\n",
    "\n",
    "This is a bit more involved and the accompanying file `te_llama.py` file provides a reference implementation.\n",
    "Briefly, \n",
    "1. `TELlamaDecoderLayer` is added as a wrapper for `TransformerLayer`. \n",
    "2. Before creating a `LlamaForCausalLM`, `LlamaDecoderLayer` is monkey-patched with `TELlamaDecoderLayer`\n",
    "3. A custom `pretrained_from_local` method is added that copies the weights from the checkpoint (which is meant for HF Llama implementation) to the modified `TELlamaForCausalLM` by carefully mapping the weights from the `LlamaDecoderLayer` (HF) to `TransformerLayer` (TE). Refer to the following diagram for more details.\n",
    "\n",
    "![Replace `LlamaDecoderLayer` with `TransformerLayer`](media/weight_swap.png \"weight swap\")\n",
    "\n",
    "\n",
    "After initializing the modified Llama model this way, the core decoder layers get changed to `TELlamaDecoderLayer` (wrapper around `TransformerLayer`) as shown in the following output:\n",
    "```\n",
    "ModuleList(\n",
    "  (0-31): 32 x TELlamaDecoderLayer(\n",
    "    (self_attention): MultiheadAttention(\n",
    "      (layernorm_qkv): LayerNormLinear()\n",
    "      (core_attention): DotProductAttention(\n",
    "        (flash_attention): FlashAttention()\n",
    "        (fused_attention): FusedAttention()\n",
    "        (unfused_attention): UnfusedDotProductAttention(\n",
    "          (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
    "          (attention_dropout): Dropout(p=0, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (proj): Linear()\n",
    "    )\n",
    "    (layernorm_mlp): LayerNormMLP()\n",
    "  )\n",
    ")\n",
    "```\n",
    "In summary, the model gets changed as follows with a bigger chunk of the implementation (core decoder layers) coming from TransformerEngine.\n",
    "\n",
    "![model change in action](media/model_change.png \"model change\")\n",
    "\n",
    "[NOTE: This implementation still runs in `BF16`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8ecc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "from te_llama import TELlamaForCausalLM\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = TELlamaForCausalLM.from_pretrained_local(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# Needed for the cases when using TELlamaForCausalLM\n",
    "model.config.use_cache=False\n",
    "\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1c2d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:394: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Init HF accelerator that's used for training. \n",
    "accelerator = Accelerator(\n",
    "    log_with=\"wandb\", \n",
    "    gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "    mixed_precision=mixed_precision\n",
    ")\n",
    "accelerator.print(f'State: {accelerator.state}')\n",
    "\n",
    "# Wrap model, optimizer/lr_scheduler, dataloaders in accelerator\n",
    "train_dataloader = get_dataloaders(accelerator, batch_size)\n",
    "optimizer = AdamW(params = model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "accelerator.init_trackers(\"fp8-benchmarks\", config={\n",
    "    \"model_name\": model_name,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"accelerator_state\": accelerator.state,\n",
    "    \"mixed_precision\": accelerator.mixed_precision,\n",
    "},\n",
    "init_kwargs={\"wandb\": {\"name\": f'{accelerator.mixed_precision}_bs_{batch_size}_{accelerator.num_processes}_gpus'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb3faf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x TELlamaDecoderLayer(\n",
       "    (self_attention): MultiheadAttention(\n",
       "      (layernorm_qkv): LayerNormLinear()\n",
       "      (core_attention): DotProductAttention(\n",
       "        (flash_attention): FlashAttention()\n",
       "        (fused_attention): FusedAttention()\n",
       "        (unfused_attention): UnfusedDotProductAttention(\n",
       "          (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
       "          (attention_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (proj): Linear()\n",
       "    )\n",
       "    (layernorm_mlp): LayerNormMLP()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe4a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 2.285313367843628, batch shape: torch.Size([8, 256]), peak gpu mem: 24.39 GB\n",
      "Step 0 time 1.4395570755004883\n",
      "Step 1: loss: 3.433112144470215, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 1 time 0.2440946102142334\n",
      "Step 2: loss: 4.062510967254639, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 2 time 0.24254989624023438\n",
      "Step 3: loss: 3.3462915420532227, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 3 time 0.24259161949157715\n",
      "Step 4: loss: 3.0718109607696533, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 4 time 0.24408507347106934\n",
      "Step 5: loss: 3.7262790203094482, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 5 time 0.24119019508361816\n",
      "Step 6: loss: 5.115753650665283, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 6 time 0.24044084548950195\n",
      "Step 7: loss: 3.9757368564605713, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 7 time 0.24254655838012695\n",
      "Step 8: loss: 3.293850898742676, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 8 time 0.2404341697692871\n",
      "Step 9: loss: 2.613889455795288, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 9 time 0.24282383918762207\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for _ in range(10):\n",
    "    if completed_steps >= num_training_steps:\n",
    "        break\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(f\"Step {step}: loss: {loss.item()}, batch shape: {batch['input_ids'].shape}, peak gpu mem: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            completed_steps += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Step {step} time {total_time}\")\n",
    "        accelerator.log({\"batch_time\": total_time, \"input_ids\": batch[\"input_ids\"].cpu().numpy(), \"attention_mask\": batch[\"attention_mask\"].cpu().numpy()})\n",
    "        start_time = end_time\n",
    "\n",
    "        if completed_steps >= num_training_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689fddf",
   "metadata": {},
   "source": [
    "## [Improvement 3] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `FP8`)\n",
    "\n",
    "Now that most of the HF Llama model implementation (`LlamaDecoderLayer`s) has been swapped with Transformer-Engine implementation (`TELlamaDecoderLayer` or `TransformerLayer`), let's see how `FP8` training helps improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458e55e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "from te_llama import TELlamaForCausalLM\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "model = TELlamaForCausalLM.from_pretrained_local(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# Needed for the cases when using TELlamaForCausalLM\n",
    "model.config.use_cache=False\n",
    "\n",
    "batch_size = 8\n",
    "mixed_precision=\"fp8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c80592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:394: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Init HF accelerator that's used for training. \n",
    "# Notice the use of `fp8` recipe\n",
    "fp8_kwarg_handler = [FP8RecipeKwargs(backend=\"te\")] if mixed_precision == \"fp8\" else None\n",
    "accelerator = Accelerator(\n",
    "    log_with=\"wandb\", gradient_accumulation_steps=gradient_accumulation_steps, \n",
    "    mixed_precision=mixed_precision, \n",
    "    kwargs_handlers=fp8_kwarg_handler\n",
    ")\n",
    "accelerator.print(f'State: {accelerator.state}')\n",
    "\n",
    "# Wrap model, optimizer/lr-scheduler, dataloaders in accelerator\n",
    "train_dataloader = get_dataloaders(accelerator, batch_size)\n",
    "optimizer = AdamW(params = model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "accelerator.init_trackers(\"fp8-benchmarks\", config={\n",
    "    \"model_name\": model_name,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"accelerator_state\": accelerator.state,\n",
    "    \"mixed_precision\": accelerator.mixed_precision,\n",
    "},\n",
    "init_kwargs={\"wandb\": {\"name\": f'{accelerator.mixed_precision}_bs_{batch_size}_{accelerator.num_processes}_gpus'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d2c349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x TELlamaDecoderLayer(\n",
       "    (self_attention): MultiheadAttention(\n",
       "      (layernorm_qkv): LayerNormLinear()\n",
       "      (core_attention): DotProductAttention(\n",
       "        (flash_attention): FlashAttention()\n",
       "        (fused_attention): FusedAttention()\n",
       "        (unfused_attention): UnfusedDotProductAttention(\n",
       "          (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
       "          (attention_dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (proj): Linear()\n",
       "    )\n",
       "    (layernorm_mlp): LayerNormMLP()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6948fc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 2.2819559574127197, batch shape: torch.Size([8, 256]), peak gpu mem: 26.61 GB\n",
      "Step 0 time 3.093244791030884\n",
      "Step 1: loss: 3.3127150535583496, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 1 time 0.7673726081848145\n",
      "Step 2: loss: 3.993682384490967, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 2 time 0.23313021659851074\n",
      "Step 3: loss: 3.22248911857605, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 3 time 0.23214364051818848\n",
      "Step 4: loss: 3.0897724628448486, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 4 time 0.23146915435791016\n",
      "Step 5: loss: 3.6932754516601562, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 5 time 0.23121118545532227\n",
      "Step 6: loss: 5.0839033126831055, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 6 time 0.23106837272644043\n",
      "Step 7: loss: 3.907317638397217, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 7 time 0.23175501823425293\n",
      "Step 8: loss: 3.2384896278381348, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 8 time 0.23119401931762695\n",
      "Step 9: loss: 2.6323776245117188, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 9 time 0.23113751411437988\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for _ in range(10):\n",
    "    if completed_steps >= num_training_steps:\n",
    "        break\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(f\"Step {step}: loss: {loss.item()}, batch shape: {batch['input_ids'].shape}, peak gpu mem: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            completed_steps += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Step {step} time {total_time}\")\n",
    "        accelerator.log({\"batch_time\": total_time, \"input_ids\": batch[\"input_ids\"].cpu().numpy(), \"attention_mask\": batch[\"attention_mask\"].cpu().numpy()})\n",
    "        start_time = end_time\n",
    "\n",
    "        if completed_steps >= num_training_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d2da3",
   "metadata": {},
   "source": [
    "## Benchmarks (revisited)\n",
    "Let's take a look at the summary of the performance numbers with various configurations.\n",
    "\n",
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 288                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 307                         | 0.94                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | BF16      | 243                         | 1.19                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | FP8       | 231                         | 1.24                    |\n",
    "\n",
    "When we use larger chunks of layers from TransformerEngine, we see larger performance improvement. Further, when `FP8` precision is enabled, we see even larger speedup as is expected.\n",
    "\n",
    "### Understanding the aberration with Improvement 2 (CPU Overheads)\n",
    "When the `nn.Linear` layers in the model are naively replaced with `TE.Linear` layers, a performance drop is seen. Although it seems like a bummer in this case, it's not unexpected. \n",
    "\n",
    "1. `TE.Linear` has to do some extra work (in python, so on CPU) before it can issue `FP8` version of GEMMs (matrix-multiplies from the linear layers) to the GPU.\n",
    "2. So, if batch-size and/or sequence-length are small enough, the GEMM will be smaller. The GEMMs on GPU therefore usually finish early than the CPU can issue them in succession. In this case, we say the the workload is CPU bound. Ideally we'd want the workload to be GPU bound, i.e. it should spend more time in the GPU to fully utilize its capability.\n",
    "3. We can verify this by looking the profiles of the two workloads (using nsight systems).\n",
    "    - For the baseline case (with `nn.Linear` layers):\n",
    "        ![baseline profile](media/baseline.png \"baseline\")\n",
    "        \n",
    "    - Improvement 1 - when `nn.Linear`s are replaced with `TE.Linear`s:\n",
    "        ![replace linears](media/replace_nnlinear_with_telinear.png \"replace linears\")\n",
    "        \n",
    "    - As we can see, the GPU is busy comparatively less in the second case, compared to the first. CPU overhead is not letting \n",
    "\n",
    "But How do we alleviate this issue? \n",
    "\n",
    "To fully utilize the Transformer-Engine's capabilities, the workload should be larger. In this present tutorial, the batch-size is 8 which is fine but sequence-length is 256 which is pretty low. If the sequence-length is large enough, even this case could be faster than the baseline. \n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
