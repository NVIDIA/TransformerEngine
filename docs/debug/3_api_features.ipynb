{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug features\n",
    "\n",
    "## LogTensorStats\n",
    "\n",
    "This feature handles the logging of basic tensor statistics.  \n",
    "\n",
    "For a distributed setting, the auxiliary stats are computed for each node and gathered after the `debug_api.step()` call. Do not forget to invoke `debug_api.step()` at every step to log stats!  \n",
    "\n",
    "`LogTensorStats` supports micro-batching. If multiple forward/backward passes are invoked per `debug_api.step()`, then stats for all tensors except weights will be accumulated.  \n",
    "\n",
    "`LogTensorStats` can induce significant overhead. To mitigate this issue, logging stats with `freq > 1` is recommended. If `LogTensorStats` is not used in a given step, the overhead is smaller. Moreover, if no other feature is used for the layer, the TE layer will run as fast as it would without `debug_api` initialized.  \n",
    "\n",
    "Keys:\n",
    "\n",
    "- `stats`: list of statistics to log:\n",
    "  - `min`\n",
    "  - `max`\n",
    "  - `mean`\n",
    "  - `std`\n",
    "  - `l1_norm`\n",
    "  - `l2_norm`\n",
    "  - `cur_amax` – maximal absolute value of a tensor,\n",
    "  - `dynamic_range` – equal to `torch.log2(amax) - torch.log2(amin)`,\n",
    "- `tensors/tensors_struct`: \n",
    "  - activation\n",
    "  - gradient\n",
    "  - weight\n",
    "- `freq`: Optional[int]\n",
    "- `start_step`: Optional[int]\n",
    "- `end_step`: Optional[int]\n",
    "- `start_end_list`: Optional[list([int, int])], non-overlapping list of (start, end) pairs in incremental order. Default = None. If not None, will ignore start_step and end_step\n",
    "\n",
    "**Example**\n",
    "```yaml\n",
    "example_tensor_stat_collection:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_name_regex_pattern: .*(fc1|self_attention).*\n",
    "  transformer_engine:\n",
    "    LogTensorStats:\n",
    "      enabled: True\n",
    "      tensors_struct:\n",
    "        - tensor: activation\n",
    "          stats: [mean]\n",
    "          freq: 10\n",
    "          start_step: 5\n",
    "          end_step: 100\n",
    "        - tensor: gradient\n",
    "          stats: [mean, max, min]\n",
    "          freq: 2\n",
    "          start_end_list: [[0, 20], [80, 100]]\n",
    "        - tensor: weight\n",
    "          stats: [dynamic_range]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## LogFp8TensorStats\n",
    "\n",
    "This feature handles logging of FP8 tensor stats. \n",
    "\n",
    "\n",
    "For a distributed setting, the auxiliary stats are computed for each node and gathered after the `debug_api.step()` call. Do not forget to invoke `debug_api.step()` at every step to log stats!  \n",
    "\n",
    "`LogFp8TensorStats` supports micro-batching. If multiple forward/backward passes are invoked per `debug_api.step()`, then stats for all tensors except weights will be accumulated.  \n",
    "\n",
    "`LogFp8TensorStats` can induce significant overhead. To mitigate this issue, logging stats with `freq > 1` is recommended. If `LogFp8TensorStats` is not used in a given step, the overhead is smaller. Moreover, if no other feature is used for the layer, the TE layer will run as fast as it would without `debug_api` initialized.  \n",
    "\n",
    "\n",
    "Keys:\n",
    "\n",
    "- `stats`:\n",
    "  - underflows%\n",
    "  - overflows%\n",
    "- `tensors/tensors_struct`: \n",
    "  - activation\n",
    "  - gradient\n",
    "  - weight\n",
    "- `freq`: Optional[int]\n",
    "- `start_step`: Optional[int]\n",
    "- `end_step`: Optional[int]\n",
    "- `start_end_list`: Optional[list([int, int])], non-overlapping list of (start, end) pairs in incremental order. Default = None. If not None, will ignore start_step and end_step\n",
    "\n",
    "**Example**\n",
    "```yaml\n",
    "example_fp8_tensor_stat_collection:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [layernorm_linear]\n",
    "  transformer_engine:\n",
    "    LogFp8TensorStats:\n",
    "        enabled: True\n",
    "        tensors_struct: \n",
    "        - tensor: activation\n",
    "          stats: [underflows%, overflows%]\n",
    "          freq: 1\n",
    "        - tensor: gradient\n",
    "          stats: [overflows%]\n",
    "          freq: 5\n",
    "        start_step: 0\n",
    "        end_step: 80\n",
    "```\n",
    "\n",
    "## DisableFp8Gemm\n",
    "\n",
    "GEMM operations are executed in higher precision, even when FP8 autocast is enabled.\n",
    "\n",
    "Keys:\n",
    "\n",
    "- `gemms`: \n",
    "  - fprop\n",
    "  - dgrad\n",
    "  - wgrad\n",
    "\n",
    "**Example**\n",
    "```yaml\n",
    "example_disable_fp8_gemm:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [fc1]\n",
    "  transformer_engine:\n",
    "    DisableFp8Gemm:\n",
    "      enabled: True\n",
    "      gemms: [dgrad, wgrad]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## DisableFp8Layer\n",
    "\n",
    "Disables all FP8 GEMMs in the layer.\n",
    "\n",
    "\n",
    "**Example**\n",
    "```yaml\n",
    "example_disable_fp8_layer:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [fc1]\n",
    "  transformer_engine:\n",
    "    DisableFp8Layer:\n",
    "      enabled: True\n",
    "```\n",
    "\n",
    "## PerTensorScaling\n",
    "\n",
    "Transformer Engine uses delayed scaling strategy on Hopper by default - you can read about it in [fp8 tutorial](../examples/fp8_primer.ipynb).\n",
    "You can switch this strategy to current scaling by using this option. Then amax and dynamic range will be computed using the current tensor, not the historical ones. It can improve stability and accuracy of the training, but it's slower than delayed scaling. \n",
    "\n",
    "Note that tensors in this feature are Hopper `Float8Tensor` containing one scaling factor per tensor.\n",
    "\n",
    "\n",
    "Keys:\n",
    "\n",
    "- `gemms/gemms_struct`:\n",
    "  - fprop\n",
    "  - dgrad\n",
    "  - wgrad\n",
    "- `tensors/tensors_struct`:\n",
    "  - activation\n",
    "  - gradient\n",
    "  - weight\n",
    "- `margin`: int - impacts the computation of scaling factors, default is 0, `amax = amax * (2^margin)`.\n",
    "\n",
    "**Example**\n",
    "```yaml\n",
    "example_per_tensor_scaling:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [transformer_layer.self_attn.layernorm_q]\n",
    "  transformer_engine:\n",
    "      PerTensorScaling:\n",
    "        enabled: True\n",
    "        margin: 1\n",
    "        gemms: [dgrad]\n",
    "        tensors: [weight, activation]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## FakeQuant\n",
    "\n",
    "Disables FP8 GEMM. Fake quantizes chosen tensors to FP8 - using per-tensor scaling factor, not delayed scaling - and runs high-precision GEMM.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/fake_quant.svg\">\n",
    "    <figcaption> Fig 1: Comparison of FP8 FPROP GEMM with the same GEMM in BF16 with fake quantization of activation tensor. Green tensors have the same values, but different dtypes. </figcaption>\n",
    "</figure>\n",
    "\n",
    "- `gemms/gemms_struct`: \n",
    "  - fprop\n",
    "  - dgrad\n",
    "  - wgrad\n",
    "- `tensors/tensors_struct`:\n",
    "  - activation\n",
    "  - gradient\n",
    "  - weight\n",
    "- `quant_format` - specifies the FP8 format to use: \n",
    "  - FP8E5M2\n",
    "  - FP8E4M3\n",
    "- `margin`: int - impacts the computation of scaling factors, default is 0, `amax = amax * (2^margin)`.\n",
    "\n",
    "**Example**\n",
    "```yaml\n",
    "example_fake_quant_fp8:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [transformer_layer.layernorm_mlp.fc1]\n",
    "  transformer_engine:\n",
    "      FakeQuant:\n",
    "        enabled: True\n",
    "        quant_format: FP8E5M2\n",
    "        gemms_struct:\n",
    "        - gemm: fprop\n",
    "          tensors: [activation, weight]\n",
    "        - gemm: dgrad\n",
    "          tensors: [gradient]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
