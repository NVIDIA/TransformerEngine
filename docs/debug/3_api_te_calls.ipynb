{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calls to Nvidia-DL-Framework-Inspect\n",
    "\n",
    "Let's look deeper into how Nvidia-DL-Framework-Inspect with Transformer Engine work together. TransformerEngine layers have some hook calls inside each of the GEMMs. User can define feature classes or use feature classes provided with TE. File `config.yaml` describes which hooks need to be used for which layers. Nvidia-DL-Framework-Inspect combines 3 things: TE training, feature classes and `config.yaml` and takes care of inserting hooks in correct places. This process is illustrated in the image below.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/api_calls1.svg\">\n",
    "    <figcaption> Fig 1: Example of Nvidia-DL-Framework-Inspect affecting training script with 1 Linear Layer. For tensors mentioned in `config.yaml`, behaviour of `modify_tensor_enabled()` and `modify_tensor()` calls are substituted with definitions from the feature class. Other calls return default values - in fact they do nothing. </figcaption>\n",
    "</figure>\n",
    "\n",
    "In this page all calls from TransformerEngine to the Nvidia-DL-Framework-Inspect for each GEMM are listed. Order of these calls is illustrated in the image below.\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/api_calls2.svg\">\n",
    "    <figcaption> Fig 2: The calls to Nvidia-DL-Framework-Inspect done for Transformer Engine. There are 2 types of calls: GEMM calls and routing callss.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "### Categories of the API calls\n",
    "\n",
    "There are 3 categories of API calls, each is used for different purposes:\n",
    "\n",
    "- GEMM calls - invoked during every GEMM, is used to process or quantize tensors and collect information about them,\n",
    "- routing calls - invoked at the beginning of forward pass - they indicate whether a feature is going to use `modify_tensor()`, ... etc.\n",
    "\n",
    "If all routing calls for the layer return `False`, then the layer is invoked in an optimized version with Transformer Engine fusions.\n",
    "If any of the routing calls return `True`, layers are run without the fusion. It is necessary, because some tensors cannot be accessed\n",
    "if fusion happens. An important remark is that if no feature is used for the layer, then it should perform as fast as the layer without initializing `debug_api`.\n",
    "\n",
    "\n",
    "### modify_tensor\n",
    "\n",
    "It allows to insert tensor processing. For example, feature `FakeQuant` uses it to emulate casting to FP8, but the tensor is returned in higher precision. It can be invoked at most once for each tensor within a given GEMM operation.\n",
    "\n",
    "This call is invoked if `modify_tensor_enabled` returns `True` and feature is enabled for the *tensor_name* and *gemm*.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `config: Dict` - dictionary containing information from `config.yaml` corressponding to the feature, tensor_name and gemm.\n",
    "- `layer_name: str`,\n",
    "- `tensor: torch.Tensor` - tensor in high precision,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`],\n",
    "- `default_quantizer : Quantizer` - quantizer which is used to cast the tensor to lower precision if *modify_tensor* is not invoked. For example, feature per tensor scale uses it to obtain FP8 dtype of the tensor. If recipe indicates that tensor is not cast - for example, if running without FP8 autocast, then `default_quantizer=None`,\n",
    "- `iteration: int` - iteration number - equal to the number of the times `debug_api.step()` was called.\n",
    "- `out: Union[torch.Tensor, transformer_engine.pytorch.QuantizerTensor]` - output tensor, used in weight caching mechanism.\n",
    "\n",
    "Should return:\n",
    "\n",
    "- `Union[torch.Tensor, transformer_engine.pytorch.QuantizerTensor, None]` - can be `torch.Tensor` or one of the Transformer Engine's `QuantizedTensor` - the rule is that both tensors returned for each GEMM should have the same type. If both are `Float8Tensor`, then GEMM is run in FP8. If both are `torch.Tensor`, GEMM is run in high precision. Please take that into account especially if only one tensor of the GEMM is processed by the `modify_tensor()`. For example, `FakeQuant` disabled FP8 GEMM to ensure that second tensor is also in high precision. If tensor is not the input for any GEMM - namely  `output`, `wgrad` and `dgrad` - the return type would match input type. \n",
    "Should return `None` if `out` is not `None`.\n",
    "\n",
    "Default behaviour:\n",
    "\n",
    "- It returns unchanged tensor.\n",
    "\n",
    "### inspect_tensor\n",
    "\n",
    "The feature is invoked if *inspect_tensor_enabled* returns `True`. It can be used to obtain information of the high precision tensor. For example, it is run by the `LogTensorStats` feature.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `config: Dict` - dictionary containing information from `config.yaml` corressponding to the feature, tensor_name and gemm.\n",
    "- `layer_name: str`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`],\n",
    "- `tensor` - tensor in high precision,\n",
    "- `iteration: int` - iteration number - equal to the number of the times `debug_api.step()` was called,\n",
    "\n",
    "Should return nothing.\n",
    "\n",
    "Default behaviour:\n",
    "\n",
    "- It does nothing.\n",
    "\n",
    "### inspect_tensor_postquantize\n",
    "\n",
    "Similar as *inspect_tensor*, but is run after one of the: fp8 cast, modify_tensor if they are run. If none of the fp8 cast or modify_tensor is invoked, then *inspect_tensor_postquantize* is also not invoked. The feature LogFp8Stats uses this call to collect FP8 statistics after the quantization.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `config: Dict` - dictionary containing information from `config.yaml` corressponding to the feature, tensor_name and gemm.\n",
    "- `layer_name: str`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`],\n",
    "- `tensor` - tensor in fp8 or processed tensor after the modify_tensor call,\n",
    "- `rowwise: bool` - whether this is the tensor or its transpose,\n",
    "- `iteration: int` - iteration number - equal to the number of the times `debug_api.step()` was called.\n",
    "\n",
    "Should return nothing.\n",
    "\n",
    "\n",
    "### modify_tensor_enabled\n",
    "\n",
    "It is used to determine whether *modify_tensor* will be run for given GEMM and tensor name. It has **higher priority** than fp8_gemm, is *modify_tensor_enabled* returns True, then modify_tensor call is invoked for respective tensor no matter what.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `config: Dict` - dictionary containing information from `config.yaml` corressponding to the feature, tensor_name and gemm.\n",
    "- `layer_name: str`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`],\n",
    "- `iteration: int` - iteration number - equal to the number of the times `debug_api.step()` was called.\n",
    "\n",
    "Should return:\n",
    "\n",
    "- `output: bool`\n",
    "\n",
    "Default behaviour:\n",
    "\n",
    "- It returns `False`.\n",
    "\n",
    "### fp8_gemm_enabled\n",
    "\n",
    "If the tensor is not processed using *modify_tensor* and the fp8 recipe is enabled, then the decision whether to cast it to fp8 is based on the value returned by the call *fp8_gemm_enabled*. If the tensor is processed using *modify_tensor* and or fp8 autocast is not enabled, result of this call does not matter.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `config: Dict` - dictionary containing information from `config.yaml` corressponding to the feature, tensor_name and gemm.\n",
    "- `layer_name: str`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `iteration: int` - iteration number - equal to the number of the times `debug_api.step()` was called.\n",
    "\n",
    "Should return:\n",
    "\n",
    "- `fp_gemm: bool` – tensor after processing.\n",
    "\n",
    "\n",
    "Default behaviour:\n",
    "\n",
    "- It returns `True`.\n",
    "\n",
    "\n",
    "### inspect_tensor_postquantize\n",
    "\n",
    "It is routing call, which is run at the initialization of the layer. If it returns true, then *inspect_tensor* for given GEMM and tensor will be invoked for every forward.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `config: Dict` - dictionary containing information from `config.yaml` corressponding to the feature, tensor_name and gemm.\n",
    "- `layer_name: str`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`].\n",
    "- `iteration: int` - iteration number - equal to the number of the times `debug_api.step()` was called.\n",
    "\n",
    "Should return:\n",
    "\n",
    "- `output: bool`\n",
    "\n",
    "Default behaviour:\n",
    "\n",
    "- It returns `False`.\n",
    "\n",
    "### inspect_tensor_postquantize_enabled\n",
    "\n",
    "\n",
    "It is routing call, which is run at the initialization of the layer. If it returns true, then *inspect_tensor_postquantize* for given GEMM and tensor will be invoked for every forward.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `config: Dict` - dictionary containing information from `config.yaml` corressponding to the feature, tensor_name and gemm.\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`].\n",
    "- `rowwise: bool` - whether this is the tensor or its transpose,\n",
    "- `iteration: int` - iteration number - equal to the number of the times `debug_api.step()` was called.\n",
    "\n",
    "Should return:\n",
    "\n",
    "- `output: bool`\n",
    "\n",
    "Default behaviour:\n",
    "\n",
    "- It returns `False`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
