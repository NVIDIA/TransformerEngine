{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing new precision with the Megatron-LM\n",
    "\n",
    "In this tutorial we present how to test new precision/recipe using Megatron-LM and Nvidia-DL-Framework-Inspect. \n",
    "\n",
    "[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) is a large-scale transformer model framework developed by NVIDIA for training natural language processing (NLP) models with billions of parameters. It is designed to optimize both model training efficiency and scalability, allowing researchers and developers to push the limits of NLP capabilities.\n",
    "\n",
    "We will show how to test ideas for new precisions/recipes in few simple steps.\n",
    "\n",
    "Consider some example precisions/recipe for training GPT-like Transformer:\n",
    "\n",
    "1. Each weight to the GEMM is casted to -1/0/1 with scaling factor. For the sake of this tutorial, suppose we have implemented function `utils.zero_one_cast(x: torch.Tensor) -> (torch.Tensor, float)` which returns `-1/0/1` tensor and scaling factor as a float. \n",
    "2. Each input is casted to 4E3M FP8 precision.\n",
    "\n",
    "We will present how to <b>emulate</b> behaviour of such recipe - tensors will be casted to this precisions and then casted back to high precision. Then GEMM will be done also in high precision.\n",
    "\n",
    "Moreover let's suppose we want to test few scenarios:\n",
    "\n",
    "1. Use new precision for both backward and forward.\n",
    "2. Use new precision only for forward and use FP8 for backward.\n",
    "3. Use new precision for both forward and backward, but one in every 5 consecutive layers will be run in high precision.\n",
    "\n",
    "We will present how to implement it using the Transformer Engine.\n",
    "\n",
    "#### Feature class implementation\n",
    "\n",
    "Let's look how feature `FakeFP8Cast` looks like.\n",
    "\n",
    "```python\n",
    "\n",
    "@Registry.register_feature(namespace=\"transformer_engine\")\n",
    "class FakeQuantFp8(TEConfigAPIMapper):\n",
    "    #(...)\n",
    "    \n",
    "    @api_method\n",
    "    def fp8_gemm(self, config, layer_name, **kwargs):\n",
    "        return False\n",
    "\n",
    "\n",
    "    @api_method\n",
    "    def use_process_tensor(self, *args, **kwargs):\n",
    "        return True\n",
    "\n",
    "\n",
    "    @api_method\n",
    "    def process_tensor(self, config, layer_name, **kwargs):\n",
    "        # (...)\n",
    "        quant_format = config[\"quant_format\"]\n",
    "        margin = config.get('margin', self._get_margin_default())\n",
    "        q_tensor = fake_quantize_fp8(kwargs[\"tensor\"], quant_format, margin=margin)\n",
    "        return q_tensor\n",
    "\n",
    "```\n",
    "\n",
    "We can see that 3 things happened there:\n",
    "\n",
    "1. FP8 quantization is disabled and GEMM is run in high precision.\n",
    "2. `use_process_tensor` returning `True` informs that we will use `process_tensor()`.\n",
    "3. `process_tensor()` fake quants value to FP8. It affects, due to Nvidia-DL-Framework-Inspect, only tensors and GEMMs specified in `config.yaml` file.\n",
    "\n",
    "If one wants to understand better when these calls are called, please read [section about API calls from the TE](./3_api_te_calls.ipynb).\n",
    "\n",
    "\n",
    "We will make something similar - but using `utils.zero_one_cast` in the case of the weight:\n",
    "```python\n",
    "\n",
    "@Registry.register_feature(namespace=\"transformer_engine\")\n",
    "@append_parent_docstring(parent=TEConfigAPIMapper)\n",
    "class NewRecipe(TEConfigAPIMapper):\n",
    "    # (...)\n",
    "    \n",
    "    @api_method\n",
    "    def fp8_gemm(self, config, layer_name, gemm, tensor_name, **kwargs):\n",
    "        return False\n",
    "      \n",
    "    @api_method\n",
    "    def use_process_tensor(self, *args, **kwargs):\n",
    "        return True\n",
    "\n",
    "    @api_method\n",
    "    def process_tensor(self, config, layer_name, tensor_name, **kwargs):\n",
    "        # (...)\n",
    "        if tensor_name == \"weight\":\n",
    "            return utils.zero_one_cast(kwargs[\"tensor\"])\n",
    "        else:\n",
    "            quant_format = config[\"quant_format\"]\n",
    "            margin = config.get('margin', self._get_margin_default())\n",
    "            q_tensor = fake_quantize_fp8(kwargs[\"tensor\"], quant_format, margin=margin)\n",
    "            return q_tensor\n",
    "```\n",
    "\n",
    "Suppose that our feature is saved in the dir `/path/to/feature/new_precision.py`.\n",
    "\n",
    "#### Integration with the Megatron-LM\n",
    "\n",
    "We have successfully defined our feature, which disabled FP8 GEMM and runs high precision GEMM with fake-casted \n",
    "tensors, emulating new precision.\n",
    "\n",
    "Now, let's look how to use our recipe with Megatron-LM training.\n",
    "Let's begin with preparing some `config.yaml` file to make experiments in different scenarios.\n",
    "\n",
    "```yaml\n",
    "Experiment1:\n",
    "  enabled: True # Experiment 1 is now enabled, one needs to change it manually to enable other experiment\n",
    "  layers:\n",
    "    layer_name_regex_pattern: '.*'\n",
    "  transformer_engine:\n",
    "    new_recipe:\n",
    "      enabled: True\n",
    "      gemms: [fprop, dgrad, wgrad] # forward and backward\n",
    "Experiment2:\n",
    "  enabled: False\n",
    "  layers:\n",
    "    layer_name_regex_pattern: '.*'\n",
    "  transformer_engine:\n",
    "    new_recipe:\n",
    "      enabled: True\n",
    "      gemms: [fprop] # forward\n",
    "Experiment3:\n",
    "  enabled: False\n",
    "  layers:\n",
    "    layer_name_regex_pattern: '.*[12346789]' # four of every 5 layers\n",
    "  transformer_engine:\n",
    "    new_recipe:\n",
    "      enabled: True\n",
    "      gemms: [fprop, dgrad, wgrad] # forward and backward\n",
    "Experiment3_part2:\n",
    "  enabled: False\n",
    "  layers:\n",
    "    layer_name_regex_pattern: '.*[05]' # one of every 5 layers run in high precision\n",
    "  transformer_engine:\n",
    "    DisableFp8Layer:\n",
    "      enabled: True\n",
    "\n",
    "```\n",
    "\n",
    "To run Nvidia-DL-Framework-Inspect with the TE one need to run Megatron-LM with the following flags.\n",
    "```bash\n",
    "... megatrone-lm-script.sh ... \\\n",
    "  --enable-nvdlfw-inspect \\\n",
    "  --nvdlfw-config-path /path/to/config/file.yaml \\\n",
    "  --nvdlfw-features-path /path/to/feature/ \\\n",
    "  --nvdlfw-log-dir /path/to/nv_dlfw_logs/\n",
    "```\n",
    "\n",
    "To edit which layers are affected you can modify `config.yaml` file. That's all you need!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
