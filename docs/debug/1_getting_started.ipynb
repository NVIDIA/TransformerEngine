{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Warning</b>\n",
    "\n",
    "Precision debug tools with [Nvidia-DL-Framework-Inspect](https://github.com/NVIDIA/nvidia-dlfw-inspect) for Transformer Engine is currently supported only for Torch.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Transformer Engine provides set of precision debug tools which easily allow to:\n",
    "\n",
    "- log the statistics for each of the tensor in every GEMM,\n",
    "- run some specific GEMMs in Higher Precision etc.,\n",
    "- run current scaling - with one scaling factor per tensor - on Hopper,\n",
    "- test new precisions and easily integrate them with FP8 training,\n",
    "- ... and many more.\n",
    "\n",
    "All these things nees only few small changes in code.\n",
    "\n",
    "To use them one need to install [Nvidia-DL-Framework-Inspect](https://github.com/NVIDIA/nvidia-dlfw-inspect) tool. \n",
    "User defines in `config.yaml` which features need to be used in which layers and then Nvidia-DL-Framework-Inspect takes care of the rest. There are 2 kinds of features:\n",
    "\n",
    "- provided by the Transformer Engine - for example DisableFP8GEMM or LogTensorStats - they are listed in [debug features API](./3_api_features.ipynb)section\n",
    "- defined by the user - for example for testing new precisions - please read [calls to nvidia-dlframework-inspect](./3_api_te_calls.ipynb) section.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/introduction.svg\">\n",
    "    <figcaption> Fig 1: Example of Nvidia-DL-Framework-Inspect affecting traning script with 3 TE Linear Layers. \n",
    "    There is specification in `config.yaml` for each layers which features should be used. There are feature class files - some are provided by the TE,\n",
    "    one - `UserProvidedPrecision` - is implemented by the user. Nvidia-DL-Framework-Inspect insterts features into the Layers as it is described in the config.\n",
    "     </figcaption>\n",
    "</figure>\n",
    "\n",
    "#### Example training script\n",
    "\n",
    "Let's look at a simple example of training a Transformer layer using Transformer Engine with FP8 precision. This example demonstrates how to set up the layer, define an optimizer, and perform a few training iterations using dummy data.\n",
    "\n",
    "```python\n",
    "# train.py\n",
    "\n",
    "from transformer_engine.pytorch import TransformerLayer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformer_engine.pytorch as te\n",
    "\n",
    "hidden_size = 512\n",
    "num_attention_heads = 8\n",
    "\n",
    "transformer_layer = TransformerLayer(\n",
    "    hidden_size=hidden_size,\n",
    "    ffn_hidden_size=hidden_size,\n",
    "    num_attention_heads=num_attention_heads\n",
    ").cuda()\n",
    "\n",
    "dummy_input = torch.randn(10, 32, hidden_size).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(transformer_layer.parameters(), lr=1e-4)\n",
    "dummy_target = torch.randn(10, 32, hidden_size).cuda()\n",
    "\n",
    "for epoch in range(5):\n",
    "    transformer_layer.train()\n",
    "    optimizer.zero_grad()\n",
    "    with te.fp8_autocast(enabled=True):\n",
    "        output = transformer_layer(dummy_input)\n",
    "    loss = criterion(output, dummy_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "We will demonstrate two debug features on the code above:\n",
    "\n",
    "1. Disabling FP8 precision for a specific GEMM operations, such as the FC1 and FC2 forward propagation GEMM.\n",
    "2. Logging statistics for other GEMM operations, such as gradient statistics for dgradÂ GEMM within the LayerNormLinear layer.\n",
    "\n",
    "\n",
    "There are 4 things one needs to do to use Transformer Engine debug features:\n",
    "\n",
    "1. Create a **config.yaml** file to configure the desired features.\n",
    "2. Install, import and initialize Nvidia-DL-Framework-Inspect tool before initializing any Transormer Engine layers.\n",
    "3. One can pass `debug_name=\"...\"` to init of every TE layer to easier identify layer names. If this will not be provided, names will be infered automatically.\n",
    "4. Invoke `debug_api.step()` at the end of one forward-backward pass.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "To use the debug features of Transformer Engine, you need to install the [Nvidia-DL-Framework-Inspect](https://github.com/NVIDIA/nvidia-dlfw-inspect) package provided by NVIDIA. You can install it by following these steps:\n",
    "\n",
    "```\n",
    "git clone [link]\n",
    "cd nvidia-dlfw-inspect\n",
    "pip install .\n",
    "```\n",
    "\n",
    "#### Config file\n",
    "\n",
    "We need to prepare **config.yaml** file, as below\n",
    "\n",
    "```yaml\n",
    "# config.yaml\n",
    "\n",
    "fc1_fprop_to_fp8:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [fc1, fc2] # contains fc1 or fc2 in name\n",
    "  transformer_engine:\n",
    "    DisableFp8Gemm:\n",
    "      enabled: True\n",
    "      gemms: [fprop]\n",
    "\n",
    "log_tensor_stats:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [layernorm_linear] # contains layernorm_linear in name\n",
    "  transformer_engine:\n",
    "    LogTensorStats:\n",
    "      enabled: True\n",
    "      stats: [max, min, mean, std, l1_norm]\n",
    "      tensors: [activation]\n",
    "      freq: 1\n",
    "      start_step: 2\n",
    "      end_step: 5\n",
    "```\n",
    "\n",
    "Further explanation on how to create config files is in the [next part of the documentation](./2_config_file_structure.ipynb).\n",
    "\n",
    "#### Adjusting Python file\n",
    "\n",
    "```python\n",
    "# (...)\n",
    "\n",
    "import nvdlfw_inspect.api as debug_api\n",
    "debug_api.initialize(\n",
    "    config_file=\"./config.yaml\",\n",
    "    feature_dirs=[\"/path/to/transformer_engine/debug/features\"],\n",
    "    log_dir=\"./log\",\n",
    "    default_logging_enabled=True)\n",
    "\n",
    "# initilization of the TransformerLayer after the \n",
    "# debug_api.initialize(...)\n",
    "transformer_layer = TransformerLayer(\n",
    "  debug_name=\"transformer_layer\",\n",
    "  # ...\n",
    "\n",
    "# (...)\n",
    "for epoch in range(5):\n",
    "  # forward and backward pass\n",
    "  # ...\n",
    "  debug_api.step()\n",
    "```\n",
    "\n",
    "In the modified code above, the following changes were made:\n",
    "\n",
    "1. Added an import for `nvtorch_inspect.api`.\n",
    "2. Initialized the Nvidia-DL-Framework-Inspect by calling `debug_api.initialize()` with appropriate configuration, specifying the path to the config file, feature directories, and log directory.\n",
    "3. Added `debug_api.step()` after each of the forward-backward pass.\n",
    "\n",
    "#### Inspecting the logs\n",
    "\n",
    "Let's look at the files with the logs. Two files will be created:\n",
    "\n",
    "1. First for main debug logs.\n",
    "2. Second for statistics logs.\n",
    "\n",
    "Let's look inside them!\n",
    "\n",
    "```\n",
    "# log/nvdlfw_inspect_logs/nvdlfw_inspect_globalrank-0.log\n",
    "\n",
    "INFO - Default logging to file enabled at ./log\n",
    "INFO - Reading config from ./config.yaml.\n",
    "INFO - Loaded configs for dict_keys(['fc1_fprop_to_fp8', 'log_tensor_stats']).\n",
    "WARNING - > UserBuffers are not supported in debug module. Using UB optimization will not affect the debug module. \n",
    "INFO - transformer_layer.self_attention.layernorm_qkv: Tensor: activation, gemm fprop - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv: Tensor: activation, gemm wgrad - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv: Tensor: weight, gemm fprop - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv: Tensor: weight, gemm dgrad - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv: Tensor: gradient, gemm dgrad - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv: Tensor: gradient, gemm wgrad - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.proj: Tensor: activation, gemm fprop - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.proj: Tensor: activation, gemm wgrad - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.proj: Tensor: weight, gemm fprop - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.proj: Tensor: weight, gemm dgrad - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.proj: Tensor: gradient, gemm dgrad - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.proj: Tensor: gradient, gemm wgrad - FP8 quanitation\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Tensor: activation, gemm fprop - High precision\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Tensor: activation, gemm wgrad - FP8 quanitation\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Tensor: weight, gemm fprop - High precision\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Tensor: weight, gemm dgrad - FP8 quanitation\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Tensor: gradient, gemm dgrad - FP8 quanitation\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Tensor: gradient, gemm wgrad - FP8 quanitation\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Tensor: activation, gemm fprop - High precision\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Tensor: activation, gemm wgrad - FP8 quanitation\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Tensor: weight, gemm fprop - High precision\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Tensor: weight, gemm dgrad - FP8 quanitation\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Tensor: gradient, gemm dgrad - FP8 quanitation\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Tensor: gradient, gemm wgrad - FP8 quanitation\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv: Feature=LogTensorStats, API=look_at_tensor_before_process: activation\n",
    "....\n",
    "```\n",
    "\n",
    "In the main log file, you can find detailed information about the transformer's layer GEMMs behavior. You can see that `fc1` and `fc2` fprop GEMMs are run in high precision, as intended.\n",
    "\n",
    "```\n",
    "# log/nvdlfw_inspect_statistics_logs/nvdlfw_inspect_globalrank-0.log\n",
    "\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_max \t\t\t\t iteration=000002 \t\t\t\t value=4.3188\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_min \t\t\t\t iteration=000002 \t\t\t\t value=-4.3386\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_mean \t\t\t\t iteration=000002 \t\t\t\t value=0.0000\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_std \t\t\t\t iteration=000002 \t\t\t\t value=0.9998\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_l1_norm             iteration=000002 \t\t\t\t value=130799.6953\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_max \t\t\t\t iteration=000003 \t\t\t\t value=4.3184\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_min \t\t\t\t iteration=000003 \t\t\t\t value=-4.3381\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_mean \t\t\t\t iteration=000003 \t\t\t\t value=0.0000\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_std \t\t\t\t iteration=000003 \t\t\t\t value=0.9997\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_l1_norm \t         iteration=000003 \t\t\t\t value=130788.1016\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_max \t\t\t\t iteration=000004 \t\t\t\t value=4.3181\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_min \t\t\t\t iteration=000004 \t\t\t\t value=-4.3377\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_mean \t\t\t\t iteration=000004 \t\t\t\t value=0.0000\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_std \t\t\t\t iteration=000004 \t\t\t\t value=0.9996\n",
    "INFO - transformer_layer.self_attention.layernorm_qkv_activation_l1_norm \t         iteration=000004 \t\t\t\t value=130776.7969\n",
    "\n",
    "```\n",
    "\n",
    "The second log file (`nvdlfw_inspect_globalrank-0.log`) contains statistics for tensors we requested in `config.yaml`.\n",
    "\n",
    "\n",
    "#### Logging using TensorBoard\n",
    "\n",
    "Precision debug tools supports logging using [TensorBoard](https://www.tensorflow.org/tensorboard). To enable it, one needs to pass the argument `tb_writer` to the `debug_api.initialize()`.  Let's modify `train.py` file.\n",
    "\n",
    "```python\n",
    "\n",
    "# (...)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb_writer = SummaryWriter('./tensorboard_dir/run1')\n",
    "\n",
    "# add tb_writer to the Debug API initialization\n",
    "debug_api.initialize(\n",
    "    config_file=\"./config.yaml\",\n",
    "    feature_dirs=[\"/path/to/transformer_engine/debug/features\"],\n",
    "    log_dir=\"./log\",\n",
    "    tb_writer=tb_writer)\n",
    "\n",
    "# (...)\n",
    "```\n",
    "\n",
    "Let's run training and open TensorBoard by `tensorboard --logdir=./tensorboard_dir/run1`:\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/tensorboard.png\">\n",
    "    <figcaption> Fig 2: TensorBoard with plotted stats.</figcaption>\n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
